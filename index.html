<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>937のBlog - Ciallo～(∠・ω&lt; )⌒★!</title><meta name="author" content="hhh937meow"><meta name="copyright" content="hhh937meow"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="我超，盒！(T^T)">
<meta property="og:type" content="website">
<meta property="og:title" content="937のBlog">
<meta property="og:url" content="https://937miaow.github.io/index.html">
<meta property="og:site_name" content="937のBlog">
<meta property="og:description" content="我超，盒！(T^T)">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://937miaow.github.io/img/avatar.jpg">
<meta property="article:author" content="hhh937meow">
<meta property="article:tag" content="blog">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://937miaow.github.io/img/avatar.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "WebSite",
  "name": "937のBlog",
  "url": "https://937miaow.github.io/"
}</script><link rel="shortcut icon" href="/img/22.png"><link rel="canonical" href="https://937miaow.github.io/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.json","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200,"highlightFullpage":true,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '937のBlog',
  isHighlightShrink: false,
  isToc: false,
  pageType: 'home'
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">23</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">25</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="page" id="body-wrap"><header class="full_page" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><img class="site-icon" src="/img/33.png" alt="Logo"><span class="site-name">937のBlog</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="site-info"><h1 id="site-title">937のBlog</h1><div id="site-subtitle"><span id="subtitle"></span></div><div id="site_social_icons"><a class="social-icon" href="https://github.com/937miaow" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:hhh937meow@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div id="scroll-down"><i class="fas fa-angle-down scroll-down-effects"></i></div></header><main class="layout" id="content-inner"><div class="recent-posts nc" id="recent-posts"><div class="recent-post-items"><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/paper/A-Framework-of-Knowledge-Graph-Enhanced-Large-Language-Model-Based-on-Question-Decomposition-and-Atomic-Retrieval/" title="A Framework of Knowledge Graph-Enhanced Large Language Model Based on Question Decomposition and Atomic Retrieval">A Framework of Knowledge Graph-Enhanced Large Language Model Based on Question Decomposition and Atomic Retrieval</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-09-07T07:17:27.000Z" title="发表于 2025-09-07 15:17:27">2025-09-07</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-09-07T07:31:06.688Z" title="更新于 2025-09-07 15:31:06">2025-09-07</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/paper/">paper</a></span></div><div class="content">问题 (Problem) 大语言模型（LLM）在应用中存在两大核心痛点：内容幻觉 (Hallucination) 和推理过程不透明 (Lack of Interpretability)。将知识图谱（KG）作为外部知识源与LLM结合，是解决这些问题的有效途径。然而，现有的KG增强LLM方法在处理知识图谱问答（KGQA）任务时存在以下局限：  单轮检索-回答模式 (Retrieve-then-Answer)：这类方法一次性从KG中检索所有可能相关的事实，然后全部喂给LLM进行推理。这严重依赖检索结果的质量，限制了LLM在推理过程中的灵活性和主动性。 多轮多跳推理模式 (Multi-Round LLM-KG Interaction)：这类方法让LLM在KG上进行多步（hop）的探索式推理。但效率低下，因为每一步只检索一跳（one-hop）的邻居节点，且整个过程始终基于原始的复杂问题，没有针对当前推理步骤进行更具针对性的信息检索。  核心问题：现有方法都直接处理原始的复杂问题，而没有显式地分析和分解问题内部的逻辑结构，导致检索不够精准、推理效率低下。 方法 (Method) 为解决上述问...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/paper/A-survey-on-augmenting-KGs-with-LLMs-models-evaluation-metrics-benchmarks-mand-challenges/" title="A survey on augmenting KGs with LLMs: models, evaluation metrics, benchmarks,mand challenges">A survey on augmenting KGs with LLMs: models, evaluation metrics, benchmarks,mand challenges</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-09-07T06:46:11.000Z" title="发表于 2025-09-07 14:46:11">2025-09-07</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-09-07T06:48:56.714Z" title="更新于 2025-09-07 14:48:56">2025-09-07</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/paper/">paper</a></span></div><div class="content">引言 (Introduction) 当前，各行各业（如医疗、金融）数据量爆炸式增长，但传统数据管理系统在处理复杂、互联的数据时显得力不从心，常常导致信息检索效率低下和决策不优。主要问题包括：数据碎片化、信息孤岛等。 为了解决这些问题，研究者们将目光投向了两种先进技术：  知识图谱 (Knowledge Graphs, KGs)：善于用结构化的方式组织和管理知识，能够进行高效的复杂查询和推理。 大语言模型 (Large Language Models, LLMs)：在理解和生成自然语言方面表现卓越。  将二者结合，可以创造出一个强大的框架，既能处理非结构化文本，又能利用结构化知识，从而提升AI系统进行实时数据分析和高效决策的能力。 论文的核心贡献在于，系统性地将LLM与KG的集成方法分为三大范式，并对每种方法的优势、劣下及应用进行了深入探讨。  大语言模型 (LLM) 背景知识 LLMs是基于海量文本数据预训练的深度学习模型，在自然语言处理（NLP）领域取得了革命性突破。   发展历程  90年代: 统计模型，如 N-grams 和隐马尔可夫模型 (HMMs)。 2013-2014...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/paper/GER-GENERATION-EVALUATION-AND-REFLECTION-ENHANCED-LLM-FOR-KNOWLEDGE-GRAPH-QUESTION-ANSWERING/" title="GER: GENERATION, EVALUATION AND REFLECTION ENHANCED LLM FOR KNOWLEDGE GRAPH QUESTION ANSWERING">GER: GENERATION, EVALUATION AND REFLECTION ENHANCED LLM FOR KNOWLEDGE GRAPH QUESTION ANSWERING</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-09-03T10:29:55.000Z" title="发表于 2025-09-03 18:29:55">2025-09-03</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-09-03T10:37:42.099Z" title="更新于 2025-09-03 18:37:42">2025-09-03</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/paper/">paper</a></span></div><div class="content">问题 (Problem) 这篇论文致力于解决知识图谱问答（KGQA）中的一个核心挑战：大型语言模型（LLM）的幻觉问题。 传统的知识图谱问答方法，如图神经网络（GNN）为基础的方法，虽然能很好地适应图结构，但在理解自然语言问题的深层意图方面能力有限。近年来，LLM凭借其强大的自然语言理解能力被引入KGQA领域，并取得了一定的成功。 然而，现有方法普遍忽略了一个严重问题：当LLM与庞大的知识图谱（KG）结合时，KG中包含的大量不相关信息会放大LLM的幻觉。这导致模型会生成一些看似正确但实际上与事实不符的答案，极大地降低了问答系统的可靠性。 因此，本文要解决的核心问题是：如何设计一个框架，能够系统性地减少由KG中无关信息引发的LLM幻觉，从而提升KGQA任务的准确性和可靠性。 方法 (Method) 为了解决上述问题，作者提出了一个名为GER（Generation-Evaluation-Reflection，生成-评估-反思） 的LLM增强反思性推理框架。该框架通过在传统的“生成”之后引入“评估”和“反思”两个关键步骤，让LLM能够利用KG中的事实信息来审视和修正自己的答案。 整体框...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/paper/Self-Refine-Instruction-Tuning-for-Aligning-Reasoning-in-Language-Models/" title="Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models">Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-09-03T07:19:21.000Z" title="发表于 2025-09-03 15:19:21">2025-09-03</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-09-03T10:29:00.382Z" title="更新于 2025-09-03 18:29:00">2025-09-03</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/paper/">paper</a></span></div><div class="content">问题 (Problem) 这篇论文旨在解决将大型语言模型（LLM）强大的推理能力，特别是链式思考（Chain-of-Thought, CoT）能力，迁移到小型语言模型（SLM）上时遇到的核心挑战。 当前主流的方法是监督微调（Supervised Fine-Tuning, SFT），即使用 LLM 生成的推理过程作为示例来训练 SLM。但这种方法存在明显缺陷：  泛化能力不足：SFT 仅仅是让 SLM 模仿 LLM 提供的特定推理路径。然而，同一个问题可能存在多种有效的推理路径，导致模型学到的能力不够通用，泛化性差。 能力差距：SLM 本身不具备像 LLM 那样强大的“涌现”推理能力，简单的模仿训练无法完全弥补这一根本差距。 对齐不完全：训练后的 SLM 在性能上与“教师”LLM 之间仍然存在显著差距。  因此，核心问题是：如何设计一种更有效的方法，不仅能将 LLM 的推理能力迁移给 SLM，还能让 SLM 自我提升和完善这种能力，从而真正实现与 LLM 的推理对齐？  方法 (Method) 作者提出了一种名为 自优化指令微调（Self-Refine Instruction-Tu...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/paper/SELF-REFINE-Iterative-Refinement-with-Self-Feedback/" title="SELF-REFINE: Iterative Refinement with Self-Feedback">SELF-REFINE: Iterative Refinement with Self-Feedback</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-09-01T14:28:28.000Z" title="发表于 2025-09-01 22:28:28">2025-09-01</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-09-02T04:59:42.256Z" title="更新于 2025-09-02 12:59:42">2025-09-02</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/paper/">paper</a></span></div><div class="content">问题 (Problem) 大型语言模型（LLMs）虽然功能强大，但其初次生成的输出往往不是最优的，尤其是在处理具有复杂约束或多方面目标（如对话生成、代码优化）的任务时。传统的优化方法通常需要大量的监督训练数据、额外的模型训练或强化学习，这些都成本高昂且不易获取。因此，研究的核心问题是：能否让大型语言模型在不进行额外训练或使用外部监督数据的情况下，仅通过自我反思和修正来迭代式地提升其输出质量？  方法 (Method) 为了解决上述问题，该研究提出了 SELF-REFINE 框架，其核心思想是利用同一个大型语言模型，让它扮演三个角色：生成者 (Generator)、反馈提供者 (Feedback Provider) 和 精炼者 (Refiner)。整个过程是一个迭代循环，直到满足停止条件为止。 方法流程详解: 该方法主要包含三个核心步骤：生成 (Generate)、反馈 (Feedback) 和 精炼 (Refine)。 第一步：初始生成 (Initial Generation) 给定一个输入 xxx 和一个用于初始生成的提示 pgenp_{gen}pgen​，模型 M\mathc...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/paper/Toolformer-Language-Models-Can-Teach-Themselves-to-Use-Tools/" title="Toolformer: Language Models Can Teach Themselves to Use Tools">Toolformer: Language Models Can Teach Themselves to Use Tools</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-09-01T06:37:35.000Z" title="发表于 2025-09-01 14:37:35">2025-09-01</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-09-01T06:53:12.912Z" title="更新于 2025-09-01 14:53:12">2025-09-01</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/paper/">paper</a></span></div><div class="content">问题 (Problem) 大型语言模型（LLMs）虽然在文本生成和理解上表现出色，但存在一些固有的核心缺陷：  知识过时与幻觉：LLMs的知识被冻结在训练数据的时间点，无法获取最新信息，并且有编造事实（幻觉）的倾向。 缺乏精确计算能力：LLM不擅长进行精确的数学运算，容易在算术问题上犯错。 时间感知能力弱：模型对当前日期和时间没有概念，无法回答与时间流逝相关的问题。 低资源语言处理能力不足：在处理数据稀少的语言时表现不佳。  现有的解决方案通常依赖大量的人工标注来教模型如何使用工具，或者只能在特定任务的设定下使用工具，缺乏通用性。因此，本文旨在解决一个核心问题：如何让语言模型以一种自监督的、通用的方式，学会自己决定何时、如何以及使用何种外部工具来弥补自身缺陷？ 方法 (Method) Toolformer的核心思想是：一个有用的API调用，应该能帮助模型更好地预测未来的文本。基于此，作者提出了一种全新的、自监督的学习范式，让模型“教会自己”使用工具。该方法主要分为三个步骤，如下图2所示：   图例解读：上图以一个问答（QA）工具为例，展示了如何为一个句子 “Pittsburgh ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/paper/AUGMENTING-ZERO-SHOT-DENSE-RETRIEVERS-WITH-PLUG-IN-MIXTURE-OF-MEMORIES/" title="AUGMENTING ZERO-SHOT DENSE RETRIEVERS WITH PLUG-IN MIXTURE-OF-MEMORIES">AUGMENTING ZERO-SHOT DENSE RETRIEVERS WITH PLUG-IN MIXTURE-OF-MEMORIES</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-08-31T11:33:11.000Z" title="发表于 2025-08-31 19:33:11">2025-08-31</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-08-31T12:49:44.363Z" title="更新于 2025-08-31 20:49:44">2025-08-31</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/paper/">paper</a></span></div><div class="content">问题 (Problem) 本文旨在解决稠密检索（Dense Retrieval）模型在**零样本（Zero-Shot）**场景下的泛化能力差的问题。 传统的稠密检索模型在一个大规模的源领域（如网页搜索）上训练后，直接迁移到新的、未见过的目标领域（如生物医药、金融）时，性能会显著下降。虽然通过不断增大语言模型的参数量可以提升泛化性，但这种方式成本高昂且收益递减，在经济上是不可持续的。 因此，核心问题是：如何不通过暴力增加模型参数，而是通过更高效的方式，提升稠密检索模型在不同领域间的零样本迁移和泛化能力？  方法 (Method) 作者提出了一种名为**混合记忆增强（Mixture-Of-Memory Augmentation, MoMA）**的机制，并将其应用于一个基于T5的强大检索器，构建了名为 MoMA-DR 的新系统。 其核心思想是：在对查询（Query）进行编码表示之前，先从一个由**多个不同信息源（语料库）组成的“混合记忆”**中检索出相关的“增强文档”，将这些文档的信息融入查询中，生成一个内容更丰富、意图更明确的“增强后查询表示”，然后再用这个增强后的查询去目标语料库中...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/paper/DynaGRAG-Exploring-the-Topology-of-Information-for-Advancing-Language-Understanding-and-Generation-in-Graph-Retrieval-Augmented-Generation/" title="DynaGRAG | Exploring the Topology of Information for Advancing Language Understanding and Generation in Graph Retrieval-Augmented Generation">DynaGRAG | Exploring the Topology of Information for Advancing Language Understanding and Generation in Graph Retrieval-Augmented Generation</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-08-31T07:19:45.000Z" title="发表于 2025-08-31 15:19:45">2025-08-31</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-08-31T07:29:47.654Z" title="更新于 2025-08-31 15:29:47">2025-08-31</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/paper/">paper</a></span></div><div class="content">问题 (Problem) 大型语言模型 (LLM) 虽然强大，但在处理需要复杂推理、动态演化知识的任务时仍存在局限性。检索增强生成 (RAG) 旨在通过引入外部知识来弥补这一不足，但传统的 RAG 难以有效利用结构化数据。 现有的图增强RAG (Graph RAG) 方法也存在一些问题：  信息粒度损失：一些方法（如微软的GraphRAG）依赖于对图社群进行预先摘要，这种方式牺牲了图的粒度和灵活性。当底层图结构发生变化时，需要重新生成整个索引和摘要，适应性差且计算成本高。 兼容性与焦点局限：另一些方法在与主流LLM架构的兼容性上存在挑战，或者过于关注特定任务（如多跳推理或事实准确性），而忽略了捕捉实体关系的多样性。  因此，核心挑战在于：如何有效捕捉并融合知识图谱中丰富的语义信息和拓扑结构，以增强LLM的语言理解和生成能力，使其能够进行更深层次、更具上下文感知能力的推理。 方法 (Method) 为解决上述问题，该论文提出了一个名为 DynaGRAG (Dynamic Graph Retrieval-Augmented Generation) 的新框架。其核心思想是保留图的原生结...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/paper/GRAG-Graph-Retrieval-Augmented-Generation/" title="GRAG: Graph Retrieval-Augmented Generation">GRAG: Graph Retrieval-Augmented Generation</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-08-30T06:43:49.000Z" title="发表于 2025-08-30 14:43:49">2025-08-30</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-08-30T07:18:24.076Z" title="更新于 2025-08-30 15:18:24">2025-08-30</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/paper/">paper</a></span></div><div class="content">问题 (Problem) 传统的“朴素”检索增强生成（RAG）在处理单个文档时表现良好，但现实世界中的许多数据，如引文网络、社交媒体、知识图谱等，都以相互连接的图（Graph）形式存在。朴素 RAG 的局限性在于：  忽略拓扑信息：它只关注文档的文本内容，忽略了文档之间的连接关系（如引用、链接、关系），而这些连接关系对于深度理解和推理至关重要。 检索单元受限：它以独立的文档作为检索单元，无法检索出一个由多个相关实体及其关系组成的“子图”上下文。   因此，核心问题是：如何让大型语言模型（LLM）在执行 RAG 时，能够有效地利用这种网络化的图结构数据，从而同时理解文本内容和拓扑结构，以生成更准确、更具上下文感知能力的答案？ 这带来了两个核心挑战：  检索挑战：如何从一个大规模的文本图中高效地检索出与问题最相关的文本子图（textual subgraph）？穷举搜索所有可能的子图是一个 NP-hard 问题。 生成挑战：如何将检索到的子图所包含的**联合文本与拓扑信息（joint textual and topological information）**有效地融入到 LLM 中，...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/paper/IMPROVING-LANGUAGE-MODELS-VIA-PLUG-AND-PLAY-RETRIEVAL-FEEDBACK/" title="IMPROVING LANGUAGE MODELS VIA PLUG-AND-PLAY RETRIEVAL FEEDBACK">IMPROVING LANGUAGE MODELS VIA PLUG-AND-PLAY RETRIEVAL FEEDBACK</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-08-29T05:53:03.000Z" title="发表于 2025-08-29 13:53:03">2025-08-29</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-08-29T06:07:10.260Z" title="更新于 2025-08-29 14:07:10">2025-08-29</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/paper/">paper</a></span></div><div class="content">问题 (Problem) 这篇论文主要解决大型语言模型（LLMs）在实际应用中的几个核心痛点：  内容幻觉与不准确性: 尽管LLMs在多种自然语言处理任务上表现出色，但它们常常会生成不正确或完全捏造（即“幻觉”）的信息，这严重限制了它们在需要高可靠性场景下的应用。 知识局限性: LLMs的知识被固化在其模型参数中，这些知识可能是不完整或过时的，尤其难以覆盖训练语料中的长尾知识。 现有解决方案的缺陷:  人工反馈（如RLHF）: 通过人工标注和强化学习来对齐模型，虽然有效，但极其消耗资源、成本高昂且耗时。 实时性差: 对于已经微调好的模型，很难在推理过程中实时接收反馈并进行即时纠错。    因此，论文的核心研究问题是：我们能否在不进行昂贵微调的前提下，设计一个即插即用的自动化流程，利用外部知识库对LLM的生成内容进行反馈和修正，从而提升其准确性？  方法 (Method) 论文提出了一种名为 REFEED (REtrieval FEEDback) 的新型工作流，其核心思想是“先生成，再检索，后优化”，将检索作为一种反馈机制而非传统的输入增强。 基础工作流 (Basic Pipeli...</div></div></div></div><nav id="pagination"><div class="pagination"><span class="page-number current">1</span><a class="page-number" href="/page/2/#content-inner">2</a><a class="page-number" href="/page/3/#content-inner">3</a><a class="extend next" rel="next" href="/page/2/#content-inner"><i class="fas fa-chevron-right fa-fw"></i></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">hhh937meow</div><div class="author-info-description">Ciallo～(∠・ω< )⌒★!</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">23</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">25</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/937miaow"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/937miaow" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:hhh937meow@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">Ciallo～(∠・ω< )⌒★!</div></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/paper/A-Framework-of-Knowledge-Graph-Enhanced-Large-Language-Model-Based-on-Question-Decomposition-and-Atomic-Retrieval/" title="A Framework of Knowledge Graph-Enhanced Large Language Model Based on Question Decomposition and Atomic Retrieval">A Framework of Knowledge Graph-Enhanced Large Language Model Based on Question Decomposition and Atomic Retrieval</a><time datetime="2025-09-07T07:17:27.000Z" title="发表于 2025-09-07 15:17:27">2025-09-07</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/paper/A-survey-on-augmenting-KGs-with-LLMs-models-evaluation-metrics-benchmarks-mand-challenges/" title="A survey on augmenting KGs with LLMs: models, evaluation metrics, benchmarks,mand challenges">A survey on augmenting KGs with LLMs: models, evaluation metrics, benchmarks,mand challenges</a><time datetime="2025-09-07T06:46:11.000Z" title="发表于 2025-09-07 14:46:11">2025-09-07</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/paper/GER-GENERATION-EVALUATION-AND-REFLECTION-ENHANCED-LLM-FOR-KNOWLEDGE-GRAPH-QUESTION-ANSWERING/" title="GER: GENERATION, EVALUATION AND REFLECTION ENHANCED LLM FOR KNOWLEDGE GRAPH QUESTION ANSWERING">GER: GENERATION, EVALUATION AND REFLECTION ENHANCED LLM FOR KNOWLEDGE GRAPH QUESTION ANSWERING</a><time datetime="2025-09-03T10:29:55.000Z" title="发表于 2025-09-03 18:29:55">2025-09-03</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/paper/Self-Refine-Instruction-Tuning-for-Aligning-Reasoning-in-Language-Models/" title="Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models">Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models</a><time datetime="2025-09-03T07:19:21.000Z" title="发表于 2025-09-03 15:19:21">2025-09-03</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/paper/SELF-REFINE-Iterative-Refinement-with-Self-Feedback/" title="SELF-REFINE: Iterative Refinement with Self-Feedback">SELF-REFINE: Iterative Refinement with Self-Feedback</a><time datetime="2025-09-01T14:28:28.000Z" title="发表于 2025-09-01 22:28:28">2025-09-01</time></div></div></div></div><div class="card-widget card-categories"><div class="item-headline">
            <i class="fas fa-folder-open"></i>
            <span>分类</span>
            
          </div>
          <ul class="card-category-list expandBtn" id="aside-cat-list">
            <li class="card-category-list-item "><a class="card-category-list-link" href="/categories/DataStru-Algo/"><span class="card-category-list-name">DataStru&Algo</span><span class="card-category-list-count">2</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/paper/"><span class="card-category-list-name">paper</span><span class="card-category-list-count">21</span></a></li>
          </ul></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>标签</span></div><div class="card-tag-cloud"><a href="/tags/paper/" style="font-size: 1.45em; color: rgb(50, 120, 53);">paper</a><a href="/tags/zero-shot/" style="font-size: 1.28em; color: rgb(52, 50, 64);">zero-shot</a><a href="/tags/Dense-Retrieval/" style="font-size: 1.32em; color: rgb(50, 58, 150);">Dense Retrieval</a><a href="/tags/few-shot/" style="font-size: 1.15em; color: rgb(132, 144, 187);">few-shot</a><a href="/tags/Retrieval/" style="font-size: 1.36em; color: rgb(195, 169, 97);">Retrieval</a><a href="/tags/LLM/" style="font-size: 1.41em; color: rgb(114, 50, 145);">LLM</a><a href="/tags/Neural-IR/" style="font-size: 1.15em; color: rgb(174, 50, 158);">Neural IR</a><a href="/tags/GRAG/" style="font-size: 1.19em; color: rgb(193, 197, 50);">GRAG</a><a href="/tags/KG/" style="font-size: 1.24em; color: rgb(112, 133, 50);">KG</a><a href="/tags/Embedding/" style="font-size: 1.19em; color: rgb(75, 165, 50);">Embedding</a><a href="/tags/supervised/" style="font-size: 1.15em; color: rgb(50, 124, 192);">supervised</a><a href="/tags/Embeddings/" style="font-size: 1.15em; color: rgb(136, 179, 69);">Embeddings</a><a href="/tags/RAG/" style="font-size: 1.19em; color: rgb(77, 96, 67);">RAG</a><a href="/tags/NLP/" style="font-size: 1.15em; color: rgb(199, 74, 50);">NLP</a><a href="/tags/refine/" style="font-size: 1.19em; color: rgb(50, 50, 51);">refine</a><a href="/tags/Data-Structure/" style="font-size: 1.19em; color: rgb(111, 121, 109);">Data Structure</a><a href="/tags/Algorithm/" style="font-size: 1.19em; color: rgb(154, 50, 50);">Algorithm</a><a href="/tags/SCNU-Turing-Discussion/" style="font-size: 1.19em; color: rgb(50, 50, 129);">SCNU Turing Discussion</a><a href="/tags/Maximum-Flow/" style="font-size: 1.15em; color: rgb(58, 141, 159);">Maximum Flow</a><a href="/tags/Graph/" style="font-size: 1.15em; color: rgb(137, 158, 169);">Graph</a><a href="/tags/RL/" style="font-size: 1.15em; color: rgb(50, 155, 170);">RL</a><a href="/tags/unsupervised/" style="font-size: 1.15em; color: rgb(50, 70, 50);">unsupervised</a><a href="/tags/BST/" style="font-size: 1.15em; color: rgb(128, 50, 59);">BST</a><a href="/tags/RBT/" style="font-size: 1.15em; color: rgb(50, 120, 178);">RBT</a><a href="/tags/survey/" style="font-size: 1.15em; color: rgb(123, 50, 164);">survey</a></div></div><div class="card-widget card-archives">
    <div class="item-headline">
      <i class="fas fa-archive"></i>
      <span>归档</span>
      
    </div>
  
    <ul class="card-archive-list">
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/09/">
            <span class="card-archive-list-date">
              九月 2025
            </span>
            <span class="card-archive-list-count">6</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/08/">
            <span class="card-archive-list-date">
              八月 2025
            </span>
            <span class="card-archive-list-count">15</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2024/05/">
            <span class="card-archive-list-date">
              五月 2024
            </span>
            <span class="card-archive-list-count">2</span>
          </a>
        </li>
      
    </ul>
  </div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>网站信息</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">文章数目 :</div><div class="item-count">23</div></div><div class="webinfo-item"><div class="item-name">运行时间 :</div><div class="item-count" id="runtimeshow" data-publishDate="2025-08-09T16:00:00.000Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">本站访客数 :</div><div class="item-count" id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">本站总浏览量 :</div><div class="item-count" id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">最后更新时间 :</div><div class="item-count" id="last-push-date" data-lastPushDate="2025-09-07T07:31:11.507Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 By hhh937meow</span><span class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.4.3</a></span></div><div class="footer_custom_text">Hi, welcome to my <a href="https://937miaow.github.io">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><div class="js-pjax"><script>window.typedJSFn = {
  init: str => {
    window.typed = new Typed('#subtitle', Object.assign({
      strings: str,
      startDelay: 300,
      typeSpeed: 150,
      loop: true,
      backSpeed: 50,
    }, null))
  },
  run: subtitleType => {
    if (true) {
      if (typeof Typed === 'function') {
        subtitleType()
      } else {
        btf.getScript('https://cdn.jsdelivr.net/npm/typed.js/dist/typed.umd.min.js').then(subtitleType)
      }
    } else {
      subtitleType()
    }
  },
  processSubtitle: (content, extraContents = []) => {
    if (true) {
      const sub = ["愿此行终抵群星","Ciallo～(∠・ω< )⌒★!","爱丽丝错了，爱丽丝不该在网上口嗨的","私のオナニーを見てください","芽衣姐，窝不想似","不 会 还 有 人 盯 着 subtitle 吧 ("].slice()

      if (extraContents.length > 0) {
        sub.unshift(...extraContents)
      }

      if (typeof content === 'string') {
        sub.unshift(content)
      } else if (Array.isArray(content)) {
        sub.unshift(...content)
      }

      sub.length > 0 && typedJSFn.init(sub)
    } else {
      document.getElementById('subtitle').textContent = typeof content === 'string' ? content :
        (Array.isArray(content) && content.length > 0 ? content[0] : '')
    }
  }
}
btf.addGlobalFn('pjaxSendOnce', () => { typed.destroy() }, 'typedDestroy')
</script><script>function subtitleType () {
  typedJSFn.processSubtitle(["愿此行终抵群星","Ciallo～(∠・ω< )⌒★!","爱丽丝错了，爱丽丝不该在网上口嗨的","私のオナニーを見てください","芽衣姐，窝不想似","不 会 还 有 人 盯 着 subtitle 吧 ("])
}
typedJSFn.run(subtitleType)</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>