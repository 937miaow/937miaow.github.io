<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>937のBlog - Ciallo～(∠・ω&lt; )⌒★!</title><meta name="author" content="hhh937meow"><meta name="copyright" content="hhh937meow"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="我超，盒！(T^T)">
<meta property="og:type" content="website">
<meta property="og:title" content="937のBlog">
<meta property="og:url" content="https://937miaow.github.io/index.html">
<meta property="og:site_name" content="937のBlog">
<meta property="og:description" content="我超，盒！(T^T)">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://937miaow.github.io/img/avatar.jpg">
<meta property="article:author" content="hhh937meow">
<meta property="article:tag" content="blog">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://937miaow.github.io/img/avatar.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "WebSite",
  "name": "937のBlog",
  "url": "https://937miaow.github.io/"
}</script><link rel="shortcut icon" href="/img/22.png"><link rel="canonical" href="https://937miaow.github.io/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.json","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200,"highlightFullpage":true,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '937のBlog',
  isHighlightShrink: false,
  isToc: false,
  pageType: 'home'
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">21</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">24</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="page" id="body-wrap"><header class="full_page" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><img class="site-icon" src="/img/33.png" alt="Logo"><span class="site-name">937のBlog</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="site-info"><h1 id="site-title">937のBlog</h1><div id="site-subtitle"><span id="subtitle"></span></div><div id="site_social_icons"><a class="social-icon" href="https://github.com/937miaow" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:hhh937meow@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div id="scroll-down"><i class="fas fa-angle-down scroll-down-effects"></i></div></header><main class="layout" id="content-inner"><div class="recent-posts nc" id="recent-posts"><div class="recent-post-items"><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/paper/GER-GENERATION-EVALUATION-AND-REFLECTION-ENHANCED-LLM-FOR-KNOWLEDGE-GRAPH-QUESTION-ANSWERING/" title="GER: GENERATION, EVALUATION AND REFLECTION ENHANCED LLM FOR KNOWLEDGE GRAPH QUESTION ANSWERING">GER: GENERATION, EVALUATION AND REFLECTION ENHANCED LLM FOR KNOWLEDGE GRAPH QUESTION ANSWERING</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-09-03T10:29:55.000Z" title="发表于 2025-09-03 18:29:55">2025-09-03</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-09-03T10:37:42.099Z" title="更新于 2025-09-03 18:37:42">2025-09-03</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/paper/">paper</a></span></div><div class="content">问题 (Problem) 这篇论文致力于解决知识图谱问答（KGQA）中的一个核心挑战：大型语言模型（LLM）的幻觉问题。 传统的知识图谱问答方法，如图神经网络（GNN）为基础的方法，虽然能很好地适应图结构，但在理解自然语言问题的深层意图方面能力有限。近年来，LLM凭借其强大的自然语言理解能力被引入KGQA领域，并取得了一定的成功。 然而，现有方法普遍忽略了一个严重问题：当LLM与庞大的知识图谱（KG）结合时，KG中包含的大量不相关信息会放大LLM的幻觉。这导致模型会生成一些看似正确但实际上与事实不符的答案，极大地降低了问答系统的可靠性。 因此，本文要解决的核心问题是：如何设计一个框架，能够系统性地减少由KG中无关信息引发的LLM幻觉，从而提升KGQA任务的准确性和可靠性。 方法 (Method) 为了解决上述问题，作者提出了一个名为GER（Generation-Evaluation-Reflection，生成-评估-反思） 的LLM增强反思性推理框架。该框架通过在传统的“生成”之后引入“评估”和“反思”两个关键步骤，让LLM能够利用KG中的事实信息来审视和修正自己的答案。 整体框...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/paper/Self-Refine-Instruction-Tuning-for-Aligning-Reasoning-in-Language-Models/" title="Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models">Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-09-03T07:19:21.000Z" title="发表于 2025-09-03 15:19:21">2025-09-03</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-09-03T10:29:00.382Z" title="更新于 2025-09-03 18:29:00">2025-09-03</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/paper/">paper</a></span></div><div class="content">问题 (Problem) 这篇论文旨在解决将大型语言模型（LLM）强大的推理能力，特别是链式思考（Chain-of-Thought, CoT）能力，迁移到小型语言模型（SLM）上时遇到的核心挑战。 当前主流的方法是监督微调（Supervised Fine-Tuning, SFT），即使用 LLM 生成的推理过程作为示例来训练 SLM。但这种方法存在明显缺陷：  泛化能力不足：SFT 仅仅是让 SLM 模仿 LLM 提供的特定推理路径。然而，同一个问题可能存在多种有效的推理路径，导致模型学到的能力不够通用，泛化性差。 能力差距：SLM 本身不具备像 LLM 那样强大的“涌现”推理能力，简单的模仿训练无法完全弥补这一根本差距。 对齐不完全：训练后的 SLM 在性能上与“教师”LLM 之间仍然存在显著差距。  因此，核心问题是：如何设计一种更有效的方法，不仅能将 LLM 的推理能力迁移给 SLM，还能让 SLM 自我提升和完善这种能力，从而真正实现与 LLM 的推理对齐？  方法 (Method) 作者提出了一种名为 自优化指令微调（Self-Refine Instruction-Tu...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/paper/SELF-REFINE-Iterative-Refinement-with-Self-Feedback/" title="SELF-REFINE: Iterative Refinement with Self-Feedback">SELF-REFINE: Iterative Refinement with Self-Feedback</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-09-01T14:28:28.000Z" title="发表于 2025-09-01 22:28:28">2025-09-01</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-09-02T04:59:42.256Z" title="更新于 2025-09-02 12:59:42">2025-09-02</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/paper/">paper</a></span></div><div class="content">问题 (Problem) 大型语言模型（LLMs）虽然功能强大，但其初次生成的输出往往不是最优的，尤其是在处理具有复杂约束或多方面目标（如对话生成、代码优化）的任务时。传统的优化方法通常需要大量的监督训练数据、额外的模型训练或强化学习，这些都成本高昂且不易获取。因此，研究的核心问题是：能否让大型语言模型在不进行额外训练或使用外部监督数据的情况下，仅通过自我反思和修正来迭代式地提升其输出质量？  方法 (Method) 为了解决上述问题，该研究提出了 SELF-REFINE 框架，其核心思想是利用同一个大型语言模型，让它扮演三个角色：生成者 (Generator)、反馈提供者 (Feedback Provider) 和 精炼者 (Refiner)。整个过程是一个迭代循环，直到满足停止条件为止。 方法流程详解: 该方法主要包含三个核心步骤：生成 (Generate)、反馈 (Feedback) 和 精炼 (Refine)。 第一步：初始生成 (Initial Generation) 给定一个输入 xxx 和一个用于初始生成的提示 pgenp_{gen}pgen​，模型 M\mathc...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/paper/Toolformer-Language-Models-Can-Teach-Themselves-to-Use-Tools/" title="Toolformer: Language Models Can Teach Themselves to Use Tools">Toolformer: Language Models Can Teach Themselves to Use Tools</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-09-01T06:37:35.000Z" title="发表于 2025-09-01 14:37:35">2025-09-01</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-09-01T06:53:12.912Z" title="更新于 2025-09-01 14:53:12">2025-09-01</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/paper/">paper</a></span></div><div class="content">问题 (Problem) 大型语言模型（LLMs）虽然在文本生成和理解上表现出色，但存在一些固有的核心缺陷：  知识过时与幻觉：LLMs的知识被冻结在训练数据的时间点，无法获取最新信息，并且有编造事实（幻觉）的倾向。 缺乏精确计算能力：LLM不擅长进行精确的数学运算，容易在算术问题上犯错。 时间感知能力弱：模型对当前日期和时间没有概念，无法回答与时间流逝相关的问题。 低资源语言处理能力不足：在处理数据稀少的语言时表现不佳。  现有的解决方案通常依赖大量的人工标注来教模型如何使用工具，或者只能在特定任务的设定下使用工具，缺乏通用性。因此，本文旨在解决一个核心问题：如何让语言模型以一种自监督的、通用的方式，学会自己决定何时、如何以及使用何种外部工具来弥补自身缺陷？ 方法 (Method) Toolformer的核心思想是：一个有用的API调用，应该能帮助模型更好地预测未来的文本。基于此，作者提出了一种全新的、自监督的学习范式，让模型“教会自己”使用工具。该方法主要分为三个步骤，如下图2所示：   图例解读：上图以一个问答（QA）工具为例，展示了如何为一个句子 “Pittsburgh ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/paper/AUGMENTING-ZERO-SHOT-DENSE-RETRIEVERS-WITH-PLUG-IN-MIXTURE-OF-MEMORIES/" title="AUGMENTING ZERO-SHOT DENSE RETRIEVERS WITH PLUG-IN MIXTURE-OF-MEMORIES">AUGMENTING ZERO-SHOT DENSE RETRIEVERS WITH PLUG-IN MIXTURE-OF-MEMORIES</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-08-31T11:33:11.000Z" title="发表于 2025-08-31 19:33:11">2025-08-31</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-08-31T12:49:44.363Z" title="更新于 2025-08-31 20:49:44">2025-08-31</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/paper/">paper</a></span></div><div class="content">问题 (Problem) 本文旨在解决稠密检索（Dense Retrieval）模型在**零样本（Zero-Shot）**场景下的泛化能力差的问题。 传统的稠密检索模型在一个大规模的源领域（如网页搜索）上训练后，直接迁移到新的、未见过的目标领域（如生物医药、金融）时，性能会显著下降。虽然通过不断增大语言模型的参数量可以提升泛化性，但这种方式成本高昂且收益递减，在经济上是不可持续的。 因此，核心问题是：如何不通过暴力增加模型参数，而是通过更高效的方式，提升稠密检索模型在不同领域间的零样本迁移和泛化能力？  方法 (Method) 作者提出了一种名为**混合记忆增强（Mixture-Of-Memory Augmentation, MoMA）**的机制，并将其应用于一个基于T5的强大检索器，构建了名为 MoMA-DR 的新系统。 其核心思想是：在对查询（Query）进行编码表示之前，先从一个由**多个不同信息源（语料库）组成的“混合记忆”**中检索出相关的“增强文档”，将这些文档的信息融入查询中，生成一个内容更丰富、意图更明确的“增强后查询表示”，然后再用这个增强后的查询去目标语料库中...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/paper/DynaGRAG-Exploring-the-Topology-of-Information-for-Advancing-Language-Understanding-and-Generation-in-Graph-Retrieval-Augmented-Generation/" title="DynaGRAG | Exploring the Topology of Information for Advancing Language Understanding and Generation in Graph Retrieval-Augmented Generation">DynaGRAG | Exploring the Topology of Information for Advancing Language Understanding and Generation in Graph Retrieval-Augmented Generation</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-08-31T07:19:45.000Z" title="发表于 2025-08-31 15:19:45">2025-08-31</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-08-31T07:29:47.654Z" title="更新于 2025-08-31 15:29:47">2025-08-31</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/paper/">paper</a></span></div><div class="content">问题 (Problem) 大型语言模型 (LLM) 虽然强大，但在处理需要复杂推理、动态演化知识的任务时仍存在局限性。检索增强生成 (RAG) 旨在通过引入外部知识来弥补这一不足，但传统的 RAG 难以有效利用结构化数据。 现有的图增强RAG (Graph RAG) 方法也存在一些问题：  信息粒度损失：一些方法（如微软的GraphRAG）依赖于对图社群进行预先摘要，这种方式牺牲了图的粒度和灵活性。当底层图结构发生变化时，需要重新生成整个索引和摘要，适应性差且计算成本高。 兼容性与焦点局限：另一些方法在与主流LLM架构的兼容性上存在挑战，或者过于关注特定任务（如多跳推理或事实准确性），而忽略了捕捉实体关系的多样性。  因此，核心挑战在于：如何有效捕捉并融合知识图谱中丰富的语义信息和拓扑结构，以增强LLM的语言理解和生成能力，使其能够进行更深层次、更具上下文感知能力的推理。 方法 (Method) 为解决上述问题，该论文提出了一个名为 DynaGRAG (Dynamic Graph Retrieval-Augmented Generation) 的新框架。其核心思想是保留图的原生结...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/paper/GRAG-Graph-Retrieval-Augmented-Generation/" title="GRAG: Graph Retrieval-Augmented Generation">GRAG: Graph Retrieval-Augmented Generation</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-08-30T06:43:49.000Z" title="发表于 2025-08-30 14:43:49">2025-08-30</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-08-30T07:18:24.076Z" title="更新于 2025-08-30 15:18:24">2025-08-30</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/paper/">paper</a></span></div><div class="content">问题 (Problem) 传统的“朴素”检索增强生成（RAG）在处理单个文档时表现良好，但现实世界中的许多数据，如引文网络、社交媒体、知识图谱等，都以相互连接的图（Graph）形式存在。朴素 RAG 的局限性在于：  忽略拓扑信息：它只关注文档的文本内容，忽略了文档之间的连接关系（如引用、链接、关系），而这些连接关系对于深度理解和推理至关重要。 检索单元受限：它以独立的文档作为检索单元，无法检索出一个由多个相关实体及其关系组成的“子图”上下文。   因此，核心问题是：如何让大型语言模型（LLM）在执行 RAG 时，能够有效地利用这种网络化的图结构数据，从而同时理解文本内容和拓扑结构，以生成更准确、更具上下文感知能力的答案？ 这带来了两个核心挑战：  检索挑战：如何从一个大规模的文本图中高效地检索出与问题最相关的文本子图（textual subgraph）？穷举搜索所有可能的子图是一个 NP-hard 问题。 生成挑战：如何将检索到的子图所包含的**联合文本与拓扑信息（joint textual and topological information）**有效地融入到 LLM 中，...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/paper/IMPROVING-LANGUAGE-MODELS-VIA-PLUG-AND-PLAY-RETRIEVAL-FEEDBACK/" title="IMPROVING LANGUAGE MODELS VIA PLUG-AND-PLAY RETRIEVAL FEEDBACK">IMPROVING LANGUAGE MODELS VIA PLUG-AND-PLAY RETRIEVAL FEEDBACK</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-08-29T05:53:03.000Z" title="发表于 2025-08-29 13:53:03">2025-08-29</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-08-29T06:07:10.260Z" title="更新于 2025-08-29 14:07:10">2025-08-29</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/paper/">paper</a></span></div><div class="content">问题 (Problem) 这篇论文主要解决大型语言模型（LLMs）在实际应用中的几个核心痛点：  内容幻觉与不准确性: 尽管LLMs在多种自然语言处理任务上表现出色，但它们常常会生成不正确或完全捏造（即“幻觉”）的信息，这严重限制了它们在需要高可靠性场景下的应用。 知识局限性: LLMs的知识被固化在其模型参数中，这些知识可能是不完整或过时的，尤其难以覆盖训练语料中的长尾知识。 现有解决方案的缺陷:  人工反馈（如RLHF）: 通过人工标注和强化学习来对齐模型，虽然有效，但极其消耗资源、成本高昂且耗时。 实时性差: 对于已经微调好的模型，很难在推理过程中实时接收反馈并进行即时纠错。    因此，论文的核心研究问题是：我们能否在不进行昂贵微调的前提下，设计一个即插即用的自动化流程，利用外部知识库对LLM的生成内容进行反馈和修正，从而提升其准确性？  方法 (Method) 论文提出了一种名为 REFEED (REtrieval FEEDback) 的新型工作流，其核心思想是“先生成，再检索，后优化”，将检索作为一种反馈机制而非传统的输入增强。 基础工作流 (Basic Pipeli...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/paper/Atlas-Few-shot-Learning-with-Retrieval-Augmented-Language-Models/" title="Atlas: Few-shot Learning with Retrieval Augmented Language Models">Atlas: Few-shot Learning with Retrieval Augmented Language Models</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-08-28T14:31:28.000Z" title="发表于 2025-08-28 22:31:28">2025-08-28</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-08-28T17:09:09.592Z" title="更新于 2025-08-29 01:09:09">2025-08-29</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/paper/">paper</a></span></div><div class="content">问题 (Problem) 传统的大型语言模型（LLMs）在少样本学习（few-shot learning）上表现出色，但这通常依赖于巨大的参数量来存储世界知识。这引发了一个核心问题：强大的少样本学习能力是否必须与庞大的模型参数（即内置记忆）绑定？ 这篇论文旨在探讨是否可以将模型的“记忆”（知识存储）与“推理”（泛化能力）解耦。作者假设，通过将知识存储外包给一个外部的、可检索的知识库，模型可以将更多参数用于学习推理和泛化能力，从而在拥有较少参数的情况下，在知识密集型任务（如问答、事实核查）上实现卓越的少样本学习性能。 本文的目标是设计并训练一个精心构建的检索增强语言模型——ATLAS，验证其在知识密集型任务上，仅用少量样本就能超越巨大参数量模型的潜力。  方法 (Method) ATLAS 遵循一个统一的“文本到文本”（text-to-text）框架，其中所有任务都被建模为：输入一个文本查询（query），生成一个文本输出（output）。其核心是一个由**检索器（Retriever）和语言模型（Language Model）**组成的双模块架构。 模型架构 (Architectu...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/paper/REPLUG-Retrieval-Augmented-Black-Box-Language-Models/" title="REPLUG: Retrieval-Augmented Black-Box Language Models">REPLUG: Retrieval-Augmented Black-Box Language Models</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-08-27T12:56:55.000Z" title="发表于 2025-08-27 20:56:55">2025-08-27</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-08-27T13:10:07.555Z" title="更新于 2025-08-27 21:10:07">2025-08-27</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/paper/">paper</a></span></div><div class="content">问题 (Problem) 大型语言模型（LLMs）如GPT-3虽然强大，但存在两个核心问题：  知识局限性：模型参数中存储的知识是静态的，无法实时更新，且对于长尾知识（rare knowledge）的覆盖不全，容易产生事实性错误或“幻觉”。 黑盒特性：当前最先进的LLMs（通常 &gt;100B 参数）往往通过API提供服务，用户无法访问模型的内部参数、梯度或进行微调。这使得传统的、需要“白盒”访问权限的检索增强方法（如RETRO、Atlas）无法适用。  因此，本文的核心问题是：如何在只能“黑盒”访问（即只能输入文本、获取输出）的前提下，通过外部知识库对大型语言模型进行有效的检索增强，以提升其性能并减少幻觉？ 方法 (Method) 作者提出了 REPLUG (Retrieve and Plug) 框架，它将语言模型视为一个不可更改的黑盒，并将一个可调优的检索器作为插件来增强它。 REPLUG 推理过程 REPLUG的推理过程分为两步：文档检索 和 输入重构与集成。  REPLUG 推理流程图（论文Figure 2）   文档检索 (Document Retrieval)  给...</div></div></div></div><nav id="pagination"><div class="pagination"><span class="page-number current">1</span><a class="page-number" href="/page/2/#content-inner">2</a><a class="page-number" href="/page/3/#content-inner">3</a><a class="extend next" rel="next" href="/page/2/#content-inner"><i class="fas fa-chevron-right fa-fw"></i></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">hhh937meow</div><div class="author-info-description">Ciallo～(∠・ω< )⌒★!</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">21</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">24</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/937miaow"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/937miaow" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:hhh937meow@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">Ciallo～(∠・ω< )⌒★!</div></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/paper/GER-GENERATION-EVALUATION-AND-REFLECTION-ENHANCED-LLM-FOR-KNOWLEDGE-GRAPH-QUESTION-ANSWERING/" title="GER: GENERATION, EVALUATION AND REFLECTION ENHANCED LLM FOR KNOWLEDGE GRAPH QUESTION ANSWERING">GER: GENERATION, EVALUATION AND REFLECTION ENHANCED LLM FOR KNOWLEDGE GRAPH QUESTION ANSWERING</a><time datetime="2025-09-03T10:29:55.000Z" title="发表于 2025-09-03 18:29:55">2025-09-03</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/paper/Self-Refine-Instruction-Tuning-for-Aligning-Reasoning-in-Language-Models/" title="Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models">Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models</a><time datetime="2025-09-03T07:19:21.000Z" title="发表于 2025-09-03 15:19:21">2025-09-03</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/paper/SELF-REFINE-Iterative-Refinement-with-Self-Feedback/" title="SELF-REFINE: Iterative Refinement with Self-Feedback">SELF-REFINE: Iterative Refinement with Self-Feedback</a><time datetime="2025-09-01T14:28:28.000Z" title="发表于 2025-09-01 22:28:28">2025-09-01</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/paper/Toolformer-Language-Models-Can-Teach-Themselves-to-Use-Tools/" title="Toolformer: Language Models Can Teach Themselves to Use Tools">Toolformer: Language Models Can Teach Themselves to Use Tools</a><time datetime="2025-09-01T06:37:35.000Z" title="发表于 2025-09-01 14:37:35">2025-09-01</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/paper/AUGMENTING-ZERO-SHOT-DENSE-RETRIEVERS-WITH-PLUG-IN-MIXTURE-OF-MEMORIES/" title="AUGMENTING ZERO-SHOT DENSE RETRIEVERS WITH PLUG-IN MIXTURE-OF-MEMORIES">AUGMENTING ZERO-SHOT DENSE RETRIEVERS WITH PLUG-IN MIXTURE-OF-MEMORIES</a><time datetime="2025-08-31T11:33:11.000Z" title="发表于 2025-08-31 19:33:11">2025-08-31</time></div></div></div></div><div class="card-widget card-categories"><div class="item-headline">
            <i class="fas fa-folder-open"></i>
            <span>分类</span>
            
          </div>
          <ul class="card-category-list expandBtn" id="aside-cat-list">
            <li class="card-category-list-item "><a class="card-category-list-link" href="/categories/DataStru-Algo/"><span class="card-category-list-name">DataStru&Algo</span><span class="card-category-list-count">2</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/paper/"><span class="card-category-list-name">paper</span><span class="card-category-list-count">19</span></a></li>
          </ul></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>标签</span></div><div class="card-tag-cloud"><a href="/tags/paper/" style="font-size: 1.45em; color: rgb(114, 67, 95);">paper</a><a href="/tags/zero-shot/" style="font-size: 1.25em; color: rgb(197, 95, 132);">zero-shot</a><a href="/tags/Dense-Retrieval/" style="font-size: 1.3em; color: rgb(173, 50, 90);">Dense Retrieval</a><a href="/tags/few-shot/" style="font-size: 1.15em; color: rgb(134, 81, 50);">few-shot</a><a href="/tags/Retrieval/" style="font-size: 1.35em; color: rgb(50, 50, 50);">Retrieval</a><a href="/tags/LLM/" style="font-size: 1.4em; color: rgb(122, 101, 186);">LLM</a><a href="/tags/Neural-IR/" style="font-size: 1.15em; color: rgb(50, 50, 128);">Neural IR</a><a href="/tags/GRAG/" style="font-size: 1.2em; color: rgb(50, 65, 112);">GRAG</a><a href="/tags/KG/" style="font-size: 1.15em; color: rgb(50, 50, 121);">KG</a><a href="/tags/Embedding/" style="font-size: 1.2em; color: rgb(50, 200, 50);">Embedding</a><a href="/tags/supervised/" style="font-size: 1.15em; color: rgb(62, 195, 109);">supervised</a><a href="/tags/Embeddings/" style="font-size: 1.15em; color: rgb(124, 51, 114);">Embeddings</a><a href="/tags/RAG/" style="font-size: 1.2em; color: rgb(167, 134, 84);">RAG</a><a href="/tags/NLP/" style="font-size: 1.15em; color: rgb(50, 91, 104);">NLP</a><a href="/tags/refine/" style="font-size: 1.2em; color: rgb(122, 110, 110);">refine</a><a href="/tags/Data-Structure/" style="font-size: 1.2em; color: rgb(56, 104, 160);">Data Structure</a><a href="/tags/Algorithm/" style="font-size: 1.2em; color: rgb(130, 173, 96);">Algorithm</a><a href="/tags/SCNU-Turing-Discussion/" style="font-size: 1.2em; color: rgb(50, 60, 167);">SCNU Turing Discussion</a><a href="/tags/Maximum-Flow/" style="font-size: 1.15em; color: rgb(50, 50, 50);">Maximum Flow</a><a href="/tags/Graph/" style="font-size: 1.15em; color: rgb(159, 178, 57);">Graph</a><a href="/tags/RL/" style="font-size: 1.15em; color: rgb(99, 50, 129);">RL</a><a href="/tags/unsupervised/" style="font-size: 1.15em; color: rgb(50, 133, 103);">unsupervised</a><a href="/tags/BST/" style="font-size: 1.15em; color: rgb(85, 153, 50);">BST</a><a href="/tags/RBT/" style="font-size: 1.15em; color: rgb(186, 50, 79);">RBT</a></div></div><div class="card-widget card-archives">
    <div class="item-headline">
      <i class="fas fa-archive"></i>
      <span>归档</span>
      
    </div>
  
    <ul class="card-archive-list">
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/09/">
            <span class="card-archive-list-date">
              九月 2025
            </span>
            <span class="card-archive-list-count">4</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/08/">
            <span class="card-archive-list-date">
              八月 2025
            </span>
            <span class="card-archive-list-count">15</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2024/05/">
            <span class="card-archive-list-date">
              五月 2024
            </span>
            <span class="card-archive-list-count">2</span>
          </a>
        </li>
      
    </ul>
  </div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>网站信息</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">文章数目 :</div><div class="item-count">21</div></div><div class="webinfo-item"><div class="item-name">运行时间 :</div><div class="item-count" id="runtimeshow" data-publishDate="2025-08-09T16:00:00.000Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">本站访客数 :</div><div class="item-count" id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">本站总浏览量 :</div><div class="item-count" id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">最后更新时间 :</div><div class="item-count" id="last-push-date" data-lastPushDate="2025-09-03T10:37:56.847Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 By hhh937meow</span><span class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.4.3</a></span></div><div class="footer_custom_text">Hi, welcome to my <a href="https://937miaow.github.io">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><div class="js-pjax"><script>window.typedJSFn = {
  init: str => {
    window.typed = new Typed('#subtitle', Object.assign({
      strings: str,
      startDelay: 300,
      typeSpeed: 150,
      loop: true,
      backSpeed: 50,
    }, null))
  },
  run: subtitleType => {
    if (true) {
      if (typeof Typed === 'function') {
        subtitleType()
      } else {
        btf.getScript('https://cdn.jsdelivr.net/npm/typed.js/dist/typed.umd.min.js').then(subtitleType)
      }
    } else {
      subtitleType()
    }
  },
  processSubtitle: (content, extraContents = []) => {
    if (true) {
      const sub = ["愿此行终抵群星","Ciallo～(∠・ω< )⌒★!","爱丽丝错了，爱丽丝不该在网上口嗨的","私のオナニーを見てください","芽衣姐，窝不想似","不 会 还 有 人 盯 着 subtitle 吧 ("].slice()

      if (extraContents.length > 0) {
        sub.unshift(...extraContents)
      }

      if (typeof content === 'string') {
        sub.unshift(content)
      } else if (Array.isArray(content)) {
        sub.unshift(...content)
      }

      sub.length > 0 && typedJSFn.init(sub)
    } else {
      document.getElementById('subtitle').textContent = typeof content === 'string' ? content :
        (Array.isArray(content) && content.length > 0 ? content[0] : '')
    }
  }
}
btf.addGlobalFn('pjaxSendOnce', () => { typed.destroy() }, 'typedDestroy')
</script><script>function subtitleType () {
  typedJSFn.processSubtitle(["愿此行终抵群星","Ciallo～(∠・ω< )⌒★!","爱丽丝错了，爱丽丝不该在网上口嗨的","私のオナニーを見てください","芽衣姐，窝不想似","不 会 还 有 人 盯 着 subtitle 吧 ("])
}
typedJSFn.run(subtitleType)</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>