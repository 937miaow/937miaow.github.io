[{"title":"AUGMENTING ZERO-SHOT DENSE RETRIEVERS WITH PLUG-IN MIXTURE-OF-MEMORIES","url":"/paper/AUGMENTING-ZERO-SHOT-DENSE-RETRIEVERS-WITH-PLUG-IN-MIXTURE-OF-MEMORIES/","content":"问题 (Problem)\n本文旨在解决稠密检索（Dense Retrieval）模型在**零样本（Zero-Shot）**场景下的泛化能力差的问题。\n传统的稠密检索模型在一个大规模的源领域（如网页搜索）上训练后，直接迁移到新的、未见过的目标领域（如生物医药、金融）时，性能会显著下降。虽然通过不断增大语言模型的参数量可以提升泛化性，但这种方式成本高昂且收益递减，在经济上是不可持续的。\n因此，核心问题是：如何不通过暴力增加模型参数，而是通过更高效的方式，提升稠密检索模型在不同领域间的零样本迁移和泛化能力？\n\n方法 (Method)\n作者提出了一种名为**混合记忆增强（Mixture-Of-Memory Augmentation, MoMA）**的机制，并将其应用于一个基于T5的强大检索器，构建了名为 MoMA-DR 的新系统。\n其核心思想是：在对查询（Query）进行编码表示之前，先从一个由**多个不同信息源（语料库）组成的“混合记忆”**中检索出相关的“增强文档”，将这些文档的信息融入查询中，生成一个内容更丰富、意图更明确的“增强后查询表示”，然后再用这个增强后的查询去目标语料库中检索最终的答案。\n模型架构\nMoMA-DR 系统包含两个核心组件，如下图所示：\n\n图1：MoMA-DR 模型架构示意图\n\n增强组件 (faf^afa): 这是一个独立的稠密检索器。它的任务是接收原始查询 qqq，然后从“混合记忆” M\\mathcal{M}M 中检索出 KKK 个最相关的增强文档 Da=d1a,...,dKaD^a = {d_1^a, ..., d_K^a}Da=d1a​,...,dKa​。\n主模型 (fMoMAf^{MoMA}fMoMA): 这是最终用于检索的稠密检索器。它利用 Fusion-in-Decoder (FiD) 机制来融合原始查询和增强文档的信息。\n\n首先，使用 T5 的编码器（Encoder）分别对原始查询 qqq 和 KKK 个增强文档 diad_i^adia​ 进行编码。\n然后，T5 的解码器（Decoder）同时关注（attend to）所有这些编码后的向量。\n最后，解码器输出一个融合了所有信息的、增强后的查询向量 qaq^aqa。这个 qaq^aqa 会被用来与目标语料库中的文档向量进行点积计算，以得到最终的排序。\n\n\n\n数学公式解读\n\n\n基础稠密检索 (Dense Retrieval)\n模型的打分函数是查询和文档向量的点积：\nf(q,d)=q⋅df(q,d)=q \\cdot d\nf(q,d)=q⋅d\n其中向量由一个基于 Sentence-T5 的编码器 g(⋅)g(\\cdot)g(⋅) 生成，该编码器利用 T5 的 Encoder-Decoder 架构，将文本序列输入 Encoder，并取 Decoder 输出的 [CLS] 位置的表征作为最终的文本向量：\ng(x)=Dec(Enc(x))g(x)=Dec(Enc(x))\ng(x)=Dec(Enc(x))\n\n\nMoMA 的增强过程\n\n\n从混合记忆中检索增强文档：\nDa=ANNfa(q,∘)M;M={C1,...,CM}D^{a} = ANN_{f^{a}(q,\\circ)}^{\\mathcal{M}} \\quad ; \\quad \\mathcal{M}=\\{C_{1},...,C_{M}\\}\nDa=ANNfa(q,∘)M​;M={C1​,...,CM​}\n这里，faf^afa 是增强组件的检索模型，M\\mathcal{M}M 是由多个语料库 CiC_iCi​ 组成的混合记忆池，ANNANNANN 代表高效的近似最近邻搜索。\n\n\n生成增强查询向量 (Fusion-in-Decoder):\nqa=gMoMA(q)=Dec(Enc(q),Enc(d1a),...,Enc(dKa))q^a = g^{MoMA}(q) = Dec(Enc(q),Enc(d_{1}^{a}),...,Enc(d_{K}^{a}))\nqa=gMoMA(q)=Dec(Enc(q),Enc(d1a​),...,Enc(dKa​))\n这个公式清晰地展示了 FiD 的过程：将查询和所有增强文档分别编码，然后由一个解码器统一处理，输出最终的查询向量 qaq^aqa。\n\n\n最终检索打分:\nfMoMA(q,d)=qa⋅df^{MoMA}(q,d) = q^{a} \\cdot d\nfMoMA(q,d)=qa⋅d\n使用增强后的查询向量 qaq^aqa 和常规的文档向量 ddd（由 g(d)g(d)g(d) 生成）进行匹配。\n\n\n\n\n联合学习机制 (Joint Learning)\n如何让增强组件 faf^afa 学会“挑选”有用的增强文档是整个方法的关键，因为我们并没有直接的标注数据。作者设计了一套巧妙的迭代式联合学习流程：\n\n\n训练主模型 (fMoMAf^{MoMA}fMoMA):\n这一步相对直接。给定增强组件 faf^afa 检索出的文档，通过标准的排序损失函数（带 ANCE 难负样本）来优化主模型。损失函数的目标是让 fMoMA(qs,d+)f^{MoMA}(q^s, d^+)fMoMA(qs,d+) 的分数高于 fMoMA(qs,d−)f^{MoMA}(q^s, d^-)fMoMA(qs,d−)。\n\n\n训练增强组件 (faf^afa):\n这是创新的核心。为了给 faf^afa 提供监督信号，作者利用了主模型解码器的注意力分数作为“伪标签”或“软标签”。\n\n计算文档有用性: 解码器在生成 qaq^aqa 时，对每个输入（原始查询和增强文档）的注意力权重总和 FidAtt(dia)FidAtt(d_i^a)FidAtt(dia​)，可以被看作是该文档 diad_i^adia​ 的“有用性”得分。\n构建正样本集 (Da+D^{a+}Da+): 用于训练 faf^afa 的正样本，由两部分组成：\n\n源任务中已知的相关文档 Ds+D^{s+}Ds+。\n在增强文档中，获得最高注意力分数的前 NNN 个文档。Da+=Ds+∪Top-NFidAtt(da),DaD^{a+}=D^{s+}\\cup \\text{Top-N}_{FidAtt(d^{a}),D^{a}}\nDa+=Ds+∪Top-NFidAtt(da),Da​\n\n\n\n构建负样本集 (Da−D^{a-}Da−): 同样采用 ANCE 方式，但关键在于，负样本是从整个混合记忆池 M\\mathcal{M}M 中挖掘的。这迫使增强组件 faf^afa 必须学会在不同风格和领域的语料中进行区分和选择，从而提升其泛化能力。\n迭代训练: 整个训练过程是迭代的，在每个 episode 中，先用上一轮的模型挖掘难负样本和增强文档，然后依次更新主模型和增强组件。\n\n\n\n即插即用记忆 (Plug-in Memory)\nMoMA 机制的一个巨大优势是其灵活性。在推理（测试）阶段，当模型面对一个新领域的任务时，我们可以直接将该任务的目标语料库 CtC^tCt “即插即用”地加入到混合记忆池 M\\mathcal{M}M 中（即 M=M∪Ct\\mathcal{M} = \\mathcal{M} \\cup C^tM=M∪Ct），而无需重新训练任何模型参数。\n这样，增强组件 faf^afa 就可以从目标语料库中直接检索到领域相关的上下文信息来增强查询，极大地提升了模型对新领域的适应能力。\n图例解读\n\n\n图 2: 展示了联合学习过程中，模型行为的变化。随着训练的进行（从 Epi-0 到 Epi-2），MoMA-DR 逐渐学会对源数据中的“相关文档”（Marco Rel）给予更高的注意力，并且更多地将它们检索为增强文档。这证明了联合学习机制有效地将最终任务的信号反馈给了增强组件。\n\n\n\n图 3: 展示了“即插即用”机制的效果。在测试阶段，即使目标语料库是新加入的，模型依然会给予它最高的关注，并从中检索大量的增强文档。例如，在处理基于维基百科的任务（如 NQ, HotpotQA）时，模型会侧重从维基百科中获取信息；在处理医学任务（如 TREC-COVID）时，则会侧重于从新加入的目标语料库和 MeSH（医学知识库）中获取信息。这表明增强组件具备了出色的零样本泛化能力。\n\n\n\n图 4: 显示了在训练过程中，增强组件和主模型（End Retriever）的性能（在源任务 MSMARCO 上的 MRR）都在稳步提升。这说明两者形成了互相促进的良性循环。\n\n\nBaseline\n本文的实验对比了多种类型的检索模型：\n\n稀疏检索:\n\nBM25: 经典的基于词频的算法。\n\n\n稠密检索 (单向量基础模型):\n\nDPR, ANCE: 基于 BERT 的经典稠密检索模型。\nT5-ANCE: 将 ANCE 的骨干网络换成 T5，是 MoMA-DR 最直接的对照组（可以看作是“无记忆增强”的版本）。\ncoCondenser: 通过持续预训练提升泛化性的模型。\nGTR: 当时最先进的基于 T5 的稠密检索模型。\n\n\n使用生成伪标签的稠密检索:\n\nGenQ: 为每个目标任务生成伪查询-文档对，然后独立训练模型。\n\n\n作为参考的更强模型:\n\nColBERT: 多向量表示的晚期交互模型，索引和计算开销更大。\nGTR-large: 使用了更大参数量 T5-large 的 GTR 模型。\n\n\n\n\n数据集 (Datasets)\n\n源领域 (Source Domain):\n\nMS MARCO: 一个大规模的网页问答数据集，用于模型的基础训练。\n\n\n评估/目标领域 (Target Domains):\n\nBEIR Benchmark: 一个包含 18 个不同数据集的、异构的零样本信息检索评测基准。涵盖的领域非常广泛，包括：\n\n生物医药: TREC-COVID, BioASQ\n开放域问答: NQ, HotpotQA\n金融: FIQA-2018\n新闻: TREC-NEWS\n事实核查: SciFact, FEVER\n…\n\n\n\n\n增强语料库 (Augmenting Corpora):\n\n在训练阶段，混合记忆池由 MS MARCO、Wikipedia（英文维基百科）和一个医学知识图谱 (MeSH) 组成。\n\n\n\n\n可复现性 (Reproducibility)\n\n代码: 作者计划在论文被接收后，开源全部代码、增强数据、模型检查点和分析脚本。\n算力: 实验使用了 8 块 Nvidia A100 (80GB) GPU 进行 FP16 混合精度训练。完成三个 episode 的联合训练，总耗时约 13 个小时（增强组件 6.6 小时，主模型 6.3 小时）。论文附录中给出了详细的超参数设置（如 Table 7），可复现性较好。\n\n\n可改进的几个点 (Potential Improvements)\n\n记忆工程 (Memory Engineering): 论文只探索了将源语料库替换为目标语料库。未来可以研究更复杂的记忆组合策略，比如根据查询的意图动态选择或加权不同的记忆库，或者允许用户手动注入特定的知识库来控制模型行为。\n效率优化: 增加的增强步骤（检索+多文档编码）无疑会带来额外的计算延迟。虽然 FiD 机制相对高效，但对于在线服务来说，延迟可能仍然是一个挑战。研究如何蒸馏、量化或缓存这个增强过程，以降低延迟，将是重要的方向。\n超越查询增强: 目前的工作只对查询端进行了增强。未来可以探索是否也可以对文档端进行增强，或者设计一种查询和文档联合增强的机制。\n记忆库的选择: 如何选择最优的初始记忆库组合？除了“大而全”之外，是否存在一些指导原则来构建一个高效且泛化能力强的混合记忆池。\n增强文档的数量: 文中将增强文档数量 KKK 固定为 10。对于不同类型的查询，最优的增强文档数量可能是不同的。可以研究自适应地决定 KKK 值的方法。\n\n\n可以被引用的一些结论 (Key Takeaways)\n\n混合记忆是提升泛化性的有效途径: 通过从多个、异构的语料库中进行检索增强，可以显著提升稠密检索模型的零样本泛化能力，这是除了扩大模型参数之外的另一条有效路径。\n记忆的多样性至关重要: 实验表明，仅使用单一的外部记忆（即使是源领域的语料库）进行增强，对未见任务的泛化帮助有限，甚至可能产生负面影响。一个内容丰富且多样化的混合记忆池是成功的关键。\n联合学习+难负样本是必要条件: 简单的使用注意力分数进行软标签蒸馏不足以训练出强大的增强组件。必须将其与跨混合记忆库的难负样本挖掘相结合，才能让增强组件学会在多样化的信息源中进行有效检索。\n“即插即用”机制展现了强大的零样本适应能力: MoMA 机制允许在推理时动态地加入新的、与任务相关的语料库，模型无需重新训练就能立即利用这些领域内信息，这是一种非常实用和高效的领域自适应方法。\nMoMA-DR 性能卓越: 在包含 18 个任务的 BEIR 基准测试上，MoMA-DR 的零样本性能优于同等规模的其他稠密检索模型，并能达到与更大参数量模型或更复杂架构（如 ColBERT）相媲美的水平。\n\n","categories":["paper"],"tags":["paper","zero-shot","Dense Retrieval"]},{"title":"Atlas: Few-shot Learning with Retrieval Augmented Language Models","url":"/paper/Atlas-Few-shot-Learning-with-Retrieval-Augmented-Language-Models/","content":"问题 (Problem)\n传统的大型语言模型（LLMs）在少样本学习（few-shot learning）上表现出色，但这通常依赖于巨大的参数量来存储世界知识。这引发了一个核心问题：强大的少样本学习能力是否必须与庞大的模型参数（即内置记忆）绑定？\n这篇论文旨在探讨是否可以将模型的“记忆”（知识存储）与“推理”（泛化能力）解耦。作者假设，通过将知识存储外包给一个外部的、可检索的知识库，模型可以将更多参数用于学习推理和泛化能力，从而在拥有较少参数的情况下，在知识密集型任务（如问答、事实核查）上实现卓越的少样本学习性能。\n本文的目标是设计并训练一个精心构建的检索增强语言模型——ATLAS，验证其在知识密集型任务上，仅用少量样本就能超越巨大参数量模型的潜力。\n\n方法 (Method)\nATLAS 遵循一个统一的“文本到文本”（text-to-text）框架，其中所有任务都被建模为：输入一个文本查询（query），生成一个文本输出（output）。其核心是一个由**检索器（Retriever）和语言模型（Language Model）**组成的双模块架构。\n模型架构 (Architecture)\n当处理一个任务时，模型首先使用检索器从大规模语料库中召回 top-k 个最相关的文档，然后将这些文档连同原始查询一起输入语言模型，最终生成答案。\n\n\n检索器 (Retriever)：基于 Contriever 模型，这是一个使用双编码器（dual-encoder）架构的稠密检索器。\n\n它使用一个 Transformer 编码器独立地将查询（query）和文档（document）编码成向量（通过对最后一层输出进行平均池化）。\n查询和文档之间的相关性分数通过计算它们向量表示的点积得到。\nContriever 的一个优点是它可以通过无监督的对比学习进行预训练。\n\n\n\n语言模型 (Language Model)：基于 T5 (Seq2Seq) 架构，并采用了 Fusion-in-Decoder (FiD) 的思想。\n\n处理方式：为了高效处理多个文档，模型将原始查询与每个检索到的文档拼接，然后让编码器（Encoder）独立地处理每一个 [query, document] 对。\n融合方式：所有文档经过编码器后的表征（representations）被拼接（concatenate）成一个长序列。解码器（Decoder）在这个长序列上执行交叉注意力（cross-attention），从而在生成答案时融合所有文档的信息。\n优势：这种方式避免了将所有文档拼接成一个超长序列输入编码器，从而绕开了 Transformer 自注意力机制的二次方复杂度问题，扩展性更好。\n\n\n\n下图清晰地展示了 ATLAS 的工作流程：\n\n图1 解读：该图展示了 ATLAS 在预训练和少样本微调两个阶段的工作模式。\n\n预训练 (Pretraining)：以“遮盖语言建模 (Masked Language Modelling)”为例，模型输入一个带 &lt;MASK&gt; 的句子，然后从知识库中检索相关文档（例如关于“百慕大三角”的传说），最终目标是预测出被遮盖的词（“western part”）。\n少样本学习 (Few-shot)：以下游任务为例，如“事实核查”或“问答”。模型接收一个输入（如一个待核查的陈述或一个问题），检索相关文档，并基于输入和文档内容生成最终输出（“False” 或 “Western part of the North Atlantic Ocean”）。\n\n检索器的训练目标 (Training Objectives for the Retriever)\n本文的核心创新之一在于如何联合训练检索器和语言模型，特别是如何让语言模型为检索器提供监督信号，而无需人工标注的“查询-相关文档”对。论文探讨了四种损失函数：\n\n\n注意力蒸馏 (Attention Distillation, ADist)\n\n思想：语言模型解码器在生成答案时，对不同文档的交叉注意力分数可以被视为该文档重要性的代理指标。\n改进：传统的注意力蒸馏只考虑注意力权重 αn\\alpha_nαn​。本文提出，一个词元（token）的实际贡献度应同时考虑其注意力权重和其值向量（value vector）的范数，即 αn∣vn∣2\\alpha_n |v_n|_2αn​∣vn​∣2​。\n损失函数：将上述贡献度在所有头、层和词元上聚合，为每个文档计算一个总分，并通过 Softmax 得到目标概率分布 pATTNp_{ATTN}pATTN​。然后，最小化 pATTNp_{ATTN}pATTN​ 与检索器输出的概率分布 pRETRp_{RETR}pRETR​ 之间的 KL 散度。\n公式：\n\n检索器分布：pRETR(d∣q)=exp⁡(s(d,q)/θ)∑k=1Kexp⁡(s(dk,q)/θ)p_{RETR}(d|q) = \\frac{\\exp(s(d,q)/\\theta)}{\\sum_{k=1}^{K}\\exp(s(d_{k},q)/\\theta)}pRETR​(d∣q)=∑k=1K​exp(s(dk​,q)/θ)exp(s(d,q)/θ)​\n损失函数：L=KL(pATTN∣∣pRETR)=∑k=1KpATTN(dk)log⁡(pATTN(dk)pRETR(dk))\\mathcal{L} = KL(p_{ATTN} || p_{RETR}) = \\sum_{k=1}^{K} p_{ATTN}(d_k) \\log \\left( \\frac{p_{ATTN}(d_k)}{p_{RETR}(d_k)} \\right)L=KL(pATTN​∣∣pRETR​)=∑k=1K​pATTN​(dk​)log(pRETR​(dk​)pATTN​(dk​)​)\n\n\n注意：在计算时，会对 pATTNp_{ATTN}pATTN​ 施加 STOP_GRADIENT，确保损失只用于更新检索器参数。\n\n\n\n端到端训练 (EMDR²)\n\n思想：受期望最大化（EM）算法启发，将被检索的文档视为隐变量（latent variables）。\n损失函数：最大化生成正确答案 aaa 的边际对数似然。\n公式：L=log⁡[∑k=1KpLM(a∣q,dk)pRETR(dk∣q)]\\mathcal{L} = \\log\\left[\\sum_{k=1}^{K}p_{LM}(a|q,d_{k})p_{RETR}(d_{k}|q)\\right]L=log[∑k=1K​pLM​(a∣q,dk​)pRETR​(dk​∣q)]\n注意：同样对语言模型概率 pLMp_{LM}pLM​ 施加 STOP_GRADIENT。\n\n\n\n困惑度蒸馏 (Perplexity Distillation, PDist)\n\n思想：训练检索器去预测“哪个文档能最大程度地降低语言模型生成正确答案时的困惑度（Perplexity）”。\n损失函数：为每个文档 dkd_kdk​ 计算语言模型生成答案的对数概率 log⁡pLM(a∣dk,q)\\log p_{LM}(a|d_k, q)logpLM​(a∣dk​,q)，将其通过 Softmax 归一化后作为目标分布 yky_kyk​。然后最小化 yky_kyk​ 和 pRETRp_{RETR}pRETR​ 的 KL 散度。\n公式：\n\n目标分布：yk=exp⁡(log⁡pLM(a∣dk,q))∑i=1Kexp⁡(log⁡pLM(a∣di,q))y_{k} = \\frac{\\exp(\\log p_{LM}(a|d_{k},q))}{\\sum_{i=1}^{K}\\exp(\\log p_{LM}(a|d_{i},q))}yk​=∑i=1K​exp(logpLM​(a∣di​,q))exp(logpLM​(a∣dk​,q))​\n损失函数：L=KL(y∣∣pRETR)\\mathcal{L} = KL(y || p_{RETR})L=KL(y∣∣pRETR​)\n\n\n结论：实验表明 PDist 效果好、计算稳定，被选为最终模型使用的目标。\n\n\n\n留一法困惑度蒸馏 (LOOP)\n\n思想：一个文档的重要性体现在“当从上下文中移除该文档时，模型预测的性能会下降多少”。\n损失函数：对于每个文档 dkd_kdk​，计算在缺少它的情况下（即使用其他 K−1K-1K−1 个文档）生成答案的负对数概率 −log⁡pLM(a∣DK∖dk,q)-\\log p_{LM}(a|\\mathcal{D}_K \\setminus {d_k}, q)−logpLM​(a∣DK​∖dk​,q) 作为其重要性分数。将这些分数通过 Softmax 归一化得到目标分布 pLOOPp_{LOOP}pLOOP​，并最小化其与 pRETRp_{RETR}pRETR​ 的 KL 散度。\n公式：\n\n目标分布：pLOOP(dk)=exp⁡(−log⁡pLM(a∣DK\\dk,q))∑i=1Kexp⁡(−log⁡pLM(a∣DK\\di,q))p_{LOOP}(d_{k}) = \\frac{\\exp(-\\log p_{LM}(a|\\mathcal{D}_{K}\\backslash{d_{k}},q))}{\\sum_{i=1}^{K}\\exp(-\\log p_{LM}(a|\\mathcal{D}_{K}\\backslash{d_{i}},q))}pLOOP​(dk​)=∑i=1K​exp(−logpLM​(a∣DK​\\di​,q))exp(−logpLM​(a∣DK​\\dk​,q))​\n\n\n特点：计算成本更高，但其评估方式（使用 K-1 个文档）更接近语言模型的实际工作状态。\n\n\n\n预训练任务 (Pretext Tasks)\n为了让模型学习到如何利用检索到的知识，研究者们设计了三种无监督的联合预训练任务：\n\n前缀语言建模 (Prefix LM)：将一段文本一分为二，用前半段作为查询，预测后半段。\n遮盖语言建模 (Masked LM)：T5 风格的任务，随机遮盖文本中的一些片段（span），用带遮盖的文本作为查询，预测被遮盖的内容。实验表明这是效果最好的任务。\n标题到章节生成 (Title to section generation)：用维基百科文章的标题和章节标题作为查询，生成该章节的内容。\n\n高效的检索器微调 (Efficient Retriever Fine-tuning)\n在下游任务上微调时，如果检索器的文档编码器（document encoder）更新了，整个知识库的索引（数百万甚至上亿个文档的向量）都需要重新计算，这非常耗时。为此，论文提出了三种策略：\n\n全量索引更新 (Full index update)：每隔 R 个训练步就重新计算一次整个索引。开销大。\n重排序 (Re-ranking)：用旧的（stale）索引先检索出 L (L &gt; K) 个候选文档，然后用更新后的文档编码器只对这 L 个文档重新编码和排序，取 top-K 送给语言模型。开销显著降低。\n仅查询侧微调 (Query-side fine-tuning)：在微调时，冻结文档编码器的参数，只更新查询编码器（query encoder）。这样文档索引就无需更新。实验证明，这种方法在少样本场景下效果非常好，甚至能防止过拟合，是少样本微调的首选策略。\n\n\nBaseline (对比模型)\n\n闭卷模型 (Closed-book T5)：使用相同规模的 T5 模型，但不进行检索。这是证明检索增强有效性的关键对照组。\n其他大型语言模型：在少样本评测中，与 GPT-3 (175B)、Gopher (280B)、Chinchilla (70B)、PaLM (540B) 等参数量远超 ATLAS (11B) 的模型进行比较。\n其他检索增强模型：在全数据集（full-dataset）设定下，与 FiD、SEAL 等先进的检索模型进行比较。\n\n\n数据集 (Datasets)\n\n知识库/预训练语料：2021年12月的英文维基百科快照（3700万个段落）和 Common Crawl (CCNet) 网络文本（3.5亿个段落）。\n评测基准：\n\nMMLU：包含57个领域的人类考试多项选择题，用于评估模型的广博知识和推理能力。\nKILT：包含11个知识密集型任务的数据集集合，如 Natural Questions (NQ), TriviaQA, FEVER, Wizard of Wikipedia (WoW) 等。\n原始开放域问答集：原始版本的 NQ 和 TriviaQA。\nTempLAMA：一个专门构建的、用于测试模型对时间敏感信息处理能力的数据集，其中问题的答案会随时间变化（例如，2017年和2020年的答案不同）。\n\n\n\n\n可复现性 (Reproducibility)\n\n代码与模型：论文明确指出，代码、预训练的 ATLAS 模型检查点和相关数据都在 GitHub 上开源，可复现性非常高。\n\nGitHub: facebookresearch/atlas\n\n\n算力与配置：\n\n模型规模：从 770M 到 11B 参数不等，主要报告的是 11B 模型的结果。\n硬件需求：训练和运行 ATLAS 需要相当大的计算资源。特别是其知识库索引，即使在半精度（fp16）下，维基百科索引也需要 49GB 的 GPU 显存，而混合索引则需要 587GB。\n解决方案：论文展示了通过乘积量化 (Product Quantization) 技术可以大幅压缩索引。例如，混合索引可以从 587GB 压缩到 50GB，而下游任务性能下降很小，这使得在单张 80GB 的 GPU 上部署成为可能。\n\n\n\n图4 解读：该图展示了在 64-shot NQ 任务上，索引大小与模型性能的关系。\n\n\n上排：维基百科 + CC 混合索引。下排：仅维基百科索引。\n左列：检索召回率 (Recall@50)。右列：问答准确率 (Exact Match)。\n结论：可以将索引压缩一个数量级（例如从 ~500GB 压缩到 ~50GB，或从 ~50GB 压缩到 ~4GB），而召回率和最终准确率几乎没有损失。只有在极度压缩时，性能才会显著下降。\n\n\n可改进的几个点 (Potential Improvements)\n\n更复杂的推理模式：当前模型对于一次检索就能解决的问题表现优异，但对于需要**多步推理（multi-hop reasoning）**的复杂问题，一次性检索可能不足。未来的工作可以探索迭代式检索和推理的框架。\n检索与语言模型的融合方式：Fusion-in-Decoder 是一种有效的融合策略，但信息主要在解码器层面融合。探索更深层次的融合机制，让检索到的信息在模型的每一层都与原始查询进行交互，可能会带来性能提升。\n索引内容与结构：目前的索引是扁平的段落集合。引入结构化知识（如知识图谱）或不同粒度的文本（句子、篇章）可能会让检索更精准。\n训练信号的探索：尽管论文提出了四种为检索器提供监督信号的方法，但它们之间的性能差异不大。这表明可能还存在更优、更直接的训练范式有待发掘。\n\n\n可以被引用的一些结论 (Key Takeaways / Citable Conclusions)\n\n少样本学习的核心结论：通过检索增强，模型可以在参数量远小于传统 LLMs 的情况下，在知识密集型任务上取得SOTA（state-of-the-art）的少样本学习效果。记忆可以被有效外包。\n惊人的性能数据：\n\n在 Natural Questions (64-shot) 任务上，11B 参数的 ATLAS 准确率超过 42%，比 540B 参数的 PaLM 高出 3%，而参数量仅为其 1/50。\n在 TriviaQA (64-shot) 任务上，ATLAS 取得了 84.7% 的高准确率。\n\n\n联合预训练的重要性：检索器和语言模型的联合预训练是实现强大少样本能力的关键。只在微调阶段引入检索的模型性能远不如联合预训练过的模型。\n模型的可更新性 (Updatability)：ATLAS 展示了卓越的可更新能力。通过简单地更换知识库索引（例如，从2017年维基百科换成2020年），模型无需重新训练就能正确回答与时间相关的问题，这是纯参数模型无法做到的。\n高效微调策略：在少样本微调时，仅更新查询编码器 (Query-side fine-tuning) 是一种计算高效且性能优异的策略，它避免了昂贵的索引重建。\n可解释性与实用性：\n\n通过分析检索到的文档，可以理解模型的决策依据。例如，在 MMLU 任务上，当正确答案在检索文档中出现次数越多，模型准确率越高。\n通过索引压缩技术，可以显著降低模型的部署成本，使其更具实用价值。\n\n\n\n","categories":["paper"],"tags":["paper","few-shot","Retrieval","LLM"]},{"title":"ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction","url":"/paper/ColBERTv2-Effective-and-Efficient-Retrieval-via-Lightweight-Late-Interaction/","content":"问题 (Problem)\n传统的神经信息检索（Neural IR）模型主要分为两类，各自有明显的优缺点：\n\n\n单向量（Single-Vector）模型:\n\n工作方式: 将查询（query）和文档（document）分别编码成一个高维向量，然后通过计算这两个向量的点积来评估相关性。\n优点: 存储开销小，检索速度快。\n缺点: 表达能力有限且脆弱。模型需要将查询和文档的所有复杂语义关系压缩到一个单一的向量点积中，这对编码器提出了极高的要求。\n\n\n\n晚期交互（Late Interaction）模型 (如 ColBERTv1):\n\n工作方式: 将查询和文档的每个词元（token）都编码成一个向量，形成多向量表示。相关性计算分解为词元级别的计算，例如计算每个查询词元向量与所有文档词元向量的最大相似度之和 [: 20]。\n优点: 效果好，表达能力强，因为它将复杂的匹配任务交给了交互机制，减轻了编码器的负担。\n缺点: 空间占用巨大。由于需要为每个词元存储一个向量，其索引大小比单向量模型大一个数量级，这在网络规模的语料库上是难以接受的。\n\n\n\n核心问题: 如何在保持晚期交互模型强大效果（Effectiveness）的同时，解决其巨大的空间占用问题，提升其效率（Efficiency）？\n\n方法 (Method)\nColBERTv2 通过两大创新来解决上述问题：**降噪监督（Denoised Supervision）**用于提升模型质量，**残差压缩（Residual Compression）**用于减小空间占用。\n模型架构 (Modeling)\nColBERTv2 沿用了 ColBERT 的晚期交互架构。\n\n\n\n图例解读 (Figure 1)\n\n编码 (Encoding): 查询（Question）和文档（Passage）分别通过独立的BERT编码器（Question Encoder, Passage Encoder）。编码器的输出是每个词元（token）的上下文向量表示。\n交互 (Interaction): 交互过程是“晚期”的，即在编码之后发生。对于查询中的每一个词元向量，都会计算它与文档中所有词元向量的相似度，并取其中的最大值（MaxSim操作）。\n评分 (Scoring): 最终的查询-文档相关性分数是所有查询词元的 MaxSim 分数之和（Σ 操作）。\n\n\n\n数学公式解读\n模型的核心评分公式如下:\nSq,d=∑i=1Nmaxj=1MQi⋅DjTS_{q,d}=\\sum_{i=1}^{N}max_{j=1}^{M}Q_{i}\\cdot D_{j}^{T}\nSq,d​=i=1∑N​maxj=1M​Qi​⋅DjT​\n其中：\n\nQQQ 是一个 N×dN \\times dN×d 的矩阵，代表查询 qqq 的 NNN 个词元向量。\nDDD 是一个 M×dM \\times dM×d 的矩阵，代表文档 ddd 的 MMM 个词元向量。\nQiQ_iQi​ 是查询的第 iii 个词元向量。\nmaxj=1MQi⋅DjTmax_{j=1}^{M}Q_{i}\\cdot D_{j}^{T}maxj=1M​Qi​⋅DjT​ 计算的是查询词元 QiQ_iQi​ 与文档中所有词元向量的最大余弦相似度。\n∑i=1N\\sum_{i=1}^{N}∑i=1N​ 将每个查询词元找到的最佳匹配分数相加，得到最终总分。\n\n\n\n降噪监督 (Denoised Supervision)\n为了提升模型质量，超越那些经过高度优化的单向量模型，ColBERTv2 采用了一种更强的监督策略。\n\n生成高质量标签: 使用一个初步训练好的 ColBERT 模型从训练集中为每个查询检索 top-k 个候选段落。\n教师模型重排: 将这些（查询，段落）对送入一个更强大但更慢的交叉编码器（cross-encoder）教师模型进行重排序，得到更精确的相关性分数。\n知识蒸馏: 使用 KL 散度损失函数，将交叉编码器的分数蒸馏到 ColBERTv2 的架构中。这使得 ColBERTv2 能学习到教师模型的精细判断能力。\n难负例挖掘: 训练过程中使用的负例是经过初步模型和教师模型筛选后的高分负例（hard negatives），这使得训练更具挑战性也更有效。\n\n残差压缩 (Residual Compression)\n这是解决空间占用的核心技术。其基本假设是：特定词元在不同上下文中的向量表示会聚集在语义空间的特定区域内。\n\n聚类中心: 首先，对样本向量进行 k-means 聚类，得到一组聚类中心（centroids）CCC。\n编码: 对于语料库中的任意一个词元向量 vvv，它的压缩表示由两部分组成：\n\n离它最近的聚类中心的索引 ttt。\n残差向量 r=v−Ctr = v - C_tr=v−Ct​ 的量化版本 r~\\tilde{r}r~。残差的每个维度被量化为1或2个比特。\n\n\n解码: 在检索时，通过 v~=Ct+r~\\tilde{v} = C_t + \\tilde{r}v~=Ct​+r~ 来近似重构原始向量。\n\n对于 b−bitb-bitb−bit 编码的 nnn 维向量，每个向量需要 $\\left \\lceil log|C| \\right \\rceil + bn $ bits的空间。\n\n\n\n通过这种方式，原本需要256字节（128维，16位精度）存储的向量，现在只需要20或36字节（4字节索引 + 16/32字节量化残差），实现了6-10倍的空间压缩。\n\nBaseline (基线模型)\n论文与多种先进的检索模型进行了比较，可以分为几类：\n\n单向量模型:\n\n无蒸馏: RepBERT, DPR, ANCE。\n有蒸馏/特殊预训练: TAS-B, PAIR, coCondenser, RocketQAv2。\n\n\n稀疏/词汇模型: SPLADEv2，这是一个非常强的基线，它也将相关性计算分解到词元级别，但生成的是稀疏的词汇权重向量。\n晚期交互模型: ColBERT (v1)。\n传统模型: BM25。\n\n\n数据集 (Datasets)\n论文在多种数据集上进行了广泛的评估，以验证模型的性能和泛化能力。\n\n\n训练:\n\nMS MARCO Passage Ranking: 业界标准的 IR 训练和评测数据集。\n\n\n\n域内（In-Domain）评测:\n\nMS MARCO Dev Set: 在训练数据所在的领域进行评测。\n\n\n\n域外（Out-of-Domain）评测:\n\nBEIR: 一个包含13个不同领域（如生物医学、金融、科学）检索任务的异构评测基准，用于测试模型的零样本（zero-shot）泛化能力。\nWikipedia Open-QA: 包括 Natural Questions (NQ), TriviaQA (TQ), SQUAD 数据集，用于评测开放域问答的段落检索任务。\nLoTTE (论文新贡献): 全称 Long-Tail Topic-stratified Evaluation，是一个专注于长尾主题和自然用户查询的新评测基准。它源于 StackExchange 社区，包含 “Search” (来自Google搜索) 和&quot;Forum&quot; (来自帖子标题) 两种查询类型，覆盖写作、科技、生活等5大领域。\n\n\n\n\n可复现性 (Reproducibility)\n\n代码/模型: 论文明确指出代码、模型和 LoTTE 数据集都在 GitHub 上开源维护，这为复现工作提供了极大的便利。\n算力:\n\n训练: 需要4块80GB的A100 GPU，训练约5天。\n推理: 使用4块12GB的Titan V GPU。\n总计: 作者估计整个项目（包括开发、实验、评估）大约消耗了20个GPU月。\n结论: 复现单个模型的训练对于有高端GPU资源的实验室或公司是可行的，但完成所有实验的成本较高。\n\n\n\n\n可改进的点 (Potential Improvements)\n论文在结尾处也探讨了未来可能的研究方向：\n\n多语言支持: 目前所有实验均在英文上进行。将该方法扩展并验证到其他语言是一个重要的方向。\n训练数据依赖: 所有域外评测的模型都是在 MS MARCO 上训练的。在其他（尤其是较小的）数据集上训练时的表现有待探索。\n更优的压缩算法: 论文采用了相对简单的残差压缩。未来可以探索更复杂的量化或压缩技术，以期在不损失质量的前提下获得更高的压缩率。\n训练流程简化: 带有知识蒸馏的训练流程比原版 ColBERT 更复杂、成本更高。研究如何简化训练流程，降低训练成本，将是一个有价值的方向。\n系统级优化: 当前实现是基于Python和PyTorch。通过C++或CUDA进行底层优化，可以进一步降低检索延迟。\n\n\n可引用的结论 (Citable Conclusions)\n这篇论文提供了一些非常有价值和可引用的结论：\n\n晚期交互模型的潜力: 通过降噪监督（知识蒸馏和难负例挖掘），晚期交互模型的性能可以被大幅提升，显著超越了当前最先进的单向量模型和稀疏模型。\n效率与效果的统一: ColBERTv2 提出的残差压缩机制，可以在几乎不损失模型检索质量的情况下，将索引大小减少6-10倍，成功解决了晚期交互模型的空间占用瓶颈，使其在存储上与单向量模型相当。\n强大的泛化能力: 无论是在域内（MS MARCO）还是在多个域外基准（BEIR, Open-QA, LoTTE）上，ColBERTv2 都取得了SOTA（state-of-the-art）的成绩，证明了其架构和训练方法的鲁棒性和泛化能力。\n词元表示的语义结构: 实验分析表明，ColBERT 的词元级向量天然具有语义聚集性，即同一词语的不同“意义”会对应到向量空间中的不同簇，这是残差压缩有效性的根本原因。\n长尾主题检索的重要性: 论文新提出的LoTTE数据集，填补了现有评测基准在长尾、专业领域自然查询方面的空白，为未来检索模型的泛化能力评测提供了新的视角和资源。\n\n","categories":["paper"],"tags":["paper","Retrieval","Neural IR"]},{"title":"DynaGRAG | Exploring the Topology of Information for Advancing Language Understanding and Generation in Graph Retrieval-Augmented Generation","url":"/paper/DynaGRAG-Exploring-the-Topology-of-Information-for-Advancing-Language-Understanding-and-Generation-in-Graph-Retrieval-Augmented-Generation/","content":"问题 (Problem)\n大型语言模型 (LLM) 虽然强大，但在处理需要复杂推理、动态演化知识的任务时仍存在局限性。检索增强生成 (RAG) 旨在通过引入外部知识来弥补这一不足，但传统的 RAG 难以有效利用结构化数据。\n现有的图增强RAG (Graph RAG) 方法也存在一些问题：\n\n信息粒度损失：一些方法（如微软的GraphRAG）依赖于对图社群进行预先摘要，这种方式牺牲了图的粒度和灵活性。当底层图结构发生变化时，需要重新生成整个索引和摘要，适应性差且计算成本高。\n兼容性与焦点局限：另一些方法在与主流LLM架构的兼容性上存在挑战，或者过于关注特定任务（如多跳推理或事实准确性），而忽略了捕捉实体关系的多样性。\n\n因此，核心挑战在于：如何有效捕捉并融合知识图谱中丰富的语义信息和拓扑结构，以增强LLM的语言理解和生成能力，使其能够进行更深层次、更具上下文感知能力的推理。\n方法 (Method)\n为解决上述问题，该论文提出了一个名为 DynaGRAG (Dynamic Graph Retrieval-Augmented Generation) 的新框架。其核心思想是保留图的原生结构，在查询时进行实时遍历和检索，并动态地构建和优化信息，最终生成高质量的回复。\n其工作流程分为两个主要阶段：索引阶段 (Indexing Phase) 和 查询阶段 (Query Phase)。\n\nDynaGRAG 流程图 (论文 Figure 4)\n索引阶段 (Indexing Phase)\n\n\n知识图谱构建:\n\n首先将源文档分割成文本块 (Text Chunks)。\n利用 LLM 从文本块中提取关键实体 (Entities)、实体摘要及其之间的关系 (Relations)。\n基于提取的信息构建一个初始的知识图谱。\n\n\n\n图整合 (Graph Consolidation):\n\n这是一个关键创新，旨在提升图的密度和表示质量。\n去重与合并: 利用LLM分析实体标签和上下文的语义相似性，将同义或高度相似的实体（如 “compute”、“compute resources”）合并为一个节点。\n两步均值池化 (Two-step Mean Pooling): 为了在合并实体时保留信息的多样性，该方法首先对完全相同的实体的嵌入 (Embeddings) 进行平均，然后再对语义相似的实体的嵌入进行平均。这确保了每个独特的实体表示被精确捕捉，同时避免了高频实体的主导，保留了丰富的潜在特征。\n\n\n\n子图编码 (Subgraph Encoding):\n\n为图中的每个节点预计算一个以其为中心的 自我图 (ego-graph)，通常包含其k跳邻居（论文中 k=3）。\n为了将每个子图表示为一个向量，该方法计算了加权的节点和边特征，并最终聚合成一个总图嵌入。\n加权节点特征 (hweightedh_{weighted}hweighted​):\n\nhweighted=hi⋅wih_{weighted} = h_i \\cdot w_i\nhweighted​=hi​⋅wi​\n其中 hih_ihi​ 是节点 iii 的嵌入， wiw_iwi​ 是其权重（如出现频率）。\n\n加权边特征 (eijweightede_{ij}^{weighted}eijweighted​):\n\neijweighted=hi+hk+rijn⋅wije_{ij}^{weighted} = \\frac{h_i + h_k + r_{ij}}{n} \\cdot w_{ij}\neijweighted​=nhi​+hk​+rij​​⋅wij​\n其中 hih_ihi​ 和 hkh_khk​ 分别是源节点和目标节点的嵌入， rijr_{ij}rij​ 是关系的嵌入， wijw_{ij}wij​ 是边的权重， nnn 是被平均的嵌入数量（此处为3）。\n\n总图嵌入 (ggg):\n\ng=∑ihiweighted+∑(i,j)eijweightedwtotalg = \\frac{\\sum_i h_i^{weighted} + \\sum_{(i,j)} e_{ij}^{weighted}}{w_{total}}\ng=wtotal​∑i​hiweighted​+∑(i,j)​eijweighted​​\n其中 wtotalw_{total}wtotal​ 是所有节点和边权重的总和。这个嵌入向量代表了整个子图的结构和语义信息。\n\n\n存入向量数据库 (Vector DB):\n\n将所有子图的嵌入向量存入向量数据库，以便快速检索。\n\n\n\n查询阶段 (Query Phase)\n\n\n子图检索 (Subgraph Retrieval):\n\n将用户查询也转换为嵌入向量。\n在向量数据库中，使用余弦相似度计算查询与所有预编码子图的相似度，并检索出 Top-N 个最相关的子图。\n多样性优先机制: 为了避免检索到的子图过于同质化，该机制会追踪已检索子图中的核心节点。当添加新的子图时，会优先选择那些包含不同核心实体的子图，从而确保结果覆盖知识图谱的不同区域，提供更丰富的视角。\n\n\n\n剪枝 (Pruning):\n\n对检索到的子图进行精炼，以突出与查询最相关的部分。\n计算显着性分数: 该分数结合了结构重要性（预先计算的权重）和语义对齐度。\n语义对齐度通过计算查询嵌入与节点/边嵌入之间的欧氏距离来衡量：\n\n节点-查询距离: Distancenode=∣∣hi−q∣∣2=∑j=1d(hi,j−qj)2Distance_{node} = ||h_i - q||_2 = \\sqrt{\\sum_{j=1}^{d}(h_{i,j}-q_{j})^2}Distancenode​=∣∣hi​−q∣∣2​=∑j=1d​(hi,j​−qj​)2​\n边-查询距离: Distanceedge=∣∣rij−q∣∣2=∑j=1d(rij,j−qj)2Distance_{edge} = ||r_{ij} - q||_2 = \\sqrt{\\sum_{j=1}^{d}(r_{ij,j}-q_{j})^2}Distanceedge​=∣∣rij​−q∣∣2​=∑j=1d​(rij,j​−qj​)2​\n\n\n图卷积网络 (GCN): 将初始的显着性分数输入GCN。GCN通过其消息传递机制，结合图的拓扑结构来动态更新和优化每个节点和边的重要性分数。\n软掩码 (Soft Masking): 框架并不直接删除低分数的组件，而是使用“软掩码”根据其剪枝分数来缩放其权重，从而降低其影响力但仍保留其信息。\n\n\n\n层级化提示生成 (Hierarchical Prompt Generation):\n\n动态相似度感知BFS (DSA-BFS) 算法: 这是一种新颖的图遍历算法。它在传统的广度优先搜索 (BFS) 的基础上，根据节点与查询的相似度得分动态调整探索顺序，优先访问与查询更相关的邻居。这有助于发现传统BFS可能错过的深层联系。\n通过前序遍历 (Pre-order Traversal) 算法收集剪枝后子图的实体摘要、关系和权重，并将其组织成一个具有层级结构的字符串（类似于JSON或XML格式）。\n这种结构化的硬提示 (Hard Prompt) 能够充分利用LLM在处理层级数据方面的优势。\n\n\n\n生成回复 (Generating Responses):\n\n中间回复: 首先，为每个检索并精炼后的子图生成一个“中间回复”，每个回复都从不同角度切入问题。\n排序与整合: 系统为每个中间回复计算一个“帮助性分数”，并按降序排列。\n最终回复: 将排序后的中间回复送入LLM，生成一个连贯、全面且无冗余的最终答案。这个过程鼓励LLM在形成最终意见前先进行“思考”。\n\n\n\nBaseline\n实验对比了三种不同的架构：\n\nLLM Pipeline (Vanilla LLM): 标准LLM，直接将查询输入给 GPT-4o Mini 或 Gemini 1.5 Flash，不提供任何外部上下文。\nLLM with RAG (Naïve RAG): 一个朴素的RAG实现。将播客文稿分块并向量化，检索与查询最相关的5个文本块作为上下文提供给LLM。\nDynaGRAG: 本文提出的框架。\n\n数据集 (Dataset)\n\n数据源: 2024年度 Dwarkesh Patel 播客的所有文字记录。\n规模: 共计 460k tokens。\n查询集: 人工生成了180个非事实性的、需要推理能力的问题，以全面评估模型的性能。\n\n可复现性 (Reproducibility)\n\n代码: 论文未提供公开的代码库。\n算力与限制: 作者提到，作为一名独立开发者，在进行大规模实验时遇到了显著的计算瓶颈。\n\n直接与微软的 GraphRAG 在 500k token 规模上进行比较非常困难，因为开源实现受到 Groq 等平台的 token 限制，或在 Ollama 上运行速度过慢。\n这表明，复现完整的实验或将其扩展到更大规模的数据集需要强大的计算资源。\n不过，论文也指出，DynaGRAG 处理一次查询大约需要一分钟，在无需额外微调 LLM 的情况下，这体现了其在实际应用中的高效性和成本效益。\n\n\n\n可改进的几个点 (Areas for Improvement)\n\n与SOTA模型的直接对比: 由于算力限制，未能与微软的 GraphRAG 等业界领先的 Graph RAG 框架进行直接比较。未来可以在一个较小的数据集上（如 100k tokens）进行更全面的基准测试。\n大规模稀疏图的适应性: DynaGRAG 在连接紧密的稠密图上表现优异。然而，当扩展到更大、更稀疏的图时，节点间的连接可能被“稀释”，从而影响信息合成的效果。未来需要研究如何在大规模数据上保持图的密度。\n层级化子图: 进一步研究如何构建层级化的子图表示，可能有助于提升模型的可扩展性和上下文理解能力。\n评估指标的局限性: 论文承认，现有的评估指标（如连贯性、相关性）难以完全捕捉到回答中“推理的深度和质量”。这是一个更广泛的领域挑战。\n\n可以被引用的一些结论 (Citable Conclusions)\n\n性能优势: DynaGRAG 在生成全面、多样化且有深度的回答方面，显著优于标准LLM和朴素RAG方法，尤其在处理需要复杂推理的查询时。\n图结构的重要性: 保持图的原生结构，并进行动态、实时的检索与剪枝，比依赖预先摘要或简单的文本块检索更有效。\n方法创新点的有效性: 框架中的多个创新点，如两步均值池化、多样性优先检索和动态相似度感知BFS算法，被证明能有效提升子图表示质量和连接发现能力。\n与LLM的协同: 通过生成层级化的硬提示，DynaGRAG 能够有效利用LLM处理结构化数据的内在优势，实现更好的协同工作。\n伦理对齐的自然涌现: 该研究表明，强大的推理能力可以自然地引导出符合伦理和上下文的输出，而无需进行明确的指令或微调。DynaGRAG通过整合多样的观点及其影响，能够生成更负责任的AI回复。\n实用性与效率: 该框架在无需昂贵的LLM微调的情况下，能在一分钟左右处理复杂查询，展示了其在真实世界应用中的潜力和成本效益。\n\n","categories":["paper"],"tags":["paper","LLM","GRAG"]},{"title":"GER: GENERATION, EVALUATION AND REFLECTION ENHANCED LLM FOR KNOWLEDGE GRAPH QUESTION ANSWERING","url":"/paper/GER-GENERATION-EVALUATION-AND-REFLECTION-ENHANCED-LLM-FOR-KNOWLEDGE-GRAPH-QUESTION-ANSWERING/","content":"问题 (Problem)\n这篇论文致力于解决知识图谱问答（KGQA）中的一个核心挑战：大型语言模型（LLM）的幻觉问题。\n传统的知识图谱问答方法，如图神经网络（GNN）为基础的方法，虽然能很好地适应图结构，但在理解自然语言问题的深层意图方面能力有限。近年来，LLM凭借其强大的自然语言理解能力被引入KGQA领域，并取得了一定的成功。\n然而，现有方法普遍忽略了一个严重问题：当LLM与庞大的知识图谱（KG）结合时，KG中包含的大量不相关信息会放大LLM的幻觉。这导致模型会生成一些看似正确但实际上与事实不符的答案，极大地降低了问答系统的可靠性。\n因此，本文要解决的核心问题是：如何设计一个框架，能够系统性地减少由KG中无关信息引发的LLM幻觉，从而提升KGQA任务的准确性和可靠性。\n方法 (Method)\n为了解决上述问题，作者提出了一个名为GER（Generation-Evaluation-Reflection，生成-评估-反思） 的LLM增强反思性推理框架。该框架通过在传统的“生成”之后引入“评估”和“反思”两个关键步骤，让LLM能够利用KG中的事实信息来审视和修正自己的答案。\n整体框架\nGER框架包含三个核心模块，如下图所示：\n\n\n①生成 (Generation): 首先，给定一个问题，LLM会探索知识图谱，生成初步的候选答案和推理路径。这些初步答案可能包含错误和幻觉。\n②评估 (Evaluation): 接着，评估模块会对生成的候选答案进行严格审查。这一步是GER的核心创新，它采用双粒度评估策略来判断答案的完整性和正确性。\n③反思 (Reflection): 最后，根据评估模块提供的反馈（Feedback），反思模块会引导LLM过滤掉幻觉内容，探索被遗漏的正确答案，并重新生成最终的、更可靠的答案。\n\n数学形式化\n作者将GER框架形式化为一个优化问题，目标是最大化在给定问题 qqq 和知识图谱 G\\mathcal{G}G 的情况下，生成正确答案 aaa 的概率。这个过程通过对所有可能的初始答案 a′a^{\\prime}a′ 和评估结果 e′e^{\\prime}e′ 进行边缘化来实现：\nPθ(a∣q,G)=∑a′∑e′Pgen(a′∣q,G;θ)Peval(e′∣q,G,a′;θ)Pref(a∣q,G,a′,e′;θ)P_{\\theta}(a|q,\\mathcal{G})=\\sum_{a^{\\prime}}\\sum_{e^{\\prime}}P_{gen}(a^{\\prime}|q,\\mathcal{G};\\theta)P_{eval}(e^{\\prime}|q,\\mathcal{G},a^{\\prime};\\theta)P_{ref}(a|q,\\mathcal{G},a^{\\prime},e^{\\prime};\\theta)\nPθ​(a∣q,G)=a′∑​e′∑​Pgen​(a′∣q,G;θ)Peval​(e′∣q,G,a′;θ)Pref​(a∣q,G,a′,e′;θ)\n其中：\n\nPgen(a′∣q,G;θ)P_{gen}(a^{\\prime}|q,\\mathcal{G};\\theta)Pgen​(a′∣q,G;θ): 生成模块，表示在给定问题和KG的情况下，生成初始答案 a′a^{\\prime}a′ 的概率。\nPeval(e′∣q,G,a′;θ)P_{eval}(e^{\\prime}|q,\\mathcal{G},a^{\\prime};\\theta)Peval​(e′∣q,G,a′;θ): 评估模块，表示对初始答案 a′a^{\\prime}a′ 进行评估后，得到评估结果 e′e^{\\prime}e′ 的概率。\nPref(a∣q,G,a′,e′;θ)P_{ref}(a|q,\\mathcal{G},a^{\\prime},e^{\\prime};\\theta)Pref​(a∣q,G,a′,e′;θ): 反思模块，表示结合初始答案 a′a^{\\prime}a′ 和评估结果 e′e^{\\prime}e′，生成最终答案 aaa 的概率。\n\n模块详解\nA. 生成模块 (Generation Module)\n该模块的目标是生成候选答案。它通过设计提示（Prompt）来引导LLM：\n\n生成关系路径：首先，LLM被要求为问题生成一个可能有效的关系路径，如 &quot;&lt;PATH&gt; relation_1 &lt;SEP&gt; relation_2 &lt;/PATH&gt;&quot;。\n检索推理路径：根据生成的关系路径，在知识图谱 G\\mathcal{G}G 中从问题实体出发进行检索，得到包含实体和关系的完整推理路径 Wz\\mathcal{W}_{z}Wz​。\n生成初始答案：基于检索到的推理路径 Wz\\mathcal{W}_{z}Wz​，再次引导LLM生成初步的候选答案。\n\nB. 评估模块 (Evaluation Module)\n这是GER框架的关键，它通过双粒度评估来确保答案的质量：\n\n\n问题级评估 (Question-level Evaluation):\n\n目的: 评估所有候选答案作为一个整体是否完整，即有没有遗漏正确答案。\n方法: 将问题、推理路径和所有候选答案一起输入LLM，让其判断答案集合是否完整，并给出简要理由。\n形式化:arg⁡max⁡θ1n∑i=1nlog⁡Pθ(ai∣q,G)\\arg\\max_{\\theta}\\frac{1}{n}\\sum_{i=1}^{n}\\log P_{\\theta}(a_{i}|q,\\mathcal{G})\nargθmax​n1​i=1∑n​logPθ​(ai​∣q,G)\n这个公式旨在最大化所有候选答案 aia_iai​ 在给定问题 qqq 和KG G\\mathcal{G}G 下的联合概率，以此来衡量答案集的整体完备性。\n\n\n\n答案级评估 (Answer-level Evaluation):\n\n目的: 评估单个答案的推理过程是否合理，过滤掉不合逻辑的幻觉答案。\n方法: 将问题、单个推理路径和与之对应的单个答案输入LLM，让其判断该答案是否正确。\n形式化:arg⁡max⁡θmax⁡log⁡Pθ(a∣q,r,G)\\arg\\max_{\\theta}\\max\\log P_{\\theta}(a|q,r,\\mathcal{G})\nargθmax​maxlogPθ​(a∣q,r,G)\n这个公式旨在最大化单个答案 aaa 在给定问题 qqq、特定推理路径 rrr 和KG G\\mathcal{G}G 下的正确概率，以此来确保每个答案的逻辑准确性。\n\n\n\nC. 反思模块 (Reflection Module)\n该模块利用评估模块的反馈来优化答案。\n\n目的: 消除幻觉，并根据评估反馈找到被遗漏的正确答案。\n方法: 论文提出了一种基于反馈的训练范式。将问题、推理路径、初始答案以及评估模块给出的反馈（例如：“不完整”或“不正确”）一并输入LLM，引导它生成一个修正后的最终答案。\n形式化:arg⁡max⁡log⁡Pθ(a∣q,G,a′,e′)\\arg\\max\\log P_{\\theta}(a|q,\\mathcal{G},a^{\\prime},e^{\\prime})\nargmaxlogPθ​(a∣q,G,a′,e′)\n这个公式旨在最大化最终答案 aaa 的生成概率，其条件是初始答案 a′a^{\\prime}a′ 和评估反馈 e′e^{\\prime}e′，从而使模型学会如何根据反馈进行修正。\n\n案例分析\n论文中的图例生动地展示了GER如何修正错误：\n\n\n案例1: 对于问题“安娜·布莱代表哪个选区？”，基线模型Rog生成了错误的推理路径，得出了错误答案“西澳大利亚州”。而GER通过评估模块识别出这个答案是错误的，然后反思模块根据反馈探索了正确的推理路径，最终得到正确答案“南布里斯班选区”。\n案例2: 对于问题“哈珀·李上的是哪所高中？”，基线模型Rog遗漏了正确的推理路径。GER的评估模块识别出答案不完整，反思模块捕捉到了被遗漏的路径，最终给出了正确答案“门罗县高中”。\n\nBaseline (对比模型)\nGER与多种类型的现有方法进行了比较，主要分为三类：\n\nGNNs: 纯粹基于图神经网络的方法，如 EmbedKGQA, NSM, GraftNet 等。\nLLMs: 仅使用大语言模型的方法，如 Flan-T5-xl, Alpaca-7B, LLaMA2-Chat-7B, ChatGPT (包含CoT) 等。\nLLMs+KGs: 结合大模型和知识图谱的混合方法，也是本文最重要的比较对象。包括 UniKGQA, ToG, EtD 以及被认为是当前最先进的 Rog 模型。\n\n数据集 (Datasets)\n实验在两个广泛使用的KGQA基准数据集上进行：\n\nWebQuestionSP (WebQSP): 包含2,826个训练问题和1,628个测试问题。\nComplex WebQuestions (CWQ): 规模更大，更具挑战性，包含27,639个训练问题和3,531个测试问题。\n\n这两个数据集都使用 Freebase 作为其底层的知识图谱。评估指标采用 Hits@1 (Top-1预测的准确率) 和 F1 (综合考虑预测的精确率和召回率)。\n可复现性 (Reproducibility)\n\n代码: 论文中未提及是否开源代码。\n算力: 实验使用的基础模型是 LLAMA2-Chat-7B。作者在WebQSP和CWQ的训练集上进行了3个epoch的指令微调（instruction fine-tuning）。这意味着复现该工作需要能够微调7B参数量级模型的计算资源（例如，高端多GPU服务器）。\n\n可改进的几个点 (Potential Improvements)\n论文在结论部分也坦诚地指出了当前方法的局限性，这些也是未来可以改进的方向：\n\n对知识图谱质量的依赖: GER的性能受限于底层KG的完整性和准确性。如果KG数据稀疏或存在错误，模型的表现会下降。\n计算复杂度: 引入额外的评估和反思步骤增加了计算开销，对于复杂查询可能会影响响应时间和可扩展性。\n算法效率: 未来的工作可以探索更高效的算法来降低GER框架的计算成本。\n处理不完整数据: 需要研究更有效的策略来应对知识图谱数据不完整的情况。\n\n可以被引用的一些结论 (Citable Conclusions)\n\n核心发现: 简单地将LLM与KG结合会因KG中的海量无关信息而加剧LLM的幻觉问题。\n方法论贡献: 显式地引入“评估”和“反思”机制，可以系统性地减少KGQA中的错误与幻觉，是提升LLM在知识驱动任务中可靠性的有效途径。\n性能突破: GER框架在WebQSP和CWQ两个主流KGQA数据集上均取得了新的SOTA（State-of-the-art）性能。相比之前的最佳模型Rog，在WebQSP上Hits@1提升5.0%，在CWQ上Hits@1提升6.8%。\n评估策略的有效性: 双粒度评估策略（问题级评估答案完整性，答案级评估单点正确性）对于提升LLM问答的可靠性和完整性至关重要。\n\n","categories":["paper"],"tags":["paper","LLM","KG"]},{"title":"GRAG: Graph Retrieval-Augmented Generation","url":"/paper/GRAG-Graph-Retrieval-Augmented-Generation/","content":"问题 (Problem)\n传统的“朴素”检索增强生成（RAG）在处理单个文档时表现良好，但现实世界中的许多数据，如引文网络、社交媒体、知识图谱等，都以相互连接的图（Graph）形式存在。朴素 RAG 的局限性在于：\n\n忽略拓扑信息：它只关注文档的文本内容，忽略了文档之间的连接关系（如引用、链接、关系），而这些连接关系对于深度理解和推理至关重要。\n检索单元受限：它以独立的文档作为检索单元，无法检索出一个由多个相关实体及其关系组成的“子图”上下文。\n\n\n因此，核心问题是：如何让大型语言模型（LLM）在执行 RAG 时，能够有效地利用这种网络化的图结构数据，从而同时理解文本内容和拓扑结构，以生成更准确、更具上下文感知能力的答案？\n这带来了两个核心挑战：\n\n检索挑战：如何从一个大规模的文本图中高效地检索出与问题最相关的文本子图（textual subgraph）？穷举搜索所有可能的子图是一个 NP-hard 问题。\n生成挑战：如何将检索到的子图所包含的**联合文本与拓扑信息（joint textual and topological information）**有效地融入到 LLM 中，以指导其生成过程？\n\n方法 (Method)\n为了解决上述问题，作者提出了 GRAG（Graph Retrieval-Augmented Generation）框架。该框架包含两个核心阶段：文本子图检索和文本图增强生成。\n文本子图检索 (Textual Subgraph Retrieval)\n为了避免 NP-hard 的穷举搜索，GRAG 采用了一种创新的“分治”（divide-and-conquer）策略来近似寻找最优子图。其核心思想是，一个重要的子图是由一些重要的“关键节点”及其邻域组成的。\n该过程可以分解为三个步骤，如下图 a 部分所示：\n\n(图例参考：原论文 Figure 2)\n\n\n索引 (Indexing)：\n\n思想：将对整个图的子图搜索问题，简化为对每个节点的“自我中心图”（ego-graph）的搜索问题。\n实现：对于图中的每一个节点 v，提取其 K-hop 邻域内的所有节点和边，构成一个 K-hop ego-graph，记为 G[NK(v)]G[\\mathcal{N}_{K}(v)]G[NK​(v)]。\n使用预训练语言模型（PLM，如 SentenceBERT）将每个 ego-graph 中所有节点和边的文本属性编码为向量，然后通过平均池化（mean pooling）操作，为每个 ego-graph 生成一个唯一的图嵌入向量 zgz_gzg​。zg=POOL(PLM({Tn}n∈Vg,{Te}e∈Eg))z_{g}=POOL(PLM(\\{T_{n}\\}_{n\\in V_{g}},\\{T_{e}\\}_{e\\in E_{g}}))\nzg​=POOL(PLM({Tn​}n∈Vg​​,{Te​}e∈Eg​​))\n\n所有 ego-graph 的嵌入向量被存储起来，建立索引。\n\n\n\n排行 (Ranking)：\n\n使用相同的 PLM 将用户查询 q 编码为查询向量 zqz_qzq​。\n计算查询向量 zqz_qzq​ 与索引中所有 ego-graph 嵌入向量 zgz_gzg​ 之间的余弦相似度。\n选出相似度最高的 Top-N 个 ego-graphs，作为候选子图。SN(G)=Top_Ng∈S(G)cos(zq,zg)\\mathcal{S}_{N}(G)=Top\\_N_{g \\in S(G)} cos(z_{q},z_{g})\nSN​(G)=Top_Ng∈S(G)​cos(zq​,zg​)\n\n\n\n\n软剪枝 (Soft Pruning) 与合并 (Merging)：\n\n动机：检索到的 ego-graphs 中可能仍包含与查询不相关的节点和边，这些冗余信息会干扰 LLM 的生成。\n实现：引入一个可学习的“软剪枝”机制。使用两个多层感知机（MLP）分别计算每个节点/边与查询 q 之间的距离，并生成一个缩放因子 α\\alphaα（值域 0-1）。zn=PLM(Tn),αn=MLPϕ1(zn⊖zq)z_{n}=PLM(T_{n}), \\alpha_{n}=MLP_{\\phi_{1}}(z_{n}\\ominus z_{q})\nzn​=PLM(Tn​),αn​=MLPϕ1​​(zn​⊖zq​)\nze=PLM(Te),αe=MLPϕ2(ze⊖zq)z_{e}=PLM(T_{e}), \\alpha_{e}=MLP_{\\phi_{2}}(z_{e}\\ominus z_{q})\nze​=PLM(Te​),αe​=MLPϕ2​​(ze​⊖zq​)\n其中 ⊖\\ominus⊖ 表示用于测量逐元素距离的运算符\n这个因子 α\\alphaα 可以理解为一个“重要性”或“相关性”权重。距离查询越远的实体，其 α\\alphaα 值越接近 0，在后续处理中其影响力会被减弱。\n最后，将这 N 个经过软剪枝“加权”的 ego-graphs 合并起来，形成最终提供给 LLM 的近似最优子图 g^\\hat{g}g^​。\n\n\n\n文本图增强生成 (Textual Graph Augmented Generation)\n为了让 LLM 全面理解检索到的子图 g^\\hat{g}g^​，GRAG 设计了两种互补的视图来向 LLM 传递信息，如上图 b 部分所示\n\n\n文本视图 (Text View) - 硬提示 (Hard Prompts)：\n\n目标：将图的拓扑结构用文本形式描述出来，作为 LLM 能够直接阅读的硬提示。\n实现：设计了一种算法，将每个检索到的 ego-graph 无损地转换为分层的文本描述 (hierarchical text descriptions)。\n\n首先，通过广度优先搜索（BFS）从 ego-graph 中提取一个生成树结构 Tg\\mathcal{T}_gTg​。\n然后，对树 Tg\\mathcal{T}_gTg​ 进行前序遍历，按照层级关系和预设的模板（如“节点 A 引用了节点 B”）生成文本。\n最后，将图中剩余的非树边（如跨层连接）作为补充关系插入到文本描述中。\n\n\n这个过程将图的结构信息编码为 LLM 善于处理的层级化文本，最终的硬提示是 [查询 q, 分层文本描述 D_g]。\n下图是一个将引文网络的 2-hop ego-graph 转换为分层文本描述的例子。\n\n(图例参考：原论文 Figure 4)\n\n\n\n图视图 (Graph View) - 软提示 (Soft Prompts)：\n\n目标：将图的拓扑信息直接编码为嵌入向量（即图令牌），作为软提示，补充文本视图无法完全捕捉的结构信息。\n实现：使用一个图神经网络（GNN，如 GAT）来编码经过软剪枝的子图。\n在 GNN 的消息传递过程中，使用之前计算出的缩放因子 α\\alphaα 来加权节点和边的特征，从而控制信息流，让与查询更相关的实体传递更多的信息。mu(l)=MSG(l)(αu⋅hu(l−1),αuv⋅euv)m_{u}^{(l)}=MSG^{(l)}(\\alpha_{u}\\cdot h_{u}^{(l-1)},\\alpha_{uv}\\cdot e_{uv})\nmu(l)​=MSG(l)(αu​⋅hu(l−1)​,αuv​⋅euv​)\n\nGNN 输出的图嵌入 hg^h_{\\hat{g}}hg^​​ 经过一个 MLP 对齐到 LLM 的词嵌入空间，成为软提示。\n\n\n\n最终，LLM 的生成过程同时由硬提示（文本嵌入 hTh_ThT​）和软提示（图嵌入 hg^h_{\\hat{g}}hg^​​）共同引导，其联合输入形式为 [hg^;hT][h_{\\hat{g}}; h_T][hg^​​;hT​]。\nBaseline\n为了验证 GRAG 的有效性，论文与以下几类方法进行了对比：\n\nLLM Baselines:\n\nFrozen LLM: 直接使用 Llama2-7b 模型回答问题，不进行任何微调或检索。\nFine-tuned LLM: 使用 LoRA 对 Llama2-7b 模型在任务数据上进行微调。\n\n\nRAG-based Retrievers:\n\n经典方法: BM25 (基于词频的统计模型)。\n基于稠密向量的检索器:\n\nMiniLM-L12-v2\nLaBSE\nmContriever\nE5\n\n\n专门针对图的检索器: G-Retriever (先检索节点和边，再用 Prize-Collecting Steiner Tree 方法构建子图)。\n\n\n\n数据集 (Datasets)\n实验在 GraphQA 基准上进行，主要使用了两个数据集：\n\nWebQSP: 一个大规模的多跳知识图谱问答（QA）数据集。图中节点和边的数量多，结构复杂。\nExplaGraphs: 一个专注于常识推理的数据集，图的规模相对较小。\n\n\n\n\n数据集\n# Graphs\n# Nodes (avg)\n# Edges (avg)\n# Tokens (avg)\n\n\n\n\nWebQSP\n4,700\n1370.89\n4252.37\n100,627\n\n\nExplaGraphs\n2,766\n5.17\n4.25\n1,396\n\n\n\n可复现性 (Reproducibility)\n\n代码: 作者在论文摘要中提供了 GitHub 仓库链接：https://github.com/HuieL/GRAG，代码是开源的。\n算力: 实验在一台配备 4 个 NVIDIA A10G GPU 的服务器上进行。\n模型与配置:\n\nLLM Backbone: Llama-2-7b-hf。\n微调方法: LoRA。\nGraph Encoder: 4 层的 GAT。\n优化器: AdamW。\nBatch Size: 2。\n\n\n\n整体来看，由于代码和详细的实验配置都已提供，该研究的可复现性较高。\n可改进的几个点 (Potential Improvements)\n论文在第 7 节（Limitations）中指出了当前方法的局限性，这些也是未来可以改进的方向：\n\n对初始排行的依赖性: GRAG 的检索效果在很大程度上取决于初始 ego-graph 排行的质量。如果图的结构复杂或节点重要性难以估计，可能会导致初始检索的 ego-graphs 质量不高，从而影响最终性能。\n剪枝机制的优化: 软剪枝机制虽然有效，但其性能依赖于 MLP 的学习能力。在更复杂的场景下，可能需要设计更先进、更具解释性的剪枝策略。\n超大图的可扩展性: 虽然分治策略提升了效率，但对于节点数以亿计的超大规模图，对每个节点建立 ego-graph 索引的开销（存储和计算）仍然巨大。需要探索更具扩展性的索引和检索方法。\n动态图的处理: 当前模型主要针对静态图。如何将其扩展到能够处理节点和边随时间变化的动态图，是一个有价值的研究方向。\n子图大小 K 的权衡: 增加 K-hop 中的 K 可以包含更多上下文，但也可能引入更多噪声并增加计算成本。如何动态地、自适应地确定最佳的 K 值是一个值得探索的问题。\n\n可以被引用的一些结论 (Key Takeaways)\n\nGRAG 显著优于传统 RAG: 在需要多跳推理的图相关任务上，GRAG 的性能显著超过了所有基于 RAG 的 baseline 和纯 LLM baseline。\n检索比微调更有效: 未经微调的 LLM 加上 GRAG 后，其性能甚至超过了在任务数据上经过 LoRA 微调的 LLM。这表明，为 LLM 提供结构化的外部知识是一种比单纯微调模型参数更有效、更经济的提升其图推理能力的方法。\n“软剪枝”至关重要: 消融实验证明，移除软剪枝模块会导致性能大幅下降。这凸显了在将检索到的信息喂给 LLM 之前，过滤掉无关实体的必要性，尤其是在边连接密集的图中。\n文本和拓扑信息缺一不可: 实验表明，同时提供“文本视图”（硬提示）和“图视图”（软提示）是实现最佳性能的关键。只提供其中一种，都会导致性能显著下降，证明了两种视图的互补性。\n更大的 LLM 未必更好: 在没有有效检索机制的情况下，单纯增加 LLM 的参数规模（从 7B 到 13B）并不能在图相关任务上带来性能提升，有时甚至会下降。这强调了先进的检索机制（如 GRAG）对于释放 LLM 在特定领域（如图推理）潜力的重要性。\nGRAG 具备良好的迁移学习能力: 在一个大规模数据集（如 WebQSP）上训练的 GRAG 模型，可以被直接用于提升在另一个小规模数据集（如 ExplaGraphs）上的性能，展现了其学习到的图编码和理解能力的通用性。\n\n","categories":["paper"],"tags":["paper","GRAG"]},{"title":"Gecko: Versatile Text Embeddings Distilled from Large Language Models","url":"/paper/Gecko-Versatile-Text-Embeddings-Distilled-from-Large-Language-Models/","content":"问题 (Problem)\n论文旨在解决当前文本嵌入模型领域的核心挑战：如何创建一个既紧凑又通用的文本嵌入模型。\n现有方法存在以下痛点：\n\n通用性差：许多模型在特定任务（如语义相似度）上表现优异，但在跨任务、跨领域（如信息检索、分类、聚类等）的泛化能力上表现不佳。\n数据依赖严重：要构建一个覆盖多领域、多任务的通用模型，通常需要海量的、高质量的人工标注数据。这个过程不仅成本高昂、耗时费力，而且难以覆盖所有场景。\n模型效率问题：为了追求高性能，模型往往变得越来越大（例如参数量超过70亿），嵌入维度也越来越高（例如超过4000维），这给实际部署和应用带来了巨大的计算和存储开销。\n\n因此，本文的核心问题是：我们能在多大程度上直接利用大型语言模型（LLM）中蕴含的丰富世界知识，来蒸馏出一个紧凑、高效且在多种任务上都表现出色的通用文本嵌入模型？\n方法 (Method)\n本文提出了 Gecko，一个通过两步式LLM知识蒸馏流程训练得到的文本嵌入模型。其核心是创建了一个名为 FRet (Few-shot Prompted Retrieval dataset) 的高质量合成数据集。\n\nFRet：两步式LLM蒸馏数据集生成\n该方法不直接使用LLM生成文本内容，而是利用LLM的理解和判断能力来生成高质量的训练标签。\n第一步：LLM生成多样化的任务和查询 (Diverse Query Generation)\n\n起点：从一个巨大的、无标签的网页语料库中随机抽取一个“种子段落” (pseedp_{seed}pseed​)。\n过程：使用一个经过小样本（few-shot）提示（Prompt）的LLM，让它读取这个种子段落，并为其生成一个相关的任务描述 (ttt) 和一个符合该任务的查询 (qqq)。\n\n数学表示为：LLM(PQG,pseed)→(t,q)LLM(\\mathbb{P}_{QG}, p_{seed}) \\rightarrow (t, q)LLM(PQG​,pseed​)→(t,q)\n其中 PQG\\mathbb{P}_{QG}PQG​ 是一个包含了指令和若干示例的固定Prompt。\n\n\n多样性来源：\n\n网页语料库本身内容丰富，涵盖博客、新闻、百科等多种风格和主题。\n通过在Prompt中设计多样化的任务示例（如“问答”、“事实核查”、“寻找相似句子”等），引导LLM生成形式多样的任务和查询。\n\n\n\n图解：如下图上半部分所示，LLM读取一个关于“Phastos…创造原子弹”的段落后，可以生成“问答”任务及对应查询“谁制造了原子弹？”，也可以生成“事实核查”任务及对应查询“Phastos创造了原子弹”。\n\n图：FRet数据集生成流程图\n第二步：LLM挖掘正负样本 (Positive and Negative Mining)\n\n核心假设：种子段落 (pseedp_{seed}pseed​) 不一定是其生成的查询 (qqq) 的最佳答案。语料库中可能存在更相关、更直接的段落。\n过程：\n\n检索：对于上一步生成的每个查询 (qqq)，使用一个预训练好的检索器，从网页语料库中检索出Top-N个最相关的候选段落 P=p(1),...,p(N)P={p^{(1)},...,p^{(N)}}P=p(1),...,p(N)。\n重排序 (Re-ranking)：利用同一个LLM对这N个候选段落进行打分和排序。为了提高排序的鲁棒性，论文融合了两种LLM打分方法：\n\n查询似然度 (Query Likelihood, QL)：计算在给定段落 ppp 的条件下，生成查询 qqq 的对数似然概率，即 LLM(q∣p,PQL)LLM(q|p, \\mathbb{P}_{QL})LLM(q∣p,PQL​)。\n相关性分类 (Relevance Classification, RC)：计算在给定查询 qqq 和段落 ppp 的条件下，生成“相关”标签的对数似然概率，即 LLM(label∣q,p,PRC)LLM(label|q,p,\\mathbb{P}_{RC})LLM(label∣q,p,PRC​)。\n融合：使用倒数排序融合 (Reciprocal Rank Fusion, RRF) 算法将QL和RC的排序结果结合起来，得到最终的排序函数 R(q,p)R(q,p)R(q,p)。\n\n\n重新标注：\n\n正样本 (p+p^+p+)：选择LLM重排序后排名第一的段落，即 p+=p1p^+ = p_1p+=p1​。实验发现，大约有15%的情况下，p1p_1p1​ 并不等于原始的 pseedp_{seed}pseed​，证明了重标注的必要性。\n难负样本 (p−p^-p−)：选择LLM重排序后排名最低的段落（例如pNp_NpN​），或者从排名靠后的段落中随机采样。\n\n\n\n\n\n通过以上两步，最终生成了包含660万个样本的FRet数据集，每个样本都包含(任务, 查询, 正样本, 负样本)四元组。\nGecko 模型训练流程\nGecko的训练分为两个阶段，基于一个12亿参数的预训练Transformer模型。\n预微调 (Pre-finetuning)\n\n\n目的：让模型接触大量不同类型的文本，学习通用的文本表示能力。\n\n\n数据：大规模的社区问答对（如论坛问答）和网页标题-正文对。\n\n\n查询向量的生成(q_i): 它的特殊之处在于，它不仅考虑了查询本身，还融入了任务描述（task）。\nqi=mean_pool∣t∣+∣qi∣[M(t⊕qi)∈R(∣t∣+∣qi∣)×d]∈Rdq_i = mean\\_ pool_{|t|+|q_i|} \\left[ \\mathcal{M}(t \\oplus q_i) \\in \\mathbb{R}^{(|t|+|q_i|) \\times d} \\right] \\in \\mathbb{R}^d\nqi​=mean_pool∣t∣+∣qi​∣​[M(t⊕qi​)∈R(∣t∣+∣qi​∣)×d]∈Rd\n\nt⊕qit \\oplus q_it⊕qi​: 这一步是文本拼接。\n\nttt 代表任务描述，例如“问答”或“事实核查”。\nqiq_iqi​ 是用户的实际查询内容，例如“谁发明了灯泡”。\n⊕\\oplus⊕ 符号表示将这两个字符串连接在一起，形成一个新的输入，如：“任务：问答 | 查询：谁发明了灯泡”。这样做是为了让模型知道它当前需要执行哪种类型的任务。\n\n\nM(...)\\mathcal{M}(...)M(...): M\\mathcal{M}M 代表一个预训练的语言模型（例如Transformer）。\n\n这个模型会读取拼接后的文本，并为其中的每一个词元（token） 都生成一个上下文相关的向量。\n输出结果是一个矩阵，维度是 (词元数量)×d(词元数量) \\times d(词元数量)×d。其中 ddd 是预设的向量维度（比如768维），词元数量是 ∣t∣+∣qi∣|t|+|q_i|∣t∣+∣qi​∣。\n\n\n$mean_pool[…]: 这是平均池化操作。\n\n它会取模型输出的那个矩阵，然后将所有词元的向量相加再取平均值。\n这就好比把一句话里每个词的“含义向量”都融合起来，得到一个代表整句话核心含义的“平均向量”。\n\n\n最终结果 (qi∈Rdq_i \\in R^dqi​∈Rd): 经过平均池化后，原来代表每个词的向量矩阵就被压缩成了一个单一的、维度为 ddd 的向量 qiq_iqi​。这个向量就是最终代表整个“任务+查询”的文本嵌入。\n\n\n\n段落向量的生成(p_i): 这个公式是为段落（passage），比如一篇文章或一个回答，生成向量。它的流程和上面类似，但更简单一些。\npi=mean_pool∣pi∣[M(pi)∈R∣pi∣×d]∈Rdp_i = mean\\_ pool_{|p_i|}[\\mathcal{M}(p_i) \\in \\mathbb{R}^{|p_i| \\times d}] \\in \\mathbb{R}^d\npi​=mean_pool∣pi​∣​[M(pi​)∈R∣pi​∣×d]∈Rd\n\n\n目标函数：使用带有批内负样本（in-batch negatives）的对比学习目标函数。对于批次中的一个查询 qiq_iqi​，其对应的段落 pip_ipi​ 是正样本，批次内所有其他的段落 pjp_jpj​ (j≠i) 都被视为负样本。\nLpre=−1B∑i=1Blog⁡esim(qi,pi)/τ∑j=1Besim(qi,pj)/τL_{pre} = -\\frac{1}{B} \\sum_{i=1}^{B} \\log \\frac{e^{sim(q_i, p_i)/\\tau}}{\\sum_{j=1}^{B} e^{sim(q_i, p_j)/\\tau}}\nLpre​=−B1​i=1∑B​log∑j=1B​esim(qi​,pj​)/τesim(qi​,pi​)/τ​\n其中:\n\nsim(x,y)=x⊤y∣∣x∣∣⋅∣∣y∣∣sim(x,y) = \\frac{x^{\\top}y}{||x|| \\cdot ||y||}sim(x,y)=∣∣x∣∣⋅∣∣y∣∣x⊤y​ 是余弦相似度函数\nτ\\tauτ 是温度超参数。\n\n\n\n微调 (Fine-tuning)\n\n目的：利用高质量的FRet和公开数据集，对模型进行多任务、指令化的微调。\n数据：一个统一的混合数据集，包含FRet以及多个学术数据集（如Natural Questions, SNLI, MNLI等），所有数据都被统一格式化为(任务, 查询, 正样本, 负样本)。\n目标函数：一个更强的批内对比学习损失函数，它不仅包含来自其他样本的正段落作为负样本，还创新性地引入了“同塔负样本 (same-tower negatives)”，即将批次内其他的查询也作为负样本。Lmain=1B∑i=1B[−log⁡esim(qi,pi+)/τ∑j=1B(esim(qi,pj+)/τ+1[j≠i]esim(qi,qj)/τ)+esim(qi,pi−)/τ]L_{main} = \\frac{1}{B} \\sum_{i=1}^{B} \\left [ -\\log \\frac{e^{sim(q_i, p_i^+)/\\tau}}{\\sum_{j=1}^{B} (e^{sim(q_i, p_j^+)/\\tau} + \\mathbb{1_{[j \\ne i]}} e^{sim(q_i, q_j)/\\tau}) + e^{sim(q_i, p_i^-)/\\tau}} \\right ]\nLmain​=B1​i=1∑B​[−log∑j=1B​(esim(qi​,pj+​)/τ+1[j=i]​esim(qi​,qj​)/τ)+esim(qi​,pi−​)/τesim(qi​,pi+​)/τ​]\n\npi+p_i^+pi+​ 是 qiq_iqi​ 的正样本。\npi−p_i^-pi−​ 是 qiq_iqi​ 的难负样本。\npj+p_j^+pj+​ (j≠i) 是批内其他样本的正样本。\nqjq_jqj​ (j≠i) 是同塔负样本，对于对称性任务（如语义相似度）尤其有效。\n\n\n多维度支持：采用了套娃表示学习 (Matryoshka Representation Learning, MRL) 损失，使同一个模型能够输出不同维度的嵌入向量（如768维和256维），增加了模型的灵活性。\n\nBaseline\nGecko与当前主流的文本嵌入模型进行了对比，可以分为两类：\n\n更大规模的模型 (≥7B 参数, &gt;3k 维度)：\n\ngritlm-8x7b, e5-mistral-7b-instruct, gritlm-7b, text-embedding-3-large (OpenAI)\n\n\n相似规模的模型 (≤5B 参数, ≤1k 维度)：\n\ngtr-t5-xxl, gtr-t5-xl, instructor-xl, text-embedding-3-large-256 (OpenAI)\n\n\n\n实验结果显示，Gecko-1B-768 (12亿参数，768维) 的平均性能（66.31）不仅全面超越了所有相似规模的模型，甚至能够与比它大7倍、维度高5倍的模型（如gritlm-7b的66.76）相媲美。Gecko-1B-256 (256维) 的性能也远超同维度的其他模型。\n数据集 (Datasets)\n\n训练数据：\n\n预微调：社区问答网站数据、网页标题-正文数据。\n微调：\n\nFRet (本文贡献)：基于网页语料库生成的660万合成数据。\n公开学术数据：Natural Questions, HotpotQA, FEVER (检索与问答), SNLI, MNLI (自然语言推断), MedMCQA (医疗问答), MIRACL (多语言检索), 以及多个Huggingface上的分类数据集。\n\n\n\n\n评测数据：\n\nMTEB (Massive Text Embedding Benchmark)：一个包含56个数据集的大规模评测基准，覆盖7大类任务：分类、聚类、成对分类、重排序、检索、语义文本相似度(STS)和摘要。\nMIRACL：一个包含18种语言的多语言检索评测基准。\n\n\n\n可复现性 (Reproducibility)\n\n代码：论文中未提及开源代码。\n算力/模型：\n\n模型依赖：训练基于一个12亿参数的Transformer模型，微调这样一个模型需要强大的计算资源（如多个高端GPU或TPU）。\n数据生成依赖：FRet数据集的生成依赖于一个未指定但性能强大的内部大语言模型（推测为Google自家的模型）。外部研究者难以获取同样性能的LLM。\n语料库依赖：生成FRet所用的网页语料库是内部数据，未公开。\n\n\n结论：由于核心的数据集生成方法依赖于非公开的LLM和数据，且没有提供源代码，该工作的外部可复现性很低。\n\n可改进的几个点\n\n降低LLM依赖：方法的核心在于强大的LLM。未来可以探索使用更小、更易于获取的开源LLM（如Llama 3, Mistral等）是否能生成类似质量的数据集，从而让该方法更具通用性和可复现性。\n数据生成效率：目前“生成-检索-重排序”的两步流程计算成本非常高。可以研究能否设计出更高效的单步流程，例如让LLM直接生成高质量的(查询, 正样本, 负样本)三元组。\n任务多样性的系统化生成：当前任务多样性依赖于Prompt中的示例。可以探索更系统化的方法，比如先让LLM生成一个包含上百种任务类型的清单，再基于这个清单去生成数据，以确保任务的广度和均衡性。\n更优的难负样本挖掘策略：论文中使用了排序最低的样本或随机采样作为难负样本。可以探索更高级的策略，例如挖掘那些与查询在语义上高度相似但在关键事实上相悖的样本，为模型提供更具挑战性的学习信号。\nFRet的多语言扩展：目前的FRet数据集是纯英文的。尽管它对多语言模型也有帮助，但如果能直接用LLM生成多种语言的FRet数据，有望在非英语任务上取得更大的性能提升。\n\n可以被引用的一些结论\n\nLLM蒸馏是创建高效嵌入模型的有效途径：通过从LLM中蒸馏知识，一个12亿参数的紧凑模型（Gecko）可以在性能上媲美甚至超越大7倍（&gt;70亿参数）的巨型模型，证明了“小模型+高质量合成数据”路线的巨大潜力。\nLLM重标注对于合成数据质量至关重要：原始用于生成查询的“种子段落”往往不是最佳正样本。利用LLM对检索到的候选集进行重排序并重新选择正样本，能显著提升模型性能。这一“重选”操作发生在约15%的数据上。\n纯合成数据具有强大的零样本泛化能力：仅在合成的FRet数据集上训练的模型，在MTEB这个对它来说是纯零样本（zero-shot）的评测基准上，表现依然非常强劲，甚至超过了许多在人工标注数据上训练的基线模型。\n任务多样性是通用嵌入模型的关键：与在单一合成任务（如纯问答）上训练相比，在一个由LLM生成的多任务混合数据集上训练，能显著提升模型在各类下游任务上的泛化能力。\n低维嵌入也能达到顶尖性能：有效的知识蒸馏使得低维度（如256维）的Gecko模型能够超越许多高维度（如768维）的强大对手，这对于资源受限的应用场景意义重大。\n融合多种LLM打分策略可提升数据标注的鲁棒性：通过RRF融合查询似然度和相关性分类两种LLM打分方式，可以获得一个在不同任务上都表现稳健的重排序器，这对于生成覆盖多种任务的高质量数据集至关重要。\n\n","categories":["paper"],"tags":["paper","LLM","Embedding"]},{"title":"IMPROVING LANGUAGE MODELS VIA PLUG-AND-PLAY RETRIEVAL FEEDBACK","url":"/paper/IMPROVING-LANGUAGE-MODELS-VIA-PLUG-AND-PLAY-RETRIEVAL-FEEDBACK/","content":"问题 (Problem)\n这篇论文主要解决大型语言模型（LLMs）在实际应用中的几个核心痛点：\n\n内容幻觉与不准确性: 尽管LLMs在多种自然语言处理任务上表现出色，但它们常常会生成不正确或完全捏造（即“幻觉”）的信息，这严重限制了它们在需要高可靠性场景下的应用。\n知识局限性: LLMs的知识被固化在其模型参数中，这些知识可能是不完整或过时的，尤其难以覆盖训练语料中的长尾知识。\n现有解决方案的缺陷:\n\n人工反馈（如RLHF）: 通过人工标注和强化学习来对齐模型，虽然有效，但极其消耗资源、成本高昂且耗时。\n实时性差: 对于已经微调好的模型，很难在推理过程中实时接收反馈并进行即时纠错。\n\n\n\n因此，论文的核心研究问题是：我们能否在不进行昂贵微调的前提下，设计一个即插即用的自动化流程，利用外部知识库对LLM的生成内容进行反馈和修正，从而提升其准确性？\n\n方法 (Method)\n论文提出了一种名为 REFEED (REtrieval FEEDback) 的新型工作流，其核心思想是“先生成，再检索，后优化”，将检索作为一种反馈机制而非传统的输入增强。\n基础工作流 (Basic Pipeline)\nREFEED的基础流程包含三个步骤，如下图1所示：\n\n\n\nStep-1: 生成初始答案 (Generate an Initial Answer)\n\n给定一个问题或查询 xxx，首先让LLM（如GPT-3.5）生成一个初步的、最可能的答案 y^\\hat{y}y^​。这一步通常采用贪心解码（greedy decoding）以保证结果的确定性。\n\n\n\nStep-2: 检索相关文档 (Retrieve Documents)\n\n关键创新点: 将初始问题 xxx 和生成的初步答案 y^\\hat{y}y^​ 拼接起来，形成一个新的、更丰富的检索查询 [x, ŷ]。\n使用这个新查询，通过一个检索模型（如BM25）从大规模文档库（如维基百科）中检索出一系列最相关的文档 d1,...,dnd_1, ..., d_nd1​,...,dn​。\n这样做的好处是，初步答案 y^\\hat{y}y^​ 极大地弥补了原始问题 xxx 与正确答案所在文档之间的“词汇和语义鸿沟”，使得检索到的文档与待验证的答案相关性更强。例如，直接用问题“新的黑魔法防御术老师是谁？”去检索，可能信息不足；但如果模型初步回答是“斯内普”，那么用“新的黑魔法防御术老师是谁？西弗勒斯·斯内普”去检索，更容易找到包含“斯内普”和“黑魔法防御术老师”的精确上下文。\n\n\n\nStep-3: 优化先前答案 (Refine the Previous Answer)\n\n将原始问题 xxx、初步答案 y^\\hat{y}y^​ 以及检索到的文档 d1,...,dnd_1, ..., d_nd1​,...,dn​ 一同作为上下文，再次输入给LLM。\n提示（Prompt）LLM参考这些文档来回答原始问题，从而生成最终的、经过优化的答案 yyy。模型会根据检索到的信息来修正、确认或重写初始答案。\n\n\n\n数学公式解读\n\n\n传统Retrieve-then-Read模型: 其概率公式可以表示为在所有可能文档上进行边缘化的结果：\np(y∣x)=∑ip(y∣di,x)p(di∣x)p(y|x) = \\sum_{i} p(y|d_i, x) p(d_i|x)\np(y∣x)=i∑​p(y∣di​,x)p(di​∣x)\n这里，文档的检索只依赖于原始问题 xxx。\n\n\nREFEED模型: 其概率公式体现了“先生成、后反馈”的思想：\np(y∣x)=∑ip(y∣di,x,y^)p(di∣y^,x)p(y^∣x)p(y|x) = \\sum_{i} p(y|d_i, x, \\hat{y}) p(d_i|\\hat{y}, x) p(\\hat{y}|x)\np(y∣x)=i∑​p(y∣di​,x,y^​)p(di​∣y^​,x)p(y^​∣x)\n这里的核心区别在于：\n\np(y^∣x)p(\\hat{y}|x)p(y^​∣x): 首先，模型会根据问题 xxx 生成一个初始答案 y^\\hat{y}y^​。\np(di∣y^,x)p(d_i|\\hat{y}, x)p(di​∣y^​,x): 然后，文档的检索是同时基于问题 xxx 和 初始答案 y^\\hat{y}y^​ 的，这体现了检索的“反馈”特性。\np(y∣di,x,y^)p(y|d_i, x, \\hat{y})p(y∣di​,x,y^​): 最后，最终答案 yyy 的生成是基于问题、初始答案和检索文档三者的。\n\n\n\n增强模块 (Enhanced Modules)\n为了进一步提升REFEED的性能和鲁棒性，论文设计了两个增强模块。\n\n\n模块1: 多样化检索反馈 (Diversifying Retrieval Feedbacks)\n\n\n思想: 单一的初始答案可能不是最优的，或者可能导致检索范围过窄。通过生成多个不同的初始答案，可以拓宽检索的覆盖面。\n\n\n实现: 在Step-1中，不使用贪心解码，而是采用采样方法（如核采样, nucleus sampling）生成 nnn 个不同的初始答案候选 y^1,...,y^n{\\hat{y}_1, ..., \\hat{y}_n}y^​1​,...,y^​n​。\n\n\n对每一个候选答案 y^j\\hat{y}_jy^​j​，都执行Step-2的检索操作，得到多组文档。\n\n\n将所有检索到的文档合并，根据检索得分（如BM25得分）进行统一排序，并选取Top-k个文档用于Step-3的优化。\n\n\n效果: 如下图2所示，这种方法增加了找到包含正确答案线索的文档的概率（提升Recall@K，见下图4），从而提高了最终答案的准确率。\n\n\n图4解读：相比仅用问题(“Q only”)或问题+单个答案(“Q+A”)进行检索，使用问题+多个多样化答案(“Q+A1,A2…An”)进行检索，在Top-1, Top-5, Top-10的文档中找到正确答案的召回率（Recall@K）显著更高。\n\n\n\n\n模块2: 集成初始与反馈后输出 (Ensembling Initial and Post-Feedback Outputs)\n\n思想: 检索到的文档并非总是可靠的，有时可能会包含误导性信息，导致模型将一个正确的初始答案改错（如下图5中的第二个例子）。\n\n实现: 该模块引入一个决策机制。在生成最终答案后，比较初始答案和优化后答案的可信度，选择更可信的一个。\n评判标准: 使用模型对答案的平均语言建模概率作为可信度的代理指标。\n\n反馈前的概率: Pbefore(y^∣x)P_{before}(\\hat{y}|x)Pbefore​(y^​∣x)\n反馈后的概率: Pafter(y∣x,y^,d)P_{after}(y|x, \\hat{y}, d)Pafter​(y∣x,y^​,d)\n\n\n决策: 如果 Pbefore&gt;PafterP_{before} &gt; P_{after}Pbefore​&gt;Pafter​，则保留初始答案；反之，采纳优化后的答案。如下图3所示。\n效果: 该方法有效结合了模型的内部知识（参数化知识）和外部知识（检索文档），缓解了检索噪声带来的负面影响。\n\n图3解读：模型计算出反馈前答案的概率为0.68，反馈后为0.65。由于反馈前的概率更高，最终选择保留初始答案。\n\n\n\n\nBaseline (对比模型)\n论文将REFEED与两类主流方法进行了比较：\n\n\n闭卷模型 (Closed-book Methods): 不使用任何外部检索文档，完全依赖模型自身的参数化知识。\n\nInstructGPT (QA prompt): 标准的问答提示。\nGenRead: 先让模型生成相关知识，再基于生成的知识回答问题。\nSelf-Prompting: 模型自己生成指令来引导回答。\n\n\n\n开卷模型 (Open-book Methods): 遵循传统的“先检索，后阅读”(Retrieve-then-Read)模式。\n\nRetrieve-Read: 先用问题检索文档，然后将文档和问题一起输入模型进行阅读理解。\nRePLUG: 一种更先进的Retrieve-then-Read黑盒模型增强方法。\n\n\n\n此外，论文还验证了REFEED与思维链 (Chain-of-Thought, CoT) 的兼容性，证明REFEED可以无缝集成到CoT流程中，进一步提升复杂推理任务的性能。\n\n数据集 (Datasets)\n实验在四个知识密集型任务的基准数据集上进行：\n\n单跳问答 (Single-hop QA):\n\nNQ (Natural Questions): 来自谷歌搜索的真实用户查询。\nTriviaQA: 知识问答竞赛类型的问题。\n\n\n多跳问答 (Multi-hop QA):\n\nHotpotQA: 需要结合多个文档信息进行推理才能回答的问题。\n\n\n对话生成 (Dialogue Generation):\n\nWoW (Wizard of Wikipedia): 需要基于维基百科知识进行开放域对话。\n\n\n\n评测指标: 主要使用精确匹配率 (Exact Match, EM)、F1分数 (F1 Score) 和 Rouge-L (R-L)。\n\n可复现性 (Reproducibility)\n\n代码: 论文承诺在会议评审结束后，会公开所有源代码和数据。\n算力/模型:\n\n主要模型: 实验主要基于 text-davinci-003 和 code-davinci-002 (Codex) 进行，这些是可通过OpenAI API访问的公开模型。\n其他模型: 论文也在最新的ChatGPT (gpt-3.5-turbo) 和 GPT-4 上进行了验证性实验，证明了方法的普适性。\n复现性考量: 论文明确指出，选择 davinci 系列作为主要模型是因为ChatGPT和GPT-4的参数会持续更新，导致实验结果难以复现。这一点体现了作者严谨的科研态度。\n超参数: 论文附录中提供了详细的超参数设置，如最大输出长度、temperature等，便于他人复现。\n\n\n\n\n可改进的几个点 (Potential Improvements)\n根据论文的局限性分析和方法设计，可以预见以下几个改进方向：\n\n任务泛化性: 当前的REFEED框架专为生成式任务设计。对于需要离散输出（如“是/否”）的分类任务，其适用性有限。未来的工作可以探索如何将这种“生成-反馈”机制应用于更广泛的任务类型。\n对模型校准的依赖: 模块2中的集成方法依赖于比较模型的输出概率，这要求语言模型本身是相对良好校准的 (well-calibrated)。对于校准性差的模型，该方法效果可能下降。可以研究不依赖于模型原始概率的集成策略，例如训练一个独立的排序或验证模块。\n检索噪声的鲁棒性: 尽管有集成模块，但错误的或有偏见的检索文档仍然可能误导模型。可以引入一个更强的事实校验 (Fact-checking) 或噪声过滤 (Noise-filtering) 模块，在Step-3之前对检索到的信息进行筛选和加权。\n计算效率: REFEED相比直接提问增加了“初始生成”和“检索”两个环节的开销。虽然性能提升显著，但可以探索如何优化效率，例如使用更轻量的模型进行初始生成，或设计更高效的文档筛选策略以减少输入给大模型的上下文长度。\n\n\n可以被引用的一些结论 (Quotable Conclusions)\n\n核心贡献: 论文提出的REFEED是一种新颖的、即插即用的工作流，它创新地将检索用作一种反馈机制来事后优化LLM的输出，从而在无需微调的情况下显著提升了生成内容的准确性和事实性。\n关键机制: 在检索查询中融合LLM的初始生成答案，是弥合问题与相关文档之间语义鸿沟的有效策略，能够显著提升检索文档的相关性和质量，这是整个反馈回路成功的关键。\n显著的性能提升: 实验证明，与不使用检索反馈的基线相比，REFEED在零样本（zero-shot）设置下相对提升了25.7%，在少样本（few-shot）设置下相对提升了13.5%。\n有效的增强策略: “多样化初始生成”和“集成反馈前后输出”是两种简单而有效的策略，能分别通过扩大证据覆盖面和抑制检索噪声来进一步提升系统的性能与鲁棒性。\n良好的兼容性: REFEED框架可以与思维链（CoT）等先进的提示（prompting）技术互补结合，在处理如多跳问答等复杂推理任务时展现出更强的能力。\n\n","categories":["paper"],"tags":["paper","Retrieval","LLM"]},{"title":"How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval","url":"/paper/How-to-Train-Your-DRAGON-Diverse-Augmentation-Towards-Generalizable-Dense-Retrieval/","content":"问题 (Problem)\n传统的稠密检索（Dense Retrieval, DR）模型普遍存在一个核心问题：在监督式评测和零样本（Zero-shot）评测之间存在明显的性能权衡（trade-off）。具体来说，一个在特定数据集（如 MS MARCO）上通过监督学习训练得很好的模型，在从未见过的新领域（零样本场景）上往往表现不佳，反之亦然。\n如 Figure 1 所示，大多数现有的检索器（除了参数量巨大（4.8B）的 GTR-XXL）都分布在一条倾斜的直线上，显示了“监督式评测”得分（X轴）和“零样本评测”得分（Y轴）之间的负相关关系。当时的普遍观点认为，要打破这种权衡，必须大幅增加模型（如BERT-base）的容量。\n本文的核心目标：挑战上述观点，证明在不增加模型大小（仍使用 BERT-base 级别）的前提下，通过一种更优的训练方法，可以训练出一个在监督式和零样本场景下均达到顶尖水平（State-of-the-Art, SOTA）的通用稠密检索器。\nFigure 1 解读:\n\n\n坐标轴:\n\nX轴: Supervised evaluation (MS MARCO Dev: RR@10) - 衡量模型在见过的、有监督数据上的性能。\nY轴: Zero-shot evaluation (BEIR 13 subsets: nDCG@10) - 衡量模型在未见过的新领域的泛化能力。\n\n\n数据点: 图中的每个点代表一个检索模型。不同形状代表不同类型的检索器（如圆形代表稠密检索，方形代表稀疏检索）。\n蓝色趋势线: 清晰地揭示了大多数模型存在的性能权衡。\nDRAGON (红色点): 本文提出的模型，显著地位于趋势线的右上方，表明它同时在监督和零样本任务上取得了高分，成功打破了这种权衡。\n\n\n方法 (Method)\n作者提出了一个名为 DRAGON (Dense Retriever trained with diverse AuGmentatiON) 的模型。其核心思想是构建一个统一的数据增强（Data Augmentation, DA）框架，通过多样化的查询增强和渐进式的相关性标签增强来训练模型。\n统一的数据增强框架\n作者将许多现有的稠密检索改进方法（如知识蒸馏、对比预训练、伪查询生成）都归纳到数据增强的框架下，主要分为两类：\n\n查询增强 (Query Augmentation): 增加训练查询的数量和多样性。\n相关性标签增强 (Relevance Label Augmentation): 为每个查询提供更丰富、更准确的正负样本标签。\n\n查询增强\nDRAGON 探索了两种主要的查询增强方式：\n\n句子裁剪 (Sentence Cropping): 一种计算成本低廉的方法，直接从语料库的文档中裁剪出句子作为查询。这种方法可以轻松地将查询规模扩大到千万级别。\n伪查询生成 (Pseudo Query Generation, GenQ): 使用大型语言模型（如 T5）为文档生成类似人类提问的查询。这种方法生成的查询质量高，但计算成本昂贵。\n\n最终，DRAGON 采用混合方式，将一半的裁剪句子和一半的生成式查询混合作为训练数据，以兼顾数量、多样性和质量。\n多样化和渐进式的标签增强\n这是 DRAGON 方法的核心创新点。传统的标签增强通常依赖于单个、强大的“教师模型”（如交叉编码器 Cross-Encoder, CE），但作者认为单个教师的视角是有限的。\n核心假设：不同的检索器（如稀疏、稠密、多向量模型）捕捉到的文本相关性信号是不同的（例如，词汇匹配 vs. 语义匹配）。为了让学生模型学到更通用的匹配能力，应该从多个具有不同优势的教师那里学习。\n具体实现：渐进式监督 (Progressive Supervision)\nDRAGON 不会一次性将所有教师的知识都灌输给模型，而是采用一种类似课程学习（Curriculum Learning）的策略，分阶段、逐步地引入新的教师。\n\n教师团队: DRAGON 使用了一组多样化的教师模型，包括：\n\n稀疏检索: uniCOIL, SPLADE++\n稠密检索: Contriever, GTR-XXL\n多向量检索: ColBERTv2\n\n\n训练过程: 训练分为多个迭代（Iteration）。\n\n第一阶段: 只使用第一个教师（如 uniCOIL）生成的排序列表来为增强查询构建训练样本（正负例）。\n第二阶段: 引入第二个教师（如 Contriever），此时，对于每个查询，模型会从第一和第二个教师的排序列表中均匀采样一个作为监督信号。\n后续阶段: 逐步引入更多的教师，在第 TTT 个阶段，模型会从前 TTT 个教师的监督信号中均匀采样。\n\n\n\nFigure 2 图解:\n\n该图直观地展示了渐进式监督的过程。\n\nIteration 1: 只有红色类别的标签（来自教师1）。模型从这些正负样本中学习。\nIteration 2: 在原有基础上，增加了绿色类别的标签（来自教师2）。此时训练数据的“视野”扩大了，模型需要学习兼容两种不同的相关性判断。\nIteration 3: 进一步增加了蓝色类别的标签（来自教师3）。模型学习的监督信号变得更加丰富和复杂。\n这个过程引导模型从简单、单一的视角逐步过渡到复杂、多元的视角，从而更有效地学习通用性。\n\n数学公式\nDRAGON 的训练遵循标准的稠密检索对比学习框架。\n\n相似度分数: 查询 qqq 和文档 ddd 之间的相似度通过它们向量表示的点积计算：\n\ns(q,d)≜Eq(q)⋅Ed(d)s(q, d) \\triangleq E_q(q) \\cdot E_d(d)\ns(q,d)≜Eq​(q)⋅Ed​(d)\n其中 EqE_qEq​ 和 EdE_dEd​ 是查询和文档的编码器（通常是 BERT），输出的是 [CLS] token 的向量表示。\n\n损失函数: 使用 InfoNCE 损失函数。对于一个查询 qqq、其相关的正样本 d+d^+d+ 和一组负样本 dj−j=1k{d_j^-}_{j=1}^kdj−​j=1k​，目标是最小化以下损失：\n\nL=−log⁡exp⁡(s(q,d+))exp⁡(s(q,d+))+∑j=1kexp⁡(s(q,dj−))L = -\\log \\frac{\\exp(s(q, d^+))}{\\exp(s(q, d^+)) + \\sum_{j=1}^k \\exp(s(q, d_j^-))}\nL=−logexp(s(q,d+))+∑j=1k​exp(s(q,dj−​))exp(s(q,d+))​\n这个损失函数旨在拉近查询与正样本的距离，同时推远与负样本的距离。在 DRAGON 的训练中，d+d^+d+ 和 dj−d_j^-dj−​ 是从教师模型生成的排序列表中采样得到的。\n\nBaseline\nDRAGON 与多种先进的检索模型进行了比较，涵盖了不同的架构和训练技术：\n\n不同架构模型:\n\nSPLADE++: 强大的稀疏检索模型。\nColBERTv2: 高效的轻量级晚期交互（multi-vector）模型。\nGTR-XXL: 参数量巨大的稠密检索模型（4.8B）。\n\n\n基线稠密检索模型 (BERT-base):\n\n知识蒸馏: RocketQAv2, CL-DRD。\n对比预训练: coCondenser, Contriever, COCO-DR。\n掩码自编码预训练: COT-MAE, RetroMAE。\n无监督领域自适应: GPL, PTR。\n\n\n\n\n数据集 (Datasets)\n\n训练数据来源:\n\nMS MARCO Passage Corpus (8.8M passages): 用于进行查询增强和标签增强，是生成所有训练数据的唯一语料库。\n\n\n评测数据集:\n\n监督式评测:\n\nMS MARCO Dev: 标准的领域内（in-domain）评测集。\nTREC Deep Learning (DL) 2019 &amp; 2020: 拥有更精细人工标注的监督评测集。\n\n\n零样本评测:\n\nBEIR: 一个包含18个不同领域、不同任务的异构零样本评测基准。\nLoTTE: 包含多个 StackExchange 社区问答数据的零样本评测集。\n\n\n\n\n\n\n可复现性 (Reproducibility)\n\n代码: 作者在论文中提供了 GitHub 仓库链接：https://github.com/facebookresearch/dpr-scale，包含了代码和模型 checkpoints。\n算力: 训练成本非常高昂。论文在“局限性”部分明确指出，训练一个完整的 DRAGON 模型需要：\n\n32 块 A100 (40 GB) GPUs\n耗时 5 天\n此外，还需要预先准备好所有训练好的教师模型，并使用它们对千万级别的增强查询进行推理以生成标签，这本身也是一个巨大的计算开销。\n\n\n\n\n可改进的几个点 (Potential Improvements)\n论文自身也指出了几个未来可以探索的方向：\n\n降低训练成本: 当前的训练数据规模高达 2800 万，其中可能包含大量重复或低质量的查询。设计一种有效的数据筛选或去重方法，可以在不牺牲性能的前提下大幅降低训练成本。\n与领域自适应结合: DRAGON 已经具备了很强的零样本能力。如果将其作为基础模型，再结合 GPL、PTR 等领域自适应技术进行微调，可能会在特定目标领域上取得更好的性能。\n与其他预训练方法结合: DRAGON+ 的实验表明，掩码自编码预训练（如 RetroMAE）与本文的对比学习方法是互补的。未来可以进一步探索如何更好地融合生成式和对比式预训练方法。\n特定领域的知识增强: 在 LoTTE 数据集的科技主题上，DRAGON 的表现不如 SPLADE++ 和 ColBERTv2。这表明，在训练中引入特定领域的语料库（如科学文献）可能会弥补这一不足。\n\n\n可以被引用的结论 (Key Takeaways)\n\n成功打破性能权衡: 本文首次证明，一个标准的 BERT-base 大小的稠密检索器，无需增加模型参数，就能同时在监督式和零样本评测中达到 SOTA 水平，打破了两者之间的固有权衡。\n多样化监督优于单一强监督: 与普遍认为“应使用最强的交叉编码器作为教师”的观点相反，本文发现，使用来自多个不同类型检索器（稀疏、稠密、多向量）的多样化监督信号，对于训练模型的泛化能力更为关键。\n渐进式学习策略的有效性: 渐进式监督（Progressive Supervision）作为一种课程学习策略，能有效引导模型学习复杂且多样的相关性信号，其效果优于一次性混合或融合所有监督信号。\n廉价数据增强的意外价值: 实验惊人地发现，通过句子裁剪这种廉价方式生成的增强查询，在提升模型泛化能力方面，效果甚至优于使用昂贵语言模型生成的“类人”查询（GenQ）。这为未来低成本、大规模训练通用检索器提供了新的思路。\n统一的数据增强框架: 论文提出的数据增强（DA）框架，为理解和归类稠密检索领域的各种训练技巧（如知识蒸馏、对比学习等）提供了一个统一且有价值的视角。\n\n","categories":["paper"],"tags":["paper","zero-shot","Dense Retrieval","Retrieval","supervised"]},{"title":"Making LLMs A Better Foundation For Dense Retrieval","url":"/paper/Making-LLMs-A-Better-Foundation-For-Dense-Retrieval/","content":"这篇论文的核心贡献是提出了一种名为 LLaRA (LLM adapted for dense RetrivAl) 的新方法，旨在解决大型语言模型（LLMs）在直接应用于稠密检索任务时的根本性问题。它通过一个高效的“事后适应”（post-hoc adaptation）阶段，显著提升了LLM作为检索模型基座（backbone）的能力。\n\n问题 (Problem)\n大型语言模型（LLMs）虽然在语义理解上能力强大，但其预训练方式与稠密检索的需求存在天然的“鸿沟” 。\n\nLLM的预训练目标：LLM（如GPT系列）主要通过自回归的文本生成任务进行预训练，其目标是预测下一个词元（token）。这使得模型生成的文本嵌入（text embedding）更侧重于捕捉**局部和短期（local and near-future）**的语义信息，以便生成连贯的下文。\n稠密检索的需求：稠密检索需要将查询（query）和文档（document）映射到一个语义空间中，并通过向量相似度来判断其相关性。这要求文本嵌入能够高度概括和表示**全局（global）**的语义信息 。\n核心矛盾：直接使用LLM（尤其是decoder-only架构）最后一个词元的输出嵌入作为全局表征，其效果会受到限制，因为这个嵌入本质上是为了生成任务优化的，而不是为了全局表示。\n\n\n方法 (Method)\n为了解决上述问题，作者提出了LLaRA，它通过两个精心设计的预训练任务（pretext tasks）来调整LLM，使其生成的嵌入更适合稠密检索。\n\n\n核心思想：通过让LLM的文本嵌入去完成“预测整个句子”而非“预测下一个词”的任务，来迫使其学习全局语义表示。\n\n\n模型框架 (Framework - Figure 1 解读)\n\n\n该框架展示了LLaRA如何处理一段文本（例如来自维基百科的两句话）。\n输入: “Norwegian forest cat is a breed cat originating in Northern Europe. This natural breed is adapted to a very cold climate.”\n处理: 模型将第一句话作为输入，并拼接上两个特殊的提示（prompt）：“the original sentence:” 和 “the next sentence:”。\n生成嵌入: LLM处理这个拼接后的序列，并在两个提示语后的特殊token（&lt;\\s&gt;）位置分别生成两个文本嵌入，一个用于EBAE任务，一个用于EBAR任务。\n任务:\n\nEBAE (绿色箭头): 使用第一个嵌入来重构（预测）原始输入句子“Norwegian forest cat…”。\nEBAR (蓝色箭头): 使用第二个嵌入来重构（预测）原始输入的下一句“This natural breed…”。\n\n\n\n\n\n两个预训练任务 (Two Pretext Tasks)\n\n\nEBAE (Embedding-Based Auto-Encoding):\n\n目标：让文本嵌入能够表示输入文本自身的全局语义。\n做法：使用LLM生成的文本嵌入 ete_tet​ 来预测输入句子本身的所有词元。如果一个嵌入能还原整个输入，那么它必然蕴含了完整的全局信息。\n应用场景：这个任务训练出的嵌入能力，天然适合处理“相似性搜索”场景，比如查找与输入文本语义上最接近的内容。\n\n\n\nEBAR (Embedding-Based Auto-Regression):\n\n目标：让文本嵌入学会关联查询（query）和文档（doc）。\n做法：使用文本嵌入 ete_tet​ 来预测输入文本的下一个句子。作者认为，一个相关的文档（如问题的答案）可以被看作是查询的一种“合理的下一句”。\n应用场景：这个任务训练出的嵌入能力，更适合处理“问答”这类需要推理和关联匹配的场景。\n\n\n\n\n\n数学公式 (Mathematical Formulas)\n\n稠密检索基础: 查询 qqq 和文档 ddd 的相关性通过嵌入向量的相似度计算：sim(q,d)=⟨eq,ed⟩sim(q, d) = \\langle e_q, e_d \\ranglesim(q,d)=⟨eq​,ed​⟩。\nLLaRA嵌入生成:\n\n为了提升效率，两个任务的提示语被合并成一个联合提示（joint prompt）：&quot;[输入文本] The original sentence: &lt;\\s&gt; The next sentence: &lt;\\s&gt;&quot;。并通过修改注意力掩码（attention mask）使两个任务的计算相互独立，在一个前向传播中同时得到两个嵌入。\nEBAE 嵌入: etα←LLaMA(T,SELF,⟨\\s⟩)[−1]e_{t}^{\\alpha} \\leftarrow LLaMA(T, SELF, \\langle\\backslash s\\rangle)[-1]etα​←LLaMA(T,SELF,⟨\\s⟩)[−1]，其中 SELF 代表提示 “The original sentence:”。\nEBAR 嵌入: etβ←LLaMA(T,NEXT,⟨s⟩)[−1]e_{t}^{\\beta} \\leftarrow LLaMA(T, NEXT, \\langle s\\rangle)[-1]etβ​←LLaMA(T,NEXT,⟨s⟩)[−1]，其中 NEXT 代表提示 “The next sentence:”。\n\n\n训练目标 (Training Objective):\n\nLLaRA的训练目标是让文本嵌入（etαe_t^\\alphaetα​ 或 etβe_t^\\betaetβ​）通过一个线性投影层来预测目标句子中的每一个词元。这被构建为一个多分类问题，其目标函数如下：min⁡∑t∈Texp⁡(eTWt)∑v∈Vexp⁡(eTWv)\\min \\sum_{t \\in \\mathcal{T}} \\frac{\\exp(e^T W_t)}{\\sum_{v \\in V} \\exp(e^T W_v)}\nmint∈T∑​∑v∈V​exp(eTWv​)exp(eTWt​)​\n\n公式解读:\n\neee: 指的是文本嵌入 etαe_t^\\alphaetα​ 或 etβe_t^\\betaetβ​。\nW∈R∣V∣×dW \\in \\mathbb{R}^{|V| \\times d}W∈R∣V∣×d: 是一个线性投影矩阵，将维度为 ddd 的文本嵌入映射到整个词表空间。VVV 是词表空间。\nT\\mathcal{T}T: 是目标句子的词元集合。对于EBAE，T\\mathcal{T}T是输入句子；对于EBAR，T\\mathcal{T}T是下一句。\n这个公式本质上是计算用单个全局嵌入 eee 去预测目标句子 T\\mathcal{T}T 中所有词元的交叉熵损失之和。\n\n\n\n\n\n\n\n\nBaseline\n论文与多种基线模型进行了全面对比：\n\n稀疏检索: BM25。\n基于BERT的稠密检索模型: ANCE, RetroMAE, SimLM等。这些是基于中等规模预训练模型（~110M参数）的代表性工作。\n基于LLM的稠密检索模型:\n\nGTR-XXL (4.8B), SGPT (5.8B), OpenAI-Ada-002。\nRepLLaMA (7B): 这是最重要的一个基线，因为它同样使用了LLaMA-2-7B模型和相同的微调方法（hard negatives），但没有经过LLaRA的适应性训练 。因此，与RepLLaMA的直接对比可以最干净地证明LLaRA方法本身的有效性。\n\n\n\n\n数据集 (Datasets)\n\n适应性训练 (Adaptation Training): 在DPR整理的无标签维基百科语料库上进行。这说明LLaRA不需要任何有监督标注数据，成本较低。\n下游任务微调 (Fine-tuning): MS MARCO 通道检索（passage retrieval）数据集。\n评测 (Evaluation):\n\nMS MARCO: 通道检索和文档检索（document retrieval）任务。\nBEIR benchmark: 一个异构的、包含多种检索场景（如问答、事实核查等）的基准测试集，用于评估模型的零样本（zero-shot）泛化能力。\n\n\n\n\n可复现性 (Reproducibility)\n\n代码: 论文承诺模型和源代码将在BGE的GitHub仓库中公开发布（https://github.com/FlagOpen/FlagEmbedding）。\n算力:\n\n模型基座: LLaMA-2-7B (base)。\n适应性训练: 在维基百科语料上训练了10,000步，批大小（batch size）为256，序列长度为1024。这对算力有一定要求。\n微调: 微调阶段采用了LoRA（Low-Rank Adaptation）技术，这是一种参数高效的微调方法，可以显著降低LLM微调的资源消耗。\n\n\n\n\n可改进的 1 个点\n\n与更先进的微调方法结合：\n\n论文中明确提到，LLaRA在微调阶段仅使用了简单的“ANN hard negatives”方法。作者自己也指出，如果未来能够利用更先进的微调技术，LLaRA的性能很可能被进一步提升。\n例如，可以尝试将LLaRA适应后的模型与知识蒸馏（knowledge distillation，从更强大的交叉编码器cross-encoder中学习）、更复杂的负采样策略（如SimANS）或其他对比学习框架结合，可能会在现有基础上取得更好的效果。\n\n\n\n\n可以被引用的一些结论\n\nLLM与检索任务的不匹配性: 大型语言模型因其自回归的生成式预训练目标，其输出嵌入天然地偏向于捕捉局部语义，这与稠密检索所需的全局语义表示存在明显的不匹配。\nLLaRA适应训练的有效性: LLaRA提出的事后适应方法能显著提升LLM的文本嵌入能力。与使用相同基座模型但未经适应的RepLLaMA相比，LLaRA在MS MARCO通道检索MRR@10上提升了1.9%，在文档检索MRR@100上提升了1.9%，在BEIR零样本评测NDCG@10上提升了1.0%。\nLLM作为检索基座的巨大潜力: 从基于BERT的编码器转向基于LLM的编码器，为稠密检索带来了巨大的性能飞跃。例如，在MS MARCO通道检索任务上，LLaRA相比之前的SOTA（如RetroMAE）在MRR@10指标上高出近4个百分点。\nLLM显著提升检索模型的泛化能力: 相比于在零样本场景下表现不佳的BERT类模型，基于LLM的检索器（特别是经过LLaRA优化的模型）在BEIR基准测试上展现了强大的泛化能力，平均性能远超BM25和BERT基线。\n\n","categories":["paper"],"tags":["paper","Dense Retrieval","Retrieval","LLM"]},{"title":"Multilingual E5 Text Embeddings","url":"/paper/Multilingual-E5-Text-Embeddings/","content":"核心问题 (Problem)\n现有的文本嵌入（Text Embedding）模型大多只在英文语料上进行训练，这极大地限制了它们在多语言场景下的应用。为了解决这一问题，微软的研究人员开发了一系列名为mE5（multilingual E5）的开源多语言文本嵌入模型，旨在提供在多种语言上都表现出色的高质量文本表示能力。\n\n核心方法 (Method)\n该论文的核心方法沿用了其英文版E5模型的两阶段训练流程：弱监督对比学习预训练 + 监督微调。此外，还引入了一个创新的**指令微调（instruction-tuned）**版本。\n模型架构与初始化\n研究人员发布了三种不同规模的模型，以平衡效果和效率：\n\nmE5-small: 基于multilingual-MiniLM初始化。\nmE5-base: 基于xlm-roberta-base初始化。\nmE5-large: 基于xlm-roberta-large初始化。\n\n第一阶段：弱监督对比学习预训练 (Weakly-supervised Contrastive Pre-training)\n此阶段的目标是让模型从海量无标注或弱标注数据中学习通用的多语言文本表示。\n\n\n训练数据: 使用了从多个来源收集的约10亿个多语言文本对。这些数据对的形式多样，例如（章节标题，段落内容）、（问题，回答）、（标题，新闻正文）等。\n\n\n数据源构成 (见下表):\n\n\n\n数据源\n样本量\n\n\n\n\nWikipedia\n1.5亿\n\n\nmC4\n1.6亿\n\n\nMultilingual CC News\n1.6亿\n\n\nNLLB\n1.6亿\n\n\nReddit\n1.6亿\n\n\nS2ORC\n5000万\n\n\nStackexchange\n5000万\n\n\nxP3\n8000万\n\n\nMisc. SBERT Data\n1000万\n\n\n总计\n约10亿\n\n\n\n\n\n\n\n训练目标: 采用标准的InfoNCE对比学习损失函数。对于一个给定的文本（锚点），模型需要将其对应的正例（如配对的标题和内容）的相似度拉近，同时将其与批次内所有其他不相关的文本（负例）的相似度推远。其数学公式如下：\nLInfoNCE=−log⁡exp⁡(sim(q,p+)/τ)∑i=1Nexp⁡(sim(q,pi)/τ)\\mathcal{L}_{\\text{InfoNCE}} = -\\log \\frac{\\exp(\\text{sim}(q, p_{+})/\\tau)}{\\sum_{i=1}^{N} \\exp(\\text{sim}(q, p_{i})/\\tau)}\nLInfoNCE​=−log∑i=1N​exp(sim(q,pi​)/τ)exp(sim(q,p+​)/τ)​\n其中，qqq 是查询（query）文本的嵌入，p+p_{+}p+​ 是其对应的正例（positive）文本的嵌入，pip_{i}pi​ 包含了正例和所有负例（in-batch negatives），τ\\tauτ 是温度超参数，用于调节相似度分布的平滑度。\n\n\n第二阶段：监督微调 (Supervised Fine-tuning)\n在预训练之后，模型会在少量高质量的标注数据集上进行微调，以进一步提升其在特定任务上的表现。\n\n\n训练数据: 使用了约160万个来自不同任务的高质量标注数据对。\n\n\n数据源构成 (见下表):\n\n\n\n数据源\n样本量（约）\n\n\n\n\nMS-MARCO Passage &amp; Document\n57万\n\n\nNQ, TriviaQA, SQUAD\n22万\n\n\nNLI\n27.5万\n\n\nELI5\n10万\n\n\nNLLB\n10万\n\n\nDuReader Retrieval\n8.6万\n\n\nFever\n7万\n\n\nHotpotQA\n7万\n\n\nQuora Duplicate Questions\n1.5万\n\n\nMr. TyDi\n5万\n\n\nMIRACL\n4万\n\n\n总计\n约160万\n\n\n\n\n\n\n\n优化技巧: 除了使用批内负例外，此阶段还引入了难负例挖掘（mined hard negatives）和来自交叉编码器（cross-encoder）模型的知识蒸馏（knowledge distillation），以增强嵌入的判别能力。\n\n\n指令微调模型 (Instruction-Tuned Model)\n为了让模型能更好地理解任务意图，研究者还训练了一个特殊的指令微调模型mE5-large-instruct。\n\n核心思想: 在输入文本前加入描述任务的自然语言指令（如“检索相关的段落”），让模型根据指令生成更具任务针对性的嵌入。\n特殊数据: 在监督微调的数据基础上，额外加入了由GPT-3.5/4生成的50万个合成数据。这些数据包含了15万个独特的指令，覆盖了93种语言，极大地增强了模型的泛化能力和多语言能力。\n\n\n基线模型 (Baseline)\n论文在多个基准上与当时最先进的多语言和单语言（英文）模型进行了对比。\n\n英文能力基准 (MTEB):\n\nLaBSE: 一个专门在翻译对上训练的多语言模型。\nCohere-multilingual-v3: 商业化的多语言模型。\nBGE-large-en-v1.5: 当时一个表现很强的纯英文模型。\n\n\n多语言检索基准 (MIRACL):\n\nBM25: 传统的稀疏检索算法。\nmDPR: 一个在MIRACL训练集上微调过的稠密检索模型。\n\n\n双语文本挖掘 (Bitext Mining):\n\nmContriever: 一个基于对比学习的检索模型。\nLaBSE: 在该任务上表现出色的强基线。\n\n\n\n\n数据集 (Datasets)\n训练数据集:\n\n预训练阶段: 详细构成见上方“核心方法”部分的表格，总量约为10亿对。\n微调阶段: 详细构成见上方“核心方法”部分的表格，总量约为160万对。mE5-large-instruct额外使用了50万合成数据。\n\n评估基准 (Evaluation Benchmarks):\n\nMTEB (Massive Text Embedding Benchmark): 用于评估模型在英文世界的综合能力，涵盖了分类、聚类、排序、检索、语义相似度等多种任务（共56个数据集）。\nMIRACL (Multilingual Information Retrieval Across a Continuum of Languages): 一个多语言检索基准，论文中评估了其在16种不同语言上的表现。\nBitext Mining: 跨语言相似度搜索任务，用于评估模型在没有词汇重叠情况下匹配语义相似句子的能力。使用了BUCC 2018（4种语言）和Tatoeba（112种语言）两个数据集。\n\n\n可复现性 (Reproducibility)\n\n代码与模型: 论文中明确指出模型权重和相关信息已在GitHub上公开发布：https://github.com/microsoft/unilm/tree/master/e5。这使得社区可以轻松使用和复现这些模型。\n算力与超参数:\n\n预训练: 使用了高达32,000的批处理大小（batch size），训练了30,000步。这需要非常强大的计算资源（通常是多机多卡的GPU集群）。\n微调: 批处理大小为512，训练2个周期（epoch）。\n学习率: 针对不同尺寸的模型和不同训练阶段设置了不同的学习率（预训练为{3,2,1}×10−4\\{3,2,1\\}\\times10^{-4}{3,2,1}×10−4，微调为{3,2,1}×10−5\\{3,2,1\\}\\times10^{-5}{3,2,1}×10−5，分别对应small/base/large模型）。\n复现门槛: 普通研究者或开发者可以轻松下载模型进行微调和推理，但从头开始复现整个预训练过程的成本极高。\n\n\n\n\n可改进的几个点 (Potential Improvements)\n\n合成数据的质量与偏差: mE5-large-instruct的卓越性能部分归功于GPT生成的合成数据。然而，这些数据可能继承了大型语言模型的偏见或事实性错误，对其进行更深入的分析和清洗可能会进一步提升模型鲁棒性。\n对极低资源语言的覆盖: 尽管模型覆盖了超过100种语言，但对于那些在预训练数据中出现频率极低的语言，其性能可能仍然有限。未来的工作可以探索如何通过迁移学习或更有效的数据采样策略来提升这些语言的效果。\n模型效率与压缩: 论文提到了小尺寸模型在效率上的优势，但性能有所牺牲。可以进一步研究模型压缩技术（如量化、剪枝）来减小mE5-large模型的存储和推理开销，同时尽可能保持其高性能。\n指令的泛化能力: 指令微调是一个很有前景的方向。可以探索更复杂、更多样化的指令形式，甚至让模型能理解零样本（zero-shot）的未知指令，从而提升其在更广泛任务上的泛化能力。\n跨语言知识的对齐: 虽然模型在多语言任务上表现出色，但其内部如何对齐不同语言的语义空间仍值得深入探究。更显式地进行跨语言对齐（cross-lingual alignment）的训练或许能带来性能提升。\n\n\n可以被引用的一些结论 (Citable Conclusions)\n\n两阶段训练范式的有效性: “弱监督对比学习预训练 + 监督微调”的训练范式是构建高性能多语言文本嵌入模型的有效路径。\n指令微调的巨大潜力: 通过使用包含任务指令的合成数据进行微调，mE5-large-instruct模型的性能得到了显著提升，甚至在英文基准上超越了同等规模的强力纯英文模型（如BGE-large-en-v1.5）。\n多语言能力的领先水平: 在多语言检索基准MIRACL上，mE5系列模型显著优于经过特定任务微调的mDPR模型。在跨语言文本匹配任务（Bitext Mining）上，mE5-large-instruct也超越了专为此类任务设计的LaBSE模型。\n一个模型，多种用途: mE5模型不仅在多语言检索任务上表现优异，也在语义相似度、文本分类和聚类等多种NLP任务中展现出强大的、可迁移的特征提取能力。\n开源模型的价值: 通过开源不同规模的mE5模型，该工作为学术界和工业界在多语言信息检索、检索增强生成（RAG）等领域提供了强大且易于使用的基础工具。\n\n","categories":["paper"],"tags":["paper","Embeddings"]},{"title":"RAG for Knowledge-Intensive NLP Tasks","url":"/paper/RAG-for-Knowledge-Intensive-NLP-Tasks/","content":"1. 摘要与引言 (Abstract &amp; Introduction)\n\n\n核心问题: 大型预训练语言模型（如BART, T5）虽然在参数中存储了大量事实知识，但在知识的精确访问和操作上能力有限 。它们无法轻易扩展或修正知识，其决策过程缺乏透明度，并且可能产生“幻觉”（即捏造事实）。\n\n\n解决方案: 提出一种名为 RAG (Retrieval-Augmented Generation) 的通用模型框架，它将预训练的参数化记忆（一个seq2seq模型）与非参数化记忆（一个可通过神经检索器访问的密集向量索引，如维基百科）相结合 。\n\n\n模型构成: RAG模型包含一个预训练的seq2seq生成器（BART）和一个预训练的密集段落检索器（DPR）。\n\n\n主要贡献:\n\n\n在三个开放域问答任务上取得了最先进的（SOTA）成果 。\n\n\n在语言生成任务上，RAG比纯参数化模型生成的内容更具体、更多样、更符合事实 。\n\n\n展示了通过替换非参数化记忆（文档索引）来有效更新模型世界知识的能力 。\n\n\n\n\n\n2. 方法 (Methods)\n\n\n整体架构 (Figure 1):\n\n\n\n输入查询 (x) 进入 查询编码器 (Query Encoder) 生成查询向量 q(x) 。\n\n\n使用 最大内积搜索 (MIPS) 在 文档索引 (Document Index) 中检索与 q(x) 最相关的Top-K个文档 z 。\n\n\n生成器 (Generator) 将原始输入 x 和检索到的文档 z 结合起来，生成最终输出 y 。\n\n\n整个过程通过端到端训练，同时优化生成器和查询编码器 。\n\n\n\n\nRAG的两种模型:\n\n\nRAG-Sequence: 模型为整个输出序列选择同一个检索文档 z 。其概率模型是对不同文档 z 生成完整序列 y 的概率进行加权求和 。\n\n公式:\np(y∣x)≈∑z∈top−k(p(⋅∣x))pη(z∣x)pθ(y∣x,z)=∑z∈top−k(p(⋅∣x))pη(z∣x)Πi=1Npθ(yi∣x,z,y1:i−1)p(y|x) \\approx \\sum_{z \\in top-k(p(·|x))} p_\\eta(z|x)p_\\theta(y|x,z) = \\sum_{z \\in top-k(p(·|x))} p_\\eta(z|x)\\Pi_{i=1}^{N}p_\\theta(y_i|x,z,y_{1:i−1})p(y∣x)≈∑z∈top−k(p(⋅∣x))​pη​(z∣x)pθ​(y∣x,z)=∑z∈top−k(p(⋅∣x))​pη​(z∣x)Πi=1N​pθ​(yi​∣x,z,y1:i−1​)\n\n\n\nRAG-Token: 模型在生成每一个目标词元 (token) 时，都可以从多个文档中汲取信息 。其概率模型是在生成每个词元时，都对所有Top-K文档的贡献进行一次加权求和，然后将每一步的概率连乘 。\n\n公式:\np(y∣x)≈Πi=1N∑z∈top−(p(⋅∣x))kpη(z∣x)pθ(yi∣x,z,y1:i−1)p(y|x) \\approx \\Pi_{i=1}^{N} \\sum_{z∈top-(p(·|x))k}p_\\eta(z|x)p_\\theta(y_i|x,z,y_{1:i−1})p(y∣x)≈Πi=1N​∑z∈top−(p(⋅∣x))k​pη​(z∣x)pθ​(yi​∣x,z,y1:i−1​)\n\n\n\n\n\n核心组件:\n\n\n检索器 (Retriever): 基于DPR（Dense Passage Retriever），它包含一个BERT-base的查询编码器和文档编码器。\n\n\n生成器 (Generator): 使用BART-large，一个拥有4亿参数的预训练seq2seq模型。检索到的文档内容与原始输入被简单地拼接在一起送入BART。\n\n\n非参数化记忆: 使用2018年12月的维基百科快照，分割成2100万个100词的文档块。\n\n\n\n\n训练与解码:\n\n\n训练: 联合训练查询编码器和生成器，但保持文档编码器（和索引）固定以降低计算成本。\n\n\n解码: RAG-Token可以直接使用标准beam search解码 。RAG-Sequence需要为每个检索到的文档分别运行beam search，然后对生成的候选集进行边缘化概率计算，有&quot;Thorough Decoding&quot;和&quot;Fast Decoding&quot;两种方式。\n\n\n\n\n\n3. 实验与结果 (Experiments &amp; Results)\n\n\n开放域问答 (Open-domain QA):\n\n\n在Natural Questions (NQ), WebQuestions (WQ), 和 CuratedTrec (CT) 数据集上，RAG均取得了SOTA成绩。\n\n\nRAG-Sequence在TQA数据集上也超过了T5-11B+SSM。\n\n\n即使正确答案未出现在任何检索到的文档中，RAG仍能在NQ上实现11.8%的准确率，这是纯抽取式模型无法做到的。\n\n\n\n\n生成任务 (Generation Tasks):\n\n\nMS-MARCO: RAG-Sequence在Bleu和Rouge-L指标上均优于BART基线。\n\n\nJeopardy问题生成: RAG-Token在Q-BLEU-1指标上表现最佳。\n人工评估显示，RAG的生成结果在事实性 (42.7% vs 7.1%) 和具体性 (37.4% vs 16.8%) 上远超BART。\n\n\n多样性: RAG的生成结果比BART更多样化，其中RAG-Sequence的多样性最高。\n\n\n\n\n事实核查 (Fact Verification):\n\n\n在FEVER任务中，RAG在未接收任何检索监督信号的情况下，其准确率与经过复杂设计的SOTA模型差距在4.3%以内。\n\n\nRAG检索到的Top-10文档中有90%的概率包含FEVER任务中的黄金标准证据文章。\n\n\n\n\n核心能力分析:\n\n\n知识更新 (Hot-swapping): 实验证明，通过切换2016年和2018年的维基百科索引，RAG能正确回答相应年份的问题，证明了其知识可以被轻松更新。\n\n\n记忆协同: 通过分析生成过程，发现检索的非参数化记忆可以引导并“触发”生成器的参数化记忆，使其生成存储在内部的特定知识（如书名。\n\n\n可学习的检索: 消融实验表明，对查询编码器进行微调（Learned Retrieval）对于大多数任务的性能至关重要，尤其是在开放域问答上。\n\n\n\n\n\n4. 讨论与影响 (Discussion &amp; Broader Impact)\n\n\n总结: RAG成功地将参数化和非参数化记忆结合，在知识密集型任务上表现出色，生成的文本更真实、更具体，并且知识易于更新。\n\n\n社会效益: RAG更强地根植于事实知识，能减少“幻觉”，提供更好的可解释性和可控性。\n\n\n潜在风险: 与其他大型语言模型类似，RAG也可能被用于生成误导性内容或垃圾邮件。但其事实性 grounding 可以在一定程度上缓解此问题。\n\n\n","categories":["paper"],"tags":["paper","RAG","NLP"]},{"title":"SELF-REFINE: Iterative Refinement with Self-Feedback","url":"/paper/SELF-REFINE-Iterative-Refinement-with-Self-Feedback/","content":"问题 (Problem)\n大型语言模型（LLMs）虽然功能强大，但其初次生成的输出往往不是最优的，尤其是在处理具有复杂约束或多方面目标（如对话生成、代码优化）的任务时。传统的优化方法通常需要大量的监督训练数据、额外的模型训练或强化学习，这些都成本高昂且不易获取。因此，研究的核心问题是：能否让大型语言模型在不进行额外训练或使用外部监督数据的情况下，仅通过自我反思和修正来迭代式地提升其输出质量？\n\n方法 (Method)\n为了解决上述问题，该研究提出了 SELF-REFINE 框架，其核心思想是利用同一个大型语言模型，让它扮演三个角色：生成者 (Generator)、反馈提供者 (Feedback Provider) 和 精炼者 (Refiner)。整个过程是一个迭代循环，直到满足停止条件为止。\n方法流程详解:\n该方法主要包含三个核心步骤：生成 (Generate)、反馈 (Feedback) 和 精炼 (Refine)。\n第一步：初始生成 (Initial Generation)\n给定一个输入 xxx 和一个用于初始生成的提示 pgenp_{gen}pgen​，模型 M\\mathcal{M}M 首先会生成一个初始输出 y0y_0y0​。\ny0=M(pgen∣∣x)y_0 = \\mathcal{M}(p_{gen} || x)\ny0​=M(pgen​∣∣x)\n这里的 pgenp_{gen}pgen​ 通常是一个任务相关的 few-shot 提示，包含了若干输入-输出的示例。\n第二步：反馈 (Feedback)\n接下来，将初始输出 y0y_0y0​ 连同原始输入 xxx 一起，再次输入给同一个模型 M\\mathcal{M}M，但这次使用一个用于生成反馈的提示 pfbp_{fb}pfb​。模型会针对自己的输出 yty_tyt​ (在第 ttt 次迭代中) 给出反馈 fbtfb_tfbt​。\nfbt=M(pfb∣∣x∣∣yt)fb_t = \\mathcal{M}(p_{fb} || x || y_t)\nfbt​=M(pfb​∣∣x∣∣yt​)\n研究强调，高质量的反馈应该是具体的 (specific) 和可操作的 (actionable)。例如，在代码优化任务中，一个好的反馈不会是“提高代码效率”，而应该是“这段代码使用了暴力循环，速度很慢，可以尝试使用数学公式 (n(n+1))/2(n(n+1))/2(n(n+1))/2 来优化”。\n第三步：精炼 (Refine)\n模型接收到自己给出的反馈 fbtfb_tfbt​ 后，结合原始输入 xxx 和历史的输出与反馈，使用精炼提示 prefinep_{refine}prefine​ 来生成一个改进后的输出 yt+1y_{t+1}yt+1​。为了让模型能从历史错误中学习，之前所有的输出-反馈对都会被保留在上下文中。\nyt+1=M(prefine∣∣x∣∣y0∣∣fb0∣∣...∣∣yt∣∣fbt)y_{t+1} = \\mathcal{M}(p_{refine} || x || y_0 || fb_0 || ... || y_t || fb_t)\nyt+1​=M(prefine​∣∣x∣∣y0​∣∣fb0​∣∣...∣∣yt​∣∣fbt​)\n迭代与停止:\n上述的“反馈”和“精炼”步骤会循环进行。停止条件可以是固定的迭代次数（例如，最多4次），或者是模型在反馈中生成了一个特定的停止信号（例如，反馈内容为“输出质量已经很高，无需修改”）。\n图例解读:\n\n\n图1 (Figure 1) 直观地展示了SELF-REFINE的循环流程。输入 xxx 首先通过模型 M\\mathcal{M}M 得到初始输出，然后进入一个循环：\n\n反馈 (Feedback): 模型 M\\mathcal{M}M 评估自己的输出并给出反馈。\n精炼 (Refine): 模型 M\\mathcal{M}M 根据收到的反馈来修正上一步的输出。\n这个循环不断重复，直到满足停止条件。\n\n\n\n\n\n图2 (Figure 2) 给出了两个具体的例子：\n\n对话生成: 初始回答“我相信这是社交和保持活力的好方法”比较平淡。模型给出的反馈是“缺乏互动性，未能理解用户需求”。精炼后的回答则更具互动性，并提出了引导性问题：“你以前玩过吗，还是想学？”\n代码优化: 初始代码使用 for 循环计算从1到N的和，效率较低。模型反馈指出这是“暴力方法”，并建议使用数学公式。精炼后的代码直接使用了公式 (n*(n+1))//2，效率大大提升。\n\n\n\n\n\n算法1 (Algorithm 1) 提供了该方法的伪代码，清晰地描述了从初始生成到迭代反馈和精炼，直至最终返回结果的完整逻辑。\n\n\nBaseline\n该研究的核心对比方法是直接生成 (Direct Generation)，即使用同一个基础大型语言模型（Base LLM），在不经过SELF-REFINE的迭代反馈和精炼过程的情况下，一次性生成最终输出。这种对比方式能够最直接地证明 SELF-REFINE 框架自身的有效性。\n在具体的任务评估中，研究还与其他特定领域的SOTA（State-of-the-art）方法进行了比较，例如：\n\n数学推理任务 (GSM8K): 与 PaL (Program-aided Language Models) 和 Self-Correct 等方法进行了比较。\n代码优化任务 (PIE): 与 CODEX, CODEGEN, SCALENE, 以及专门为该任务微调的 PIE 模型进行了比较。\n\n\n数据集 (Datasets)\n该研究在7个多样化的任务上对 SELF-REFINE 进行了评估，涵盖了自然语言和代码两大领域：\n\n对话响应生成 (Dialogue Response Generation): 使用 FED 数据集，旨在生成更丰富、更具互动性的对话。\n代码优化 (Code Optimization): 使用 PIE 数据集，目标是提升Python代码的运行效率。\n代码可读性改进 (Code Readability Improvement): 使用 CodeNet 数据集，旨在通过重构使代码更易于理解。\n数学推理 (Math Reasoning): 使用 GSM8K 数据集，解决小学数学应用题。\n情感反转 (Sentiment Reversal): 使用 Yelp 评论数据集，将评论的正面或负面情感进行反转。\n首字母缩写词生成 (Acronym Generation): 研究团队为此自建了一个包含250个缩写词的数据集。\n约束生成 (Constrained Generation): 基于 CommonGen 数据集，但增加了约束难度，要求生成的句子包含20-30个给定的关键词。\n\n\n可复现性 (Reproducibility)\n\n代码: 论文明确表示，所有代码、数据和使用的提示都已匿名公开在网站 https://selfrefine.info/ 上，这为复现研究提供了极大的便利。\n算力/模型: 实验主要使用了 OpenAI 的闭源 API 模型，包括 GPT-3.5 (text-davinci-003), ChatGPT (gpt-3.5-turbo), 和 GPT-4。对于代码任务，还使用了 CODEX (code-davinci-002)。这意味着复现研究需要调用这些商业API，会产生一定的费用。论文也坦诚地指出了这一限制。此外，研究还尝试了开源模型 Vicuna-13B，但发现其遵循复杂指令（尤其是在反馈和精炼环节）的能力较弱，效果不佳，这表明该方法对模型的基础能力有较高要求。\n\n\n可改进的几个点\n\n对弱模型的适应性: 当前方法在像 Vicuna-13B 这样的较弱模型上表现不佳。未来的工作可以探索如何调整提示工程或引入更轻量级的微调，使该框架能够更好地适用于能力稍弱或更专业化的开源模型。\n反馈质量的稳定性: 实验分析表明，错误的反馈是导致精炼失败的主要原因。可以研究如何让模型生成更稳定、更准确的反馈，例如通过引入多轮反馈审查机制，或者让模型对自己的反馈进行可信度评分。\n迭代效率问题: 多轮迭代会增加时间和计算成本。可以研究更智能的停止策略，例如，当连续两轮的改进幅度低于某个阈值时提前终止，或者设计一种机制让模型在单轮内进行更深度的“反思”，从而减少迭代次数。\n处理复杂长任务的能力: 对于需要长链条推理或多步骤规划的任务（如长篇故事写作、复杂软件开发），当前的反馈机制可能过于局部。可以探索层次化的 SELF-REFINE 结构，即先对整体大纲进行反馈和精炼，再逐步细化到段落或模块级别。\n对错误反馈的鲁棒性: 论文提到，精炼器有时能够修正部分错误的反馈。可以系统性地研究和提升模型对噪声或错误反馈的鲁棒性，使其在反馈不完美时也能做出合理的改进。\n\n\n可以被引用的一些结论\n\n核心贡献: “大型语言模型可以通过一种名为SELF-REFINE的迭代自反思框架，在无需额外训练数据或强化学习的情况下，显著提升其在多种任务上的输出质量。”\n性能提升: “在横跨对话生成、代码优化和数学推理等7个不同任务的评估中，使用SELF-REFINE后的GPT-3.5和GPT-4模型，其性能平均绝对提升了约20%。”\n反馈的重要性: “实验表明，反馈的质量至关重要。与通用的或无反馈的迭代相比，生成具体且可操作的反馈能带来最显著的性能提升。例如，在情感反转任务中，移除有效反馈会导致任务完全失败。”\n迭代的价值: “多轮迭代是有效的。分析显示，随着迭代次数的增加，输出质量通常会持续提升，尽管大部分改进发生在最初的几轮迭代中，之后收益递减。”\n模型能力要求: “SELF-REFINE框架的成功应用依赖于基础模型强大的指令遵循和上下文学习能力。在能力较弱的模型（如Vicuna-13B）上，该框架难以有效执行反馈和精炼步骤。”\n超越简单采样: “SELF-REFINE的效果优于简单地生成多个候选输出并择优的策略。实验证明，经过迭代精炼后的单个输出，其质量通常优于一次性生成的多个初始输出中的任何一个。”\n对SOTA模型的提升: “即使是像GPT-4这样顶尖的模型，其初始输出也并非完美。SELF-REFINE证明了在测试阶段（test-time）通过自我迭代，依然可以进一步解锁这些强大模型的潜力。”\n\n","categories":["paper"],"tags":["paper","LLM","refine"]},{"title":"Maximum Flow","url":"/DataStru-Algo/Maximum-Flow/","content":"\n免责声明：code太难写了，本章就不提供code了，各位自行GPT吧（\n\n基本概念阐述\n问题阐述\n\n输入：一个有向有权图 G=(V,E)G=(V,E)G=(V,E) ，源节点 sss ，汇点 ttt​\n目标：从 sss 发送尽可能多的水到 ttt​\n约束：\n\n流要小于管道容量\n\n\n\n流网络和流\n\n\n流网络\n\nG=(V,E)G=(V,E)G=(V,E)​ 是一个有向有权图\n\n∣E∣≥∣V∣−1|E|\\ge |V|-1∣E∣≥∣V∣−1\n\n\n图中每一条边 (u,v)∈E(u,v)\\in E(u,v)∈E 有一个非负的容量值 c(u,v)≥0c(u,v)\\ge 0c(u,v)≥0\n\n若 (u,v)∉E(u,v)\\notin E(u,v)∈/E ，则定义 c(u,v)=0c(u,v)=0c(u,v)=0\n\n\n源节点 sss ，汇点 ttt\n\n\n\n流 flowflowflow\n\n\n在一个流网络 G=(V,E)G=(V,E)G=(V,E) 中，设容量函数为 ccc ，源节点 sss ，汇点 ttt\n\n\nGGG 中的流为一个实值函数 f:V×V→Rf:V \\times V \\to Rf:V×V→R ，满足以下两条性质​：\n\n容量限制：对于所有的节点，0≤f(u,v)≤c(u,v)0\\le f(u,v)\\le c(u,v)0≤f(u,v)≤c(u,v) ​\n流量守恒：对于除源节点、汇点外（V−{s,t}V-\\{s,t\\}V−{s,t}）的所有节点，∑v∈Vf(v,u)=∑v∈Vf(u,v)\\sum_{v \\in V}f(v,u)=\\sum_{v \\in V} f(u,v)∑v∈V​f(v,u)=∑v∈V​f(u,v)​\n\n即流入 uuu 的总流量等于从 uuu​ 流出的总流量\n前后两个 vvv 一般是不同的，所以我更倾向写成 ∑v∈Vf(v,u)=∑v′∈Vf(u,v′)\\sum_{v \\in V}f(v,u)=\\sum_{v&#x27; \\in V} f(u,v&#x27;)∑v∈V​f(v,u)=∑v′∈V​f(u,v′)\n当 (u,v)∉E(u,v)\\notin E(u,v)∈/E ，则 f(u,v)=0f(u,v)=0f(u,v)=0​\n\n\n\n\n\n一个流的值 ∣f∣|f|∣f∣ 定义为：\n∣f∣=∑v∈Vf(s,v)−∑v∈Vf(v,s)|f|=\\sum_{v\\in V}f(s,v)-\\sum_{v \\in V}f(v,s)\n∣f∣=v∈V∑​f(s,v)−v∈V∑​f(v,s)\n\n表示从源结点流入该节点的总流量减去流入源结点的总流量。\n一般而言，没有流入源节点的流，即第二项的值为 000​ ，所以也可以写成：\n\n\n\n∣f∣=∑v∈Vf(s,v)|f|=\\sum_{v\\in V}f(s,v)\n∣f∣=v∈V∑​f(s,v)\n\n\n\n\n\n\n残存网络 Residual GraphResidual\\ GraphResidual Graph​\n\n\n残存边 (u,v)(u,v)(u,v) ，残存容量 cf=c(u,v)−f(u,v)c_f=c(u,v)-f(u,v)cf​=c(u,v)−f(u,v)​​​\n回流边 (v,u)(v,u)(v,u) ，回流值大小 cf′=f′(v,u)=f(u,v)c_{f&#x27;}=f&#x27;(v,u)=f(u,v)cf′​=f′(v,u)=f(u,v) ——在 Ford−FulkersonFord-FulkersonFord−Fulkerson 方法里会用到\n\ncf′=∑f′(v,u)c_{f&#x27;}=\\sum f&#x27;(v,u)cf′​=∑f′(v,u) ，也即合并 MergeMergeMerge 操作，后面会给出证明\n\n\n\n增广路径 Augmenting pathsAugmenting\\ pathsAugmenting paths\n\n\n增广路径 ppp 是从源节点 sss 到汇点 ttt 的一条简单路径\n路径 ppp 的瓶颈容量 cf(p)=min{cf(u,v):(u,v)∈p}c_f(p)=min\\{c_f(u,v):(u,v)\\in p\\}cf​(p)=min{cf​(u,v):(u,v)∈p}​\n\n切割\n切割，字面意思，就是把图切割成 nnn 份。在本章中，我们主要讨论的是 S−T cut\\mathcal{S-T}\\ cutS−T cut ，即把图切割成两份。\n\n\nS−T cut\\mathcal{S-T}\\ cutS−T cut\n\n\n将节点集合 VVV 切割成两个子集：S\\mathcal{S}S 和 T\\mathcal{T}T​\n\nS∪T=V\\mathcal{S} \\cup \\mathcal{T}=VS∪T=V ，S∩T=∅\\mathcal{S} \\cap \\mathcal{T}=\\emptysetS∩T=∅\ns∈Ss \\in \\mathcal{S}s∈S ， t∈Tt \\in \\mathcal{T}t∈T​\n\n\n\nS\\mathcal{S}S 和 T\\mathcal{T}T 的二元组 (S,S)(\\mathcal{S},\\mathcal{S})(S,S) 被称为 S−T cut\\mathcal{S-T}\\ cutS−T cut​\n\n\n定义 S−T\\mathcal{S-T}S−T 的容量为离开集合 S\\mathcal{S}S​ 的边的权重之和，如下：\nc(S,T)=∑u∈S∑v∈Tc(u,v)c(\\mathcal{S},\\mathcal{T})=\\sum_{u \\in \\mathcal{S}}\\sum_{v \\in \\mathcal{T}}c(u,v)\nc(S,T)=u∈S∑​v∈T∑​c(u,v)\n\n例如下图的 c(S,T)=6c(\\mathcal{S},\\mathcal{T})=6c(S,T)=6\n\n\n\n定义切割的净流量 f(S,T)f(\\mathcal{S},\\mathcal{T})f(S,T) 如下：\nf(S,T)=∑u∈S∑v∈Tf(u,v)−∑u∈S∑v∈Tf(v,u)f(\\mathcal{S},\\mathcal{T})=\\sum_{u \\in \\mathcal{S}}\\sum_{v \\in \\mathcal{T}}f(u,v)-\\sum_{u \\in \\mathcal{S}}\\sum_{v \\in \\mathcal{T}}f(v,u)\nf(S,T)=u∈S∑​v∈T∑​f(u,v)−u∈S∑​v∈T∑​f(v,u)\n\n例如下图的 f(S,T)=f(v1,v3)+f(v1,v4)+f(v2,v4)−f(v4,s)=2+2+2−4=2f(\\mathcal{S},\\mathcal{T})=f(v_1,v_3)+f(v_1,v_4)+f(v_2,v_4)-f(v_4,s)=2+2+2-4=2f(S,T)=f(v1​,v3​)+f(v1​,v4​)+f(v2​,v4​)−f(v4​,s)=2+2+2−4=2\n\n\n\ne.g.\n\n\n \n     \n     \n \n\n注意，S−T cut\\mathcal{S-T}\\ cutS−T cut 不唯一\n\n \n     \n \n\n\n最小割 Min−CutMin-CutMin−Cut\n\n使容量最小化的切割被称为最小割 Min−CutMin-CutMin−Cut\ne.g.\n\n \n     \n     \n \n\n\nNaive AlgorithmNaive\\ AlgorithmNaive Algorithm 引入\n\nNaive 不是一个人名，它的意思是天真的；幼稚的。这里应译为朴素算法\n\n​\t这个算法是初步的算法，并不一定能找到最大流（只能找到阻塞流），只是大多数情况下可以找到最大流，但是这种方法很好理解，并且后面的更优的算法都是以此为基础进行优化的，所以我们先介绍这种算法。\n算法实现步骤\n\n创建一个残存图 (residual graph)(residual\\ graph)(residual graph) ，初始化残存容量\nwhilewhilewhile  可以找到增广路径 :\n\n找一条增广路径\n找出增广路径中的瓶颈容量 xxx\n更新这条增广路径上每一条边，cf=cf−xc_f=c_f-xcf​=cf​−x ，并删除饱和边\n\n\n\n图示\n\n\n初始化\n\n\n\n第一轮循环\n\n\n随便用什么遍历方法找到一条从 sss 到 ttt​​​ 的增广路径，找到瓶颈容量，更新这条路径上的每一条边\n  \n  \n  \n  \n\n\n此时有两条边的余量为0，说明这两条管道已经饱和了，于是把余量为0的边删除\n\n\n\n\n\n\n\n\n\n第二轮循环\n\n同样的操作\n\n\n\n\n\n\n\n\n\n第三轮循环\n\n同样的操作\n\n\n\n\n    \n    \n    \n\n\n循环结束\n\n用原始图减去最终的残存图，即可得到流量图\n实际流量可以从源节点 sss 去进行计算\n\n\n\n\n    \n    \n\n该简单算法局限性\n该算法在寻找路径的时候，如果找到的路径是错误的，则最终找到的有可能不是最大流\n\n    \n    \n\n\n\t\n\t\n\n\n    \n    \n\n\n阻塞流   blocking flowblocking\\ flowblocking flow\n\n\n如果从源节点 sss 到汇点 ttt ，不能找到更多的流汇入，则该流是阻塞流\n最大流是阻塞流\n\nFord−FulkersonFord-FulkersonFord−Fulkerson 算法\n​\t上面的简单算法一旦选择了 bad pathbad\\ pathbad path 后，不能修正，就只能找到阻塞流。\n​\t而下面我们介绍的算法，则是以上面的算法为基础进行优化的，Ford−FulkersonFord-FulkersonFord−Fulkerson 算法添加了回流这一步操作，假如选到了阻塞流，则可以进行回流修正，从而解决了问题。\n图示\n​\t我们以刚刚的基本算法的示例图来进行算法介绍\n\n依旧进行初始化\n\n\n    \n\n\n\n第一轮循环\n\n这三步和之前的 naive algorithmnaive\\ algorithmnaive algorithm​ 都是一样的，关键在于之后的一步\n\n  \n  \n  \n  \n \n - 在删除了饱和边之后，$Ford-Fulkerson\\ Algorithm$ 增加了一步回流法：画出该增广路径的回流边，即从 $t$ 到 $s$​ 的流向。并且这些回流边在下一轮选择路径试仍然可以被选中。\n- 为什么可以这么做？这样做对算法的正确性有无影响？这里先介绍完算法，后文会给出正确性证明。\n\n\n\n\n\n\n第二轮循环\n\n依旧如此，进行同样的操作\n\n  \n \t\n \t\n \t\n \n - 此时我们注意到：在 $v_1 \\to s$​ 这条路径，有两条回流边，此时我们很容易就会想到一个问题，这两条边，是否可以合并？\n    - 答案是肯定的，同样的，我们在算法正确性小节会给出证明\n    - 第二轮循环结束的图如下：\n \n\n\n第三轮循环\n\n第三轮循环即是刚刚简单算法失败的地方，我们来看 Ford−FulkersonFord-FulkersonFord−Fulkerson​ 算法是如何纠正的\n\n此图很清晰地说明了回流边的作用——给后面的纠错预留操作空间\n\n\n\n\n\n\n    \n    \n    \n    \n\n\n\n循环结束\n\n此时，图中没有从 sss 到 ttt​ 的路径了，于是算法终止。\n\n \n     \n \n\n因此，我们可以得到流量图，且该图的 Maximum FlowMaximum\\ FlowMaximum Flow 为 555 。\n\n \n     \n     \n \n\n\n算法实现步骤\n刚刚我们介绍了 Naive AlgorithmNaive\\ AlgorithmNaive Algorithm ，对于 Ford−FulkersonFord-FulkersonFord−Fulkerson 算法的步骤，也只需添加一句话：\n\n创建一个残存图 (residual graph)(residual\\ graph)(residual graph) ，初始化残存容量\nwhilewhilewhile  可以找到增广路径 :\n\n找一条增广路径\n找出增广路径中的瓶颈容量 xxx\n更新这条增广路径上每一条边，cf=cf−xc_f=c_f-xcf​=cf​−x​​ ，并删除饱和边\n==添加该增广路径的回流边==\n\n\n\n最坏时间复杂度分析\n\n\nwhilewhilewhile​​ 循环最坏情况\n​\t不难看出，该图的最大流为 200200200 ，若选择路径时，非常&quot;好运&quot;地选择了 s→v1→v2→ts \\to v_1 \\to v_2 \\to ts→v1​→v2​→t 这条路径，则会发生一些 amazingamazingamazing 的事情，即后面两张图。此时，循环就会进行 200200200 遍，即进行 Amount of MaxFlowAmount\\ of\\ MaxFlowAmount of MaxFlow 遍。\n\n\n\n    \n    \n    \n\n​\t\t以下给出证明：\n​\t\t\t由于每一轮循环中，流量值至少增加 111 ，FlowvalueFlowvalueFlowvalue 从 000 开始增长到 MaxFlowMaxFlowMaxFlow\n​\t\t\t所以，Iterations≤Amount of MaxFlowIterations \\le Amount\\ of\\ MaxFlowIterations≤Amount of MaxFlow\n​\t\t\t在这个例子中，Iterations=Amount of MaxFlowIterations = Amount\\ of\\ MaxFlowIterations=Amount of MaxFlow\n​\t\t综上所述，循环的次数为：O(f∗)O(f^*)O(f∗) ，其中 f∗f^*f∗ 代表最大流的大小。\n\n\n寻找路径最坏情况\n​\t假设有向图 GGG 中有 EEE 条边，VVV 个结点，并且所有的结点都至少有一条相邻边，则 E≥V2E \\ge \\frac{V}{2}E≥2V​\n​\t所以如果使用 bfsbfsbfs 或 dfsdfsdfs ，在一个残存网络中找到一条路径的时间可以化为：O(V+E′)=O(E)O(V+E&#x27;)=O(E)O(V+E′)=O(E)​\n\n\n最坏时间复杂度\n​\t综上所述，最坏时间复杂度为：O(Ef∗)O(Ef^*)O(Ef∗)​\n\n\n*算法正确性证明\n​\t在算法介绍中，我们得知了设置回流边的意义，现在，我们来探究为什么能设置回流边，以及我们对回流边所进行的操作，是否会对算法正确性有所影响。\n对于回流操作正确性的证明\n​\t我们知道，残存网络（residual graphresidual\\ graphresidual graph）是一个特殊的流网络，其允许反向边，也即平行边的存在。\n​\t那么对于残存网络中，这&quot;特殊&quot;的流 f′f&#x27;f′ ，我们定义 $f \\uparrow f’ $ 为流 f′f&#x27;f′ 对 fff 的递增操作，定义为：\n(f↑f′)(u,v)={f(u,v)+f′(u,v)−f′(v,u)若(u,v)∈E0其他(f\\uparrow f&#x27;)(u,v)=\\begin{cases}f(u,v)+f&#x27;(u,v)-f&#x27;(v,u) &amp;若(u,v)\\in E\n \\\\0 &amp;其他\n\\end{cases}\n(f↑f′)(u,v)={f(u,v)+f′(u,v)−f′(v,u)0​若(u,v)∈E其他​\n​\t在残存网络中将流量发送到反向边上等同于在原来的网络中缩减流量，所以将边 (u,v)(u,v)(u,v) 的流量增加 f′(u,v)f&#x27;(u,v)f′(u,v) ，但减少了 f′(v,u)f&#x27;(v,u)f′(v,u) 。在残存网络中，这种将流量推流回去也称为抵消操作（cancellationcancellationcancellation​）。\n​\n​\t对于上述操作，其满足容量限制性质以及流量守恒性质，证明如下：\n​\t\t先证明容量守恒性质，\n​\t\t\t设边 (u,v)∈E(u,v)\\in E(u,v)∈E ，则 cf(v,u)=f(u,v)c_f(v,u)=f(u,v)cf​(v,u)=f(u,v) ，且 f′(v,u)≤cf(v,u)=f(u,v)f&#x27;(v,u) \\le c_f(v,u)=f(u,v)f′(v,u)≤cf​(v,u)=f(u,v) ，因此有，\n(f↑f′)(u,v)=f(u,v)+f′(u,v)−f′(v,u)≥f(u,v)+f′(u,v)−f(u,v)=f′(u,v)≥0(f↑f′)(u,v)=f(u,v)+f′(u,v)−f′(v,u)≤f(u,v)+cf(u,v)=f(u,v)+c(u,v)−f(u,v)=c(u,v)\\begin{align*}\n(f \\uparrow f&#x27;)(u,v)\n&amp;=f(u,v)+f&#x27;(u,v)-f&#x27;(v,u)\\\\\n&amp;\\ge f(u,v)+f&#x27;(u,v)-f(u,v)\\\\\n&amp;=f&#x27;(u,v)\\\\\n&amp;\\ge 0\\\\\n\\\\\n(f \\uparrow f&#x27;)(u,v)\n&amp;=f(u,v)+f&#x27;(u,v)-f&#x27;(v,u)\\\\\n&amp;\\le f(u,v)+c_f(u,v)\\\\\n&amp;=f(u,v)+c(u,v)-f(u,v)\\\\\n&amp;=c(u,v)\n\\end{align*}\n(f↑f′)(u,v)(f↑f′)(u,v)​=f(u,v)+f′(u,v)−f′(v,u)≥f(u,v)+f′(u,v)−f(u,v)=f′(u,v)≥0=f(u,v)+f′(u,v)−f′(v,u)≤f(u,v)+cf​(u,v)=f(u,v)+c(u,v)−f(u,v)=c(u,v)​\n​\t\t\t所以，0≤(f↑f′)(u,v)≤c(u,v)0 \\le (f \\uparrow f&#x27;)(u,v) \\le c(u,v)0≤(f↑f′)(u,v)≤c(u,v) ，即该操作满足容量限制性质。\n​\t\t再证明流量守恒性质，\n​\t\t\t因为 fff 和 f′f&#x27;f′ 都遵循流量守恒性质，所以对于所有的节点 u∈V−{s,t}u \\in V-\\{s,t\\}u∈V−{s,t} ，有，\n∑v∈V(f↑f′)(u,v)=∑v∈V(f(u,v)+f′(u,v)−f′(v,u))=∑v∈Vf(u,v)+∑v∈Vf′(u,v)−∑v∈Vf′(v,u)=∑v∈Vf(v,u)+∑v∈Vf′(v,u)−∑v∈Vf′(u,v)=∑v∈V(f(v,u)+f′(v,u)−f′(u,v))=∑v∈V(f↑f′)(v,u)\\begin{align*}\n\\sum_{v \\in V}(f \\uparrow f&#x27;)(u,v)\n&amp;=\\sum_{v \\in V}(f(u,v)+f&#x27;(u,v)-f&#x27;(v,u))\\\\\n&amp;=\\sum_{v \\in V}f(u,v)+\\sum_{v \\in V}f&#x27;(u,v)-\\sum_{v \\in V}f&#x27;(v,u)\\\\\n&amp;=\\sum_{v \\in V}f(v,u)+\\sum_{v \\in V}f&#x27;(v,u)-\\sum_{v \\in V}f&#x27;(u,v)\\\\\n&amp;=\\sum_{v \\in V}(f(v,u)+f&#x27;(v,u)-f&#x27;(u,v))\\\\\n&amp;=\\sum_{v \\in V}(f \\uparrow f&#x27;)(v,u)\\\\\n\\end{align*}\nv∈V∑​(f↑f′)(u,v)​=v∈V∑​(f(u,v)+f′(u,v)−f′(v,u))=v∈V∑​f(u,v)+v∈V∑​f′(u,v)−v∈V∑​f′(v,u)=v∈V∑​f(v,u)+v∈V∑​f′(v,u)−v∈V∑​f′(u,v)=v∈V∑​(f(v,u)+f′(v,u)−f′(u,v))=v∈V∑​(f↑f′)(v,u)​\n​\t\t\t其中第二行推导到第三行使用了 fff 和 f′f&#x27;f′ 的流量守恒性质。\n​\t\t\t所以，∑v∈V(f↑f′)(u,v)=∑v∈V(f↑f′)(v,u)\\sum_{v \\in V}(f \\uparrow f&#x27;)(u,v)=\\sum_{v \\in V}(f \\uparrow f&#x27;)(v,u)∑v∈V​(f↑f′)(u,v)=∑v∈V​(f↑f′)(v,u)​ ，即该操作满足流量守恒性质。\n​\t\t因此，对于 f↑f′f\\uparrow f&#x27;f↑f′​ 这个操作，其满足容量限制性质以及流量守恒性质。\n​\t所以回流操作并不会影响到流的基本性质，所以这个操作是正确的。\n对于函数 ∣f↑f′∣|f \\uparrow f&#x27;|∣f↑f′∣ 值大小的证明\n​\t那么如果根据流的值的定义，不妨猜想 ∣f↑f′∣=∣f∣+∣f′∣|f \\uparrow f&#x27;|=|f|+|f&#x27;|∣f↑f′∣=∣f∣+∣f′∣​ ，证明如下：\n​\t\t根据定义，有：\n∣f↑f′∣=∑v∈V(f↑f′)(s,v)−∑v∈V(f↑f′)(v,s)|f \\uparrow f&#x27;|=\\sum_{v\\in V}(f \\uparrow f&#x27;)(s,v)-\\sum_{v \\in V}(f \\uparrow f&#x27;)(v,s)\n∣f↑f′∣=v∈V∑​(f↑f′)(s,v)−v∈V∑​(f↑f′)(v,s)\n​\t\t因为对于每个节点 v∈Vv \\in Vv∈V ，可以有边 (s,v)(s,v)(s,v) 或 (v,s)(v,s)(v,s) ，但是二者不允许同时存在。\n​\t\t因此我们定义 V1={v:(s,v)∈E}V_1=\\{v:(s,v) \\in E\\}V1​={v:(s,v)∈E} 为有边从源节点到达的节点集合，V2={v:(v,s)∈E}V_2=\\{v:(v,s) \\in E\\}V2​={v:(v,s)∈E} 为有边通往源节点的节点集合。我们有 V1∪V2⊆VV_1 \\cup V_2 \\subseteq VV1​∪V2​⊆V 且 V1∩V2=∅V_1 \\cap V_2 = \\emptysetV1​∩V2​=∅ 。\n​\t\t所以上式可化为：\n∣f↑f′∣=∑v∈V(f↑f′)(s,v)−∑v∈V(f↑f′)(v,s)=∑v∈V1(f↑f′)(s,v)−∑v∈V2(f↑f′)(v,s)=∑v∈V1(f(s,v)+f′(s,v)−f′(v,s))−∑v∈V2(f(v,s)+f′(v,s)−f′(s,v))=∑v∈V1f(s,v)+∑v∈V1f′(s,v)−∑v∈V1f′(v,s)−∑v∈V2f(v,s)−∑v∈V2f′(v,s)+∑v∈V2f′(s,v)=∑v∈V1f(s,v)−∑v∈V2f(v,s)+∑v∈V1∪V2f′(s,v)−∑v∈V1∪V2f′(v,s)\\begin{align*}\n|f \\uparrow f&#x27;|\n&amp;=\\sum_{v\\in V}(f \\uparrow f&#x27;)(s,v)-\\sum_{v \\in V}(f \\uparrow f&#x27;)(v,s)\\\\\n&amp;=\\sum_{v\\in V_1}(f \\uparrow f&#x27;)(s,v)-\\sum_{v \\in V_2}(f \\uparrow f&#x27;)(v,s)\\\\\n&amp;=\\sum_{v\\in V_1}(f(s,v)+f&#x27;(s,v)-f&#x27;(v,s))-\\sum_{v\\in V_2}(f(v,s)+f&#x27;(v,s)-f&#x27;(s,v))\\\\\n&amp;=\\sum_{v\\in V_1}f(s,v)+\\sum_{v\\in V_1}f&#x27;(s,v)-\\sum_{v\\in V_1}f&#x27;(v,s)-\\sum_{v\\in V_2}f(v,s)-\\sum_{v\\in V_2}f&#x27;(v,s)+\\sum_{v\\in V_2}f&#x27;(s,v)\\\\\n&amp;=\\sum_{v\\in V_1}f(s,v)-\\sum_{v\\in V_2}f(v,s)+\\sum_{v \\in V_1 \\cup V_2}f&#x27;(s,v)-\\sum_{v \\in V_1 \\cup V_2}f&#x27;(v,s)\\\\\n\\end{align*}\n∣f↑f′∣​=v∈V∑​(f↑f′)(s,v)−v∈V∑​(f↑f′)(v,s)=v∈V1​∑​(f↑f′)(s,v)−v∈V2​∑​(f↑f′)(v,s)=v∈V1​∑​(f(s,v)+f′(s,v)−f′(v,s))−v∈V2​∑​(f(v,s)+f′(v,s)−f′(s,v))=v∈V1​∑​f(s,v)+v∈V1​∑​f′(s,v)−v∈V1​∑​f′(v,s)−v∈V2​∑​f(v,s)−v∈V2​∑​f′(v,s)+v∈V2​∑​f′(s,v)=v∈V1​∑​f(s,v)−v∈V2​∑​f(v,s)+v∈V1​∪V2​∑​f′(s,v)−v∈V1​∪V2​∑​f′(v,s)​\n​\t\t又因为当 v∈V1v \\in V_1v∈V1​ 时，对于在集合 V2V_2V2​ 的边，vvv 对应的流 fff 为 000 ，即每一个额外的项都为 000 ，反之亦然。\n​\t\t所以可以将 V1,V2,V1∪V2V_1,V_2,V_1\\cup V_2V1​,V2​,V1​∪V2​ 扩展到整个节点范围 VVV 中。\n​\t\t因此：\n∣f↑f′∣=∑v∈V1f(s,v)−∑v∈V2f(v,s)+∑v∈V1∪V2f′(s,v)−∑v∈V1∪V2f′(v,s)=∑v∈Vf(s,v)−∑v∈Vf(v,s)+∑v∈Vf′(s,v)−∑v∈Vf′(v,s)=∣f∣+∣f′∣\\begin{align*}\n|f \\uparrow f&#x27;|\n&amp;=\\sum_{v\\in V_1}f(s,v)-\\sum_{v\\in V_2}f(v,s)+\\sum_{v \\in V_1 \\cup V_2}f&#x27;(s,v)-\\sum_{v \\in V_1 \\cup V_2}f&#x27;(v,s)\\\\\n&amp;=\\sum_{v\\in V}f(s,v)-\\sum_{v\\in V}f(v,s)+\\sum_{v \\in V}f&#x27;(s,v)-\\sum_{v \\in V}f&#x27;(v,s)\\\\\n&amp;=|f|+|f&#x27;|\\\\\n\\end{align*}\n∣f↑f′∣​=v∈V1​∑​f(s,v)−v∈V2​∑​f(v,s)+v∈V1​∪V2​∑​f′(s,v)−v∈V1​∪V2​∑​f′(v,s)=v∈V∑​f(s,v)−v∈V∑​f(v,s)+v∈V∑​f′(s,v)−v∈V∑​f′(v,s)=∣f∣+∣f′∣​\n​\t\t即：∣f↑f′∣=∣f∣+∣f′∣|f \\uparrow f&#x27;|=|f|+|f&#x27;|∣f↑f′∣=∣f∣+∣f′∣\n对于合并操作正确性的证明\n​\t我们设 G=(V,E)G=(V,E)G=(V,E) 为一个流网络，设 fff 为 GGG 中的一个流，设 ppp 为残存网络 GfG_fGf​ 中的一条增广路径。假定将 fff 增加 fpf_pfp​ 的量，则函数 f↑fpf \\uparrow f_pf↑fp​ 是图 GGG 中的一个流，由上述证明可知其值为 ∣f↑fp∣=∣f∣+∣fp∣|f \\uparrow f_p|=|f|+|f_p|∣f↑fp​∣=∣f∣+∣fp​∣ 。\n​\t又因为合并的操作视为 ∣fp1↑fp2∣|f_{p_1} \\uparrow f_{p_2}|∣fp1​​↑fp2​​∣ ，所以合并后的值为 ∣fp1↑fp2∣=∣fp1∣+∣fp2∣|f_{p_1} \\uparrow f_{p_2}|=|f_{p_1}|+|f_{p_2}|∣fp1​​↑fp2​​∣=∣fp1​​∣+∣fp2​​∣ ，即合并操作是合理的。\n最大流最小切割定理的证明\n​\t上述证明解释了为什么我们可以进行回流、合并（MergeMergeMerge） 这些操作。那么接下来，我们来证明为什么最后得到的流一定是最大流。该证明也是对最大流最小切割定理的证明。\n前置证明准备\n​\t我们首先证明对于网络中的一个流以及任意切割 (S,T)(\\mathcal{S},\\mathcal{T})(S,T) ，其净流量 f(S,T)=∣f∣f(\\mathcal{S},\\mathcal{T})=|f|f(S,T)=∣f∣ ：\n​\t\t对于任意节点 u∈V−{s,t}u \\in V-\\{s,t\\}u∈V−{s,t} ，其流量守恒定律可改写成：\n∑v∈Vf(v,u)−∑v∈Vf(u,v)=0①\\sum_{v \\in V}f(v,u)-\\sum_{v \\in V} f(u,v)=0 \\quad\\quad①\nv∈V∑​f(v,u)−v∈V∑​f(u,v)=0①\n​\t\t根据 ∣f∣|f|∣f∣ 的定义：∣f∣=∑v∈Vf(s,v)−∑v∈Vf(v,s)|f|=\\sum_{v\\in V}f(s,v)-\\sum_{v \\in V}f(v,s)∣f∣=∑v∈V​f(s,v)−∑v∈V​f(v,s) ，我们得到：\n∣f∣=∑v∈Vf(s,v)−∑v∈Vf(v,s)②|f|=\\sum_{v\\in V}f(s,v)-\\sum_{v \\in V}f(v,s) \\quad\\quad②\n∣f∣=v∈V∑​f(s,v)−v∈V∑​f(v,s)②\n​\t\t将 ①①① 式针对所有节点 S−{s}\\mathcal{S}-\\{s\\}S−{s} 求和并加到 ②②② 中，得到：\n∣f∣=∑v∈Vf(s,v)−∑v∈Vf(v,s)+∑v∈S−{s}(∑v∈Vf(u,v)−∑v∈Vf(v,u))=∑v∈Vf(s,v)−∑v∈Vf(v,s)+∑v∈S−{s}∑v∈Vf(u,v)−∑v∈S−{s}∑v∈Vf(v,u)=∑v∈V(f(s,v)+∑v∈S−{s}f(u,v))−∑v∈V(f(v,s)+∑v∈S−{s}f(v,u))=∑v∈V∑u∈Sf(u,v)−∑v∈V∑u∈Sf(v,u)\\begin{align*}\n|f|\n&amp;=\\sum_{v\\in V}f(s,v)-\\sum_{v \\in V}f(v,s)+\\sum_{v \\in \\mathcal{S}-\\{s\\}}(\\sum_{v\\in V}f(u,v)-\\sum_{v \\in V}f(v,u))\\\\\n&amp;=\\sum_{v\\in V}f(s,v)-\\sum_{v \\in V}f(v,s)+\\sum_{v \\in \\mathcal{S}-\\{s\\}}\\sum_{v\\in V}f(u,v)-\\sum_{v \\in \\mathcal{S}-\\{s\\}}\\sum_{v \\in V}f(v,u)\\\\\n&amp;=\\sum_{v\\in V}(f(s,v)+\\sum_{v \\in \\mathcal{S}-\\{s\\}}f(u,v))-\\sum_{v \\in V}(f(v,s)+\\sum_{v \\in \\mathcal{S}-\\{s\\}}f(v,u))\\\\\n&amp;=\\sum_{v\\in V}\\sum_{u \\in \\mathcal{S}}f(u,v)-\\sum_{v\\in V}\\sum_{u \\in \\mathcal{S}}f(v,u)\n\\end{align*}\n∣f∣​=v∈V∑​f(s,v)−v∈V∑​f(v,s)+v∈S−{s}∑​(v∈V∑​f(u,v)−v∈V∑​f(v,u))=v∈V∑​f(s,v)−v∈V∑​f(v,s)+v∈S−{s}∑​v∈V∑​f(u,v)−v∈S−{s}∑​v∈V∑​f(v,u)=v∈V∑​(f(s,v)+v∈S−{s}∑​f(u,v))−v∈V∑​(f(v,s)+v∈S−{s}∑​f(v,u))=v∈V∑​u∈S∑​f(u,v)−v∈V∑​u∈S∑​f(v,u)​\n​\t\t又因为 V=S∪TV=\\mathcal{S}\\cup\\mathcal{T}V=S∪T 并且 S∩T=∅\\mathcal{S} \\cap \\mathcal{T}=\\emptysetS∩T=∅ ，所以可以将上式的 VVV 进行分解，即：\n∣f∣=∑v∈V∑u∈Sf(u,v)−∑v∈V∑u∈Sf(v,u)=∑v∈S∑u∈Sf(u,v)−∑v∈S∑u∈Sf(v,u)+∑v∈T∑u∈Sf(u,v)−∑v∈T∑u∈Sf(v,u)=∑v∈T∑u∈Sf(u,v)−∑v∈T∑u∈Sf(v,u)+(∑v∈S∑u∈Sf(u,v)−∑v∈S∑u∈Sf(v,u))\\begin{align*}\n|f|\n&amp;=\\sum_{v\\in V}\\sum_{u \\in \\mathcal{S}}f(u,v)-\\sum_{v\\in V}\\sum_{u \\in \\mathcal{S}}f(v,u)\\\\\n&amp;=\\sum_{v\\in \\mathcal{S}}\\sum_{u \\in \\mathcal{S}}f(u,v)-\\sum_{v\\in \\mathcal{S}}\\sum_{u \\in \\mathcal{S}}f(v,u)+\\sum_{v\\in \\mathcal{T}}\\sum_{u \\in \\mathcal{S}}f(u,v)-\\sum_{v\\in \\mathcal{T}}\\sum_{u \\in \\mathcal{S}}f(v,u)\\\\\n&amp;=\\sum_{v\\in \\mathcal{T}}\\sum_{u \\in \\mathcal{S}}f(u,v)-\\sum_{v\\in \\mathcal{T}}\\sum_{u \\in \\mathcal{S}}f(v,u)+(\\sum_{v\\in \\mathcal{S}}\\sum_{u \\in \\mathcal{S}}f(u,v)-\\sum_{v\\in \\mathcal{S}}\\sum_{u \\in \\mathcal{S}}f(v,u))\\\\\n\\end{align*}\n∣f∣​=v∈V∑​u∈S∑​f(u,v)−v∈V∑​u∈S∑​f(v,u)=v∈S∑​u∈S∑​f(u,v)−v∈S∑​u∈S∑​f(v,u)+v∈T∑​u∈S∑​f(u,v)−v∈T∑​u∈S∑​f(v,u)=v∈T∑​u∈S∑​f(u,v)−v∈T∑​u∈S∑​f(v,u)+(v∈S∑​u∈S∑​f(u,v)−v∈S∑​u∈S∑​f(v,u))​\n​\t\t不难看出，上式括号内的两个求和项是一样的，所以有：\n∣f∣=∑v∈T∑u∈Sf(u,v)−∑v∈T∑u∈Sf(v,u)=f(S,T)|f|=\\sum_{v\\in \\mathcal{T}}\\sum_{u \\in \\mathcal{S}}f(u,v)-\\sum_{v\\in \\mathcal{T}}\\sum_{u \\in \\mathcal{S}}f(v,u)=f(\\mathcal{S},\\mathcal{T})\n∣f∣=v∈T∑​u∈S∑​f(u,v)−v∈T∑​u∈S∑​f(v,u)=f(S,T)\n​\t综上，净流量 f(S,T)=∣f∣f(\\mathcal{S},\\mathcal{T})=|f|f(S,T)=∣f∣​ 。\n​\t由这个性质，我们可以得到一个引理：∣f∣≤c(S,T)|f| \\le c(\\mathcal{S},\\mathcal{T})∣f∣≤c(S,T) ，证明如下：\n∣f∣=f(S,T)=∑v∈T∑u∈Sf(u,v)−∑v∈T∑u∈Sf(v,u)≤∑v∈T∑u∈Sf(u,v)≤∑v∈T∑u∈Sc(u,v)=c(S,T)\\begin{align*}\n|f|\n&amp;=f(\\mathcal{S},\\mathcal{T})=\\sum_{v\\in \\mathcal{T}}\\sum_{u \\in \\mathcal{S}}f(u,v)-\\sum_{v\\in \\mathcal{T}}\\sum_{u \\in \\mathcal{S}}f(v,u)\\\\\n&amp;\\le \\sum_{v\\in \\mathcal{T}}\\sum_{u \\in \\mathcal{S}}f(u,v) \\le \\sum_{v\\in \\mathcal{T}}\\sum_{u \\in \\mathcal{S}}c(u,v) = c(\\mathcal{S},\\mathcal{T})\n\\end{align*}\n∣f∣​=f(S,T)=v∈T∑​u∈S∑​f(u,v)−v∈T∑​u∈S∑​f(v,u)≤v∈T∑​u∈S∑​f(u,v)≤v∈T∑​u∈S∑​c(u,v)=c(S,T)​\n​\t这个引理给出的一个直接结论就是：一个流网络中最大流的值不能超过该网络最小切割的容量。\n最大流最小切割定理\n​\t那么接下来，我们就来证明最大流最小切割原理。该原理表明一个最大流的值等于一个最小切割的容量。\n​\t最大流最小切割定理：\n​\t\t(1)(1)(1) fff 是 GGG 的一个最大流\n​\t\t(2)(2)(2) 残存网络 GfG_fGf​ 不包括任何增广路径\n​\t\t(3)(3)(3) ∣f∣=c(S,T)|f|=c(\\mathcal{S},\\mathcal{T})∣f∣=c(S,T) ，其中 (S,T)(\\mathcal{S},\\mathcal{T})(S,T) 是流网络 GGG 的某个切割。即 ∣f∣max=cmin(S,T)|f|_{max}=c_{min}(\\mathcal{S},\\mathcal{T})∣f∣max​=cmin​(S,T)\n​\t\t上述的三个条件是等效的，即三个条件互为充要条件。\n​\t以下是最大流最小切割定理的证明：\n​\t\t证 (1)→(2)(1) \\to (2)(1)→(2) ：\n​\t\t\t假设 fff 是 GGG 的一个最大流，但残存网络 GfG_fGf​ 同时存在一条增广路径 ppp 。如果我们对 fff 增加流量 fpf_pfp​ ，那么 ∣f↑fp∣=∣f∣+∣fp∣&gt;∣f∣|f \\uparrow f_p|=|f|+|f_p| &gt; |f|∣f↑fp​∣=∣f∣+∣fp​∣&gt;∣f∣ ，与 fff 是最大流矛盾。\n​\t\t证 (2)→(3)(2) \\to (3)(2)→(3) ：\n​\t\t\t对于节点 u∈Su \\in \\mathcal{S}u∈S 和 v∈Tv \\in \\mathcal{T}v∈T ，如果边 (u,v)∈E(u,v) \\in E(u,v)∈E ，则必有 f(u,v)=c(u,v)f(u,v)=c(u,v)f(u,v)=c(u,v) ，否则 (u,v)∈Ef(u,v) \\in E_f(u,v)∈Ef​ ，上述操作会将 vvv 置于集合 S\\mathcal{S}S 中。如果边 (v,u)∈E(v,u) \\in E(v,u)∈E ，则必有 f(v,u)=0f(v,u)=0f(v,u)=0 ，否则 cf(u,v)=f(v,u)c_f(u,v)=f(v,u)cf​(u,v)=f(v,u) 将为正值，边 (u,v)(u,v)(u,v) 将属于 EfE_fEf​ ，上述操作会将节点 vvv 置于集合 S\\mathcal{S}S 中。当边 (u,v)(u,v)(u,v) 和 (v,u)(v,u)(v,u) 都不在 EEE 中，则 f(u,v)=f(v,u)=0f(u,v)=f(v,u)=0f(u,v)=f(v,u)=0 。因此：\nf(S,T)=∑v∈T∑u∈Sf(u,v)−∑v∈T∑u∈Sf(v,u)=∑v∈T∑u∈Sc(u,v)−∑v∈T∑u∈S0=c(S,T)f(\\mathcal{S},\\mathcal{T})=\\sum_{v\\in \\mathcal{T}}\\sum_{u \\in \\mathcal{S}}f(u,v)-\\sum_{v\\in \\mathcal{T}}\\sum_{u \\in \\mathcal{S}}f(v,u)=\\sum_{v\\in \\mathcal{T}}\\sum_{u \\in \\mathcal{S}}c(u,v)-\\sum_{v\\in \\mathcal{T}}\\sum_{u \\in \\mathcal{S}}0=c(\\mathcal{S},\\mathcal{T})\nf(S,T)=v∈T∑​u∈S∑​f(u,v)−v∈T∑​u∈S∑​f(v,u)=v∈T∑​u∈S∑​c(u,v)−v∈T∑​u∈S∑​0=c(S,T)\n​\t\t证 (3)→(1)(3) \\to (1)(3)→(1) ：\n​\t\t\t由 ∣f∣≤c(S,T)|f| \\le c(\\mathcal{S},\\mathcal{T})∣f∣≤c(S,T) 即可说明 fff​ 是一个最大流,。\n​\t综上所述，Ford−FulkersonFord-FulkersonFord−Fulkerson 算法是正确的。\nEdmonds−KarpEdmonds-KarpEdmonds−Karp​ 算法\n​\t简单来说，Edmonds−KarpEdmonds-KarpEdmonds−Karp 算法是 Ford−FulkersonFord-FulkersonFord−Fulkerson 算法的一种特殊情况，全部流程跟 Ford−FulkersonFord-FulkersonFord−Fulkerson 算法基本一致。\n​\t唯一的区别是 Edmonds−KarpEdmonds-KarpEdmonds−Karp 算法在寻找增广路径中，使用寻找无权图最短路径的算法，在残存网络中将所有边的权值视为 111​ ，由此来寻找从源节点到汇点的最短路径。\n算法实现步骤\n参照 Ford−FulkersonFord-FulkersonFord−Fulkerson 算法的步骤，对于 Edmonds−KarpEdmonds-KarpEdmonds−Karp 算法的实现步骤只需修改一句话：\n\n创建一个残存图 (residual graph)(residual\\ graph)(residual graph) ，初始化残存容量\nwhilewhilewhile  可以找到增广路径 :\n\n==找到最短的增广路径==\n找出增广路径中的瓶颈容量 xxx\n更新这条增广路径上每一条边，cf=cf−xc_f=c_f-xcf​=cf​−x​​ ，并删除饱和边\n添加该增广路径的回流边\n\n\n\n时间复杂度分析\n对于寻找最短的增广路径，我们可以使用类 BFSBFSBFS 的算法。\n因此寻找最短路径的时间复杂度为：O(V+E)=O(E)O(V+E)=O(E)O(V+E)=O(E)​\n然后循环最多实现 O(VE)O(VE)O(VE) 次~~（这里就不证明了吧 详细证明看算法导论）~~\n因此时间复杂度为：O(VE2)O(VE^2)O(VE2)\n*DinicDinicDinic 算法\n​\t下面我们来介绍一个时间复杂度更低，达到了 O(V2E)O(V^2E)O(V2E) 的算法—— DinicDinicDinic 算法。\nLevel GraphLevel\\ GraphLevel Graph ​\n​\tDinicDinicDinic 算法需要用到一个新东西：Level GraphLevel\\ GraphLevel Graph 。Level GraphLevel\\ GraphLevel Graph 基于 BFSBFSBFS 实现，是一个有权无向图。其随着层数记录每一层的节点，最终到达汇点 ttt 。\n\ne.g.\n\n\n    \n    \n\n算法实现步骤\n该算法的实现与 Ford−FulkersonFord-FulkersonFord−Fulkerson 算法类似，但在操作中，我们主要以 Level GraphLevel\\ GraphLevel Graph 为主要操作对象：\n\n创建一个残存图 (residual graph)(residual\\ graph)(residual graph)​ ，初始化残存容量\nwhilewhilewhile 可以找到阻塞流：\n\n用残存图创建 level graphlevel\\ graphlevel graph\n在 level graphlevel\\ graphlevel graph 中找到阻塞流\n更新残存图，并添加反向边\n\n\n\n图示\n\n初始化\n\n\n\n\n第一次循环\n\n根据残存图创建 level graphlevel\\ graphlevel graph\n\n \n     \n     \n \n\n找到阻塞流，更新残存图\n\n \n     \n     \n     \n \n\n\n第二次循环\n\n根据上一次的结果，创建新的 level graphlevel\\ graphlevel graph\n\n \n     \n     \n \n\n在 level graphlevel\\ graphlevel graph​ 中找到阻塞流，更新残存图\n\n \n     \n     \n     \n \n\n\n第三轮循环\n\n根据上一次的结果，创建新的 level graphlevel\\ graphlevel graph\n\n \n     \n     \n \n\n因为没有边能到汇点 ttt ，循环终止，最终结果如下：\n\n \n     \n     \n \n\n\n时间复杂度\n时间复杂度为：O(V2E)O(V^2E)O(V2E)\n\n若图为一个链表时，有 nnn 层，循环最多为 O(V−1)O(V-1)O(V−1) 轮\n每一轮的时间为 O(VE)O(VE)O(VE)\n\n二部图问题\n\n嘛~有时间在做吧awa 反正是最大流之后的一章(4th edition)\n\n参考文献\n\nWang, Shusen. (n.d.). AdvancedAlgorithms. GitHub. Retrieved April 5, 2022, from https://github.com/wangshusen/AdvancedAlgorithms\nCormen, T. H., Leiserson, C. E., Rivest, R. L., &amp; Stein, C. (2009). 算法导论 (第三版). 机械工业出版社.\nCormen, T. H., Leiserson, C. E., Rivest, R. L., &amp; Stein, C. (2022). Introduction to Algorithms (4th ed.). MIT Press.\n\n","categories":["DataStru&Algo"],"tags":["Data Structure","Algorithm","SCNU Turing Discussion","Maximum Flow","Graph"]},{"title":"REPLUG: Retrieval-Augmented Black-Box Language Models","url":"/paper/REPLUG-Retrieval-Augmented-Black-Box-Language-Models/","content":"问题 (Problem)\n大型语言模型（LLMs）如GPT-3虽然强大，但存在两个核心问题：\n\n知识局限性：模型参数中存储的知识是静态的，无法实时更新，且对于长尾知识（rare knowledge）的覆盖不全，容易产生事实性错误或“幻觉”。\n黑盒特性：当前最先进的LLMs（通常 &gt;100B 参数）往往通过API提供服务，用户无法访问模型的内部参数、梯度或进行微调。这使得传统的、需要“白盒”访问权限的检索增强方法（如RETRO、Atlas）无法适用。\n\n因此，本文的核心问题是：如何在只能“黑盒”访问（即只能输入文本、获取输出）的前提下，通过外部知识库对大型语言模型进行有效的检索增强，以提升其性能并减少幻觉？\n方法 (Method)\n作者提出了 REPLUG (Retrieve and Plug) 框架，它将语言模型视为一个不可更改的黑盒，并将一个可调优的检索器作为插件来增强它。\nREPLUG 推理过程\nREPLUG的推理过程分为两步：文档检索 和 输入重构与集成。\n\nREPLUG 推理流程图（论文Figure 2）\n\n\n文档检索 (Document Retrieval)\n\n给定一个输入上下文 xxx，使用一个**密集检索器（Dense Retriever）**从一个大规模的外部语料库 D\\mathcal{D}D 中检索出与 xxx 最相关的 kkk 个文档。\n该检索器采用双编码器架构，将输入 xxx 和文档 ddd 分别编码为向量 E(x)E(x)E(x) 和 E(d)E(d)E(d)。\n相关性得分通过余弦相似度计算：s(d,x)=cos⁡(E(d),E(x))s(d,x) = \\cos(E(d), E(x))\ns(d,x)=cos(E(d),E(x))\n\n为了效率，所有文档的向量会预先计算并用FAISS等工具建立索引，以实现快速检索。\n\n\n\n输入重构与集成 (Input Reformulation &amp; Ensemble)\n\n直接将所有 kkk 个文档拼接到输入 xxx 前面会受到模型上下文窗口长度的限制。\nREPLUG 采用了一种巧妙的集成策略：\n\n对于检索到的top-k个文档 d1,d2,...,dk{d_1, d_2, ..., d_k}d1​,d2​,...,dk​，分别与原始输入 xxx 进行拼接，构造出 kkk 个独立的输入：d1∘x,d2∘x,...,dk∘x{d_1 \\circ x, d_2 \\circ x, ..., d_k \\circ x}d1​∘x,d2​∘x,...,dk​∘x。\n将这 kkk 个输入并行地送入黑盒语言模型中，得到 kkk 组关于下一个词的输出概率分布。\n最终的输出概率是对这 kkk 组概率分布进行加权平均的结果。权重由检索器的相关性得分决定。\n\n\n下一个词 yyy 的最终概率计算公式为：\n\np(y∣x,D′)=∑d∈D′p(y∣d∘x)⋅λ(d,x)p(y|x, \\mathcal{D}&#x27;) = \\sum_{d \\in \\mathcal{D}&#x27;} p(y|d \\circ x) \\cdot \\lambda(d, x)\np(y∣x,D′)=d∈D′∑​p(y∣d∘x)⋅λ(d,x)\n其中，D′\\mathcal{D}&#x27;D′ 是 top-k 文档集合，d∘xd \\circ xd∘x 表示文档和输入的拼接。权重 λ(d,x)\\lambda(d,x)λ(d,x) 是检索得分经过Softmax归一化后的结果：\nλ(d,x)=es(d,x)∑d′∈D′es(d′,x)\\lambda(d,x) = \\frac{e^{s(d,x)}}{\\sum_{d&#x27; \\in \\mathcal{D}&#x27;} e^{s(d&#x27;,x)}}\nλ(d,x)=∑d′∈D′​es(d′,x)es(d,x)​\n\n这种方法的优势是，虽然需要调用模型 kkk 次，但它绕过了上下文长度限制，可以集成更多的文档信息，并且计算可以完全并行，只是增加了API调用成本。\n\n\n\nREPLUG LSR: 训练可适变的检索器\n为了让检索器更能“投其所好”，即检索到能最大化帮助特定黑盒LLM的文档，作者提出了 REPLUG LSR (LM-Supervised Retrieval) 训练方案。其核心思想是用黑盒LLM本身作为监督信号来指导检索器的训练。\n\nREPLUG LSR 训练流程图（论文Figure 3）\n训练过程包含四个步骤：\n\n\n计算检索似然度 (Computing Retriever Likelihood, PRP_RPR​)\n\n对于一个输入 xxx，用当前检索器检索 top-k 个文档 D′\\mathcal{D}&#x27;D′。\n基于检索器的相似度得分 s(d,x)s(d,x)s(d,x)，计算出一个在 D′\\mathcal{D}&#x27;D′ 上的概率分布 PR(d∣x)P_R(d|x)PR​(d∣x)，代表了检索器认为每个文档的重要程度。\n公式为：\n\nPR(d∣x)=es(d,x)/τ∑d′∈D′es(d′,x)/τP_R(d|x) = \\frac{e^{s(d,x)/\\tau}}{\\sum_{d&#x27; \\in \\mathcal{D}&#x27;} e^{s(d&#x27;,x)/\\tau}}\nPR​(d∣x)=∑d′∈D′​es(d′,x)/τes(d,x)/τ​\n其中 τ\\tauτ 是温度超参数。\n\n\n计算语言模型似然度 (Computing LM Likelihood, QLMQ_{LM}QLM​)\n\n这是获取监督信号的关键一步。\n对于 D′\\mathcal{D}&#x27;D′ 中的每一个文档 ddd，将其与输入 xxx 拼接后，喂给黑盒LLM，计算模型对于真实答案（ground truth） yyy 的预测概率 PLM(y∣d,x)P_{LM}(y|d,x)PLM​(y∣d,x)。\n这个概率值越高，说明文档 ddd 对LLM做出正确预测的帮助越大。\n将这些概率值也转换成一个目标分布 QLM(d∣x,y)Q_{LM}(d|x,y)QLM​(d∣x,y)，代表了LLM认为每个文档的重要程度。\n公式为：\n\nQLM(d∣x,y)=ePLM(y∣d,x)/β∑d′∈D′ePLM(y∣d′,x)/βQ_{LM}(d|x, y) = \\frac{e^{P_{LM}(y|d,x)/\\beta}}{\\sum_{d&#x27; \\in \\mathcal{D}&#x27;} e^{P_{LM}(y|d&#x27;,x)/\\beta}}\nQLM​(d∣x,y)=∑d′∈D′​ePLM​(y∣d′,x)/βePLM​(y∣d,x)/β​\n其中 β\\betaβ 是另一个温度超参数。\n\n\n优化损失函数 (Loss Function)\n\n通过最小化检索器分布 PRP_RPR​ 和语言模型目标分布 QLMQ_{LM}QLM​ 之间的 KL散度 (KL Divergence) 来更新检索器的参数。\n这会迫使检索器学习去预测那些能够最大化LLM性能的文档。\n损失函数为：L=1∣B∣∑x∈BKL(PR(d∣x) ∣∣ QLM(d∣x,y))\\mathcal{L} = \\frac{1}{|\\mathcal{B}|}\\sum_{x \\in \\mathcal{B}}KL(P_R(d|x) \\ || \\ Q_{LM}(d|x,y))\nL=∣B∣1​x∈B∑​KL(PR​(d∣x) ∣∣ QLM​(d∣x,y))\n其中 B\\mathcal{B}B 是一组输入上下文\n在训练过程中，只有检索器的参数被更新，LLM始终保持冻结和黑盒。\n\n\n\n异步更新数据索引 (Asynchronous Update of Datastore Index)\n\n由于检索器的编码器在训练中不断变化，预先计算的文档向量会变得“过时”。\n因此，训练过程中需要周期性地（例如每T个训练步）使用更新后的编码器重新计算整个文档库的向量，并重建FAISS索引。这是一个计算量巨大的步骤。\n\n\n\nBaseline\n论文在多个任务上与不同的基线模型进行了对比：\n\n\n语言建模 (Language Modeling)：\n\n基线: 原始的、未增强的GPT-2和GPT-3系列模型。\n对比模型: REPLUG增强版和REPLUG LSR增强版。\n\n\n\nMMLU (多任务语言理解)：\n\n基线LLMs: Codex (175B), PaLM (540B), Flan-PaLM (540B)。\n白盒检索模型: Atlas (11B)。\n\n\n\n开放域问答 (Open-Domain QA)：\n\n基线LLMs (few-shot): Chinchilla, PaLM, Codex。\n白盒检索模型 (fine-tuned): RETRO, R2-D2, Atlas。\n\n\n\n数据集 (Datasets)\n\n\nREPLUG LSR 训练:\n\n训练查询: 从 The Pile 训练集中采样的80万个序列。\n外部文档库: 从 The Pile 训练集中采样的3600万个文档（与查询无重叠）。\n\n\n\n评估:\n\n语言建模: The Pile 基准测试集。\n多任务语言理解: MMLU 数据集，采用5-shot设置。\n开放域问答: Natural Questions (NQ) 和 TriviaQA (TQA) 数据集，采用few-shot设置。\n泛化性分析: Wikitext-103 数据集，用于测试REPLUG在GPT-2, BLOOM, OPT等不同模型家族上的效果。\n\n\n\n可复现性 (Reproducibility)\n\n代码: 论文中未提供公开的代码库链接。\n算力: 复现该方法需要巨大的计算资源。\n\nAPI成本: REPLUG LSR的训练需要大量调用一个中等大小的LLM（如GPT-3 Curie）作为监督，推理过程则需要将API调用成本乘以 kkk（集成文档的数量）。\nGPU资源: 训练一个强大的密集检索器本身就需要大量GPU算力。\n索引更新瓶颈: REPLUG LSR训练中最耗费计算的步骤是异步索引更新。周期性地为数千万文档重新编码并建立索引，需要非常强大的计算集群，是个人或小团队难以承受的。\n\n\n\n可改进的几个点 (Potential Improvements)\n\n推理效率: 推理成本是原始模型的 kkk 倍，这是一个主要缺点。未来的研究可以探索更高效的集成方法，例如，如何在单次前向传播中融合多个文档的信息，或者用更小的模型来预处理和压缩检索到的信息。\n训练效率: 异步索引更新是训练的巨大瓶颈。可以研究更高效的索引更新策略，如增量更新，或者探索不需要静态索引的检索方法。\n集成权重: 当前的集成权重 λ(d,x)\\lambda(d,x)λ(d,x) 仅依赖于检索器的初始分数。可以设计更动态的权重方案，比如结合LLM对每个文档的“置信度”进行二次加权。\n可解释性: 论文作者也提到，模型何时依赖参数知识、何时依赖检索知识是不透明的。增强模型的可解释性，让用户知道答案的来源，是未来一个重要的研究方向。\n检索粒度: REPLUG检索的是整个文档。可以探索更细粒度的检索，如段落、句子甚至短语，这可能为模型提供更精确、噪声更少的信息。\n\n可以被引用的一些结论 (Citable Conclusions)\n\n黑盒检索增强是可行且有效的：本文首次证明，在不访问模型内部参数的情况下，通过外部检索能显著提升超大规模（&gt;100B）黑盒语言模型的性能。\n语言模型自身是最好的监督者：REPLUG LSR的核心洞见是，可以用LLM本身来指导检索器的训练，让检索器学会去寻找对该LLM最有帮助的知识，实现了检索器和语言模型的“适配”。\n在多样的模型和任务上表现稳定：REPLUG框架具有普适性，能够稳定地提升包括GPT系列、BLOOM、OPT在内的多种模型家族在语言建模、知识问答和复杂推理等多个任务上的表现。\n显著提升少样本学习能力：在开放域问答的少样本（few-shot）设定下，REPLUG LSR增强的Codex模型甚至超越了使用更多样本进行微调的先进白盒模型（如Atlas），创造了新的SOTA记录。\n检索对于长尾知识至关重要：定性分析表明，REPLUG对于包含**稀有实体（rare entities）**的文本尤其有效，这证明了检索在弥补模型参数化知识短板方面的重要作用。\n用更小的模型达到更优的效果：通过REPLUG增强，一个175B参数的模型（Codex）在MMLU任务上的表现可以媲美一个三倍大于它、且经过指令微调的模型（540B的Flan-PaLM），展示了检索在提升模型效率上的巨大潜力。\n\n","categories":["paper"],"tags":["paper","Retrieval","LLM","RAG"]},{"title":"Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models","url":"/paper/Self-Refine-Instruction-Tuning-for-Aligning-Reasoning-in-Language-Models/","content":"问题 (Problem)\n这篇论文旨在解决将大型语言模型（LLM）强大的推理能力，特别是链式思考（Chain-of-Thought, CoT）能力，迁移到小型语言模型（SLM）上时遇到的核心挑战。\n当前主流的方法是监督微调（Supervised Fine-Tuning, SFT），即使用 LLM 生成的推理过程作为示例来训练 SLM。但这种方法存在明显缺陷：\n\n泛化能力不足：SFT 仅仅是让 SLM 模仿 LLM 提供的特定推理路径。然而，同一个问题可能存在多种有效的推理路径，导致模型学到的能力不够通用，泛化性差。\n能力差距：SLM 本身不具备像 LLM 那样强大的“涌现”推理能力，简单的模仿训练无法完全弥补这一根本差距。\n对齐不完全：训练后的 SLM 在性能上与“教师”LLM 之间仍然存在显著差距。\n\n因此，核心问题是：如何设计一种更有效的方法，不仅能将 LLM 的推理能力迁移给 SLM，还能让 SLM 自我提升和完善这种能力，从而真正实现与 LLM 的推理对齐？\n\n方法 (Method)\n作者提出了一种名为 自优化指令微调（Self-Refine Instruction-Tuning） 的两阶段方法来解决上述问题。\n整体框架\n该方法包含两个核心阶段：指令微调阶段 和 自优化阶段。其整体流程如下图所示：\n\n\n左侧（教师与示例）：首先，由多个“教师”LLM（如 GPT-3.5, Llama2-70b）针对特定任务生成带有详细推理步骤（CoT）的解答，这些解答构成了“示例（Demonstrations）”数据集。\n中间（第一阶段：指令微调）：然后，用这些高质量的示例对“学生”SLM（如 Llama2-7b）进行指令微调，这是知识的初次迁移。\n右侧（第二阶段：自优化）：经过初次微调的 SLM 进入一个“自优化（self-refine）”循环。在这个阶段，模型自己生成推理路径，并通过一种名为“直接偏好优化”的技术来不断完善自身的推理策略。\n\n阶段一：指令微调 (Instruction-tuning)\n这个阶段的目标是让 SLM 初步掌握遵循指令进行分步推理的能力。它是一种面向任务、特殊化的 SFT。\n\n训练数据由元组 (i,q,ai)(i, q, a_i)(i,q,ai​) 构成，其中 iii 是任务指令， qqq 是输入问题， aia_iai​ 是教师 LLM 生成的 CoT 答案。\n其优化的损失函数为指令微调损失（Instruction-tuning Loss），旨在让模型生成的结果最大限度遵循指令的引导。其数学公式为：Linst(θ)=−E(i,q,a)∼D∑t=0l−1log⁡πθ(wt+1∣st,i)\\mathcal{L}_{inst}(\\theta) = -E_{(i, q, a) \\sim D} \\sum_{t=0}^{l-1} \\log \\pi_{\\theta}(w_{t+1} | s_t, i)\nLinst​(θ)=−E(i,q,a)∼D​t=0∑l−1​logπθ​(wt+1​∣st​,i)\n\n\n其中：\n\nπθ\\pi_{\\theta}πθ​ 是学生模型的策略（即模型本身）。\nDDD 是教师 LLM 生成的示例数据集。\na=[w1,w2,...,wl]a = [w_1, w_2, ..., w_l]a=[w1​,w2​,...,wl​] 是教师生成的答案序列。\nsts_tst​ 是在生成第 ttt 个词元（token）时的状态，包含了指令、问题和已生成的部分答案。\n这个公式的核心是最大化模型在给定指令和上下文时，生成正确答案序列的对数概率。\n\n阶段二：自优化 (Self-refinement)\n这是该方法最具创新性的部分。在第一阶段的基础上，此阶段让 SLM 通过 直接偏好优化（Direct Preference Optimization, DPO） 算法进行自我改进，而无需额外的人工标注或独立的奖励模型。\n\n核心思想：让模型学会“偏爱”自己生成的、带有正确 CoT 推理的答案，而不是那些没有推理步骤或者推理错误的答案。\n具体步骤：\n\n数据生成：对于数据集中的每个问题 xxx，让第一阶段训练好的模型 πinst\\pi_{inst}πinst​ 生成两种答案：\n\n偏好的答案 (ywy_wyw​)：通过 CoT 提示（如在问题后加上 “Let’s think step by step”）引导模型生成的、推理步骤清晰且最终结果正确的答案。\n不偏好的答案 (yly_lyl​)：不使用 CoT 提示，模型直接生成的答案。\n\n\nDPO 优化：使用这些自生成的偏好对 (yw,yl)(y_w, y_l)(yw​,yl​) 来优化模型。DPO 的损失函数如下：\n\nLDPO(πθ;π_inst)=−E(x,yw,yl)∼D[log⁡σ(βlog⁡πθ(yw∣x)πinst(yw∣x)−βlog⁡πθ(yl∣x)πinst(yl∣x))]\\mathcal{L}_{DPO}(\\pi_{\\theta}; \\pi\\_{inst}) = -\\mathbb{E}_{(x, y_w, y_l) \\sim D} \\left[ \\log \\sigma \\left( \\beta \\log \\frac{\\pi_{\\theta}(y_w|x)}{\\pi_{inst}(y_w|x)} - \\beta \\log \\frac{\\pi_{\\theta}(y_l|x)}{\\pi_{inst}(y_l|x)} \\right) \\right]\nLDPO​(πθ​;π_inst)=−E(x,yw​,yl​)∼D​[logσ(βlogπinst​(yw​∣x)πθ​(yw​∣x)​−βlogπinst​(yl​∣x)πθ​(yl​∣x)​)]\n其中：\n\nπθ\\pi_{\\theta}πθ​ 是当前正在优化的模型。\nπinst\\pi_{inst}πinst​ 是优化前的参考模型（即第一阶段结束时的模型）。\nσ\\sigmaσ 是 Sigmoid 函数。\nβ\\betaβ 是一个超参数，用于控制偏好的强度。\n这个损失函数的目标是：提高模型生成偏好答案 ywy_wyw​ 的概率，同时降低生成不偏好答案 yly_lyl​ 的概率，从而将模型的行为模式“校准”到高质量的 CoT 推理风格上。\n\n\n\n\nBaseline\n论文的实验设计非常扎实，通过与多个基线的对比，清晰地展示了其方法的有效性。主要基线包括：\n\nZero-shot Prompt：未经任何微调的原始 SLM 直接回答问题。\nZero-shot CoT Prompt：原始 SLM 使用 “Let’s think step by step” 提示进行回答。\nInstruction-tuning (IT) only：只经过第一阶段指令微调，但未经过第二阶段自优化的 SLM。\nTeacher Performance：生成训练数据的“教师”LLM 的性能。这被视为学生模型性能的“上限”参考。\n\n从下图（以 OBQA 和 GSM8K 数据集为例）可以看出，Self IT (深蓝色和红色条) 的性能显著优于仅 IT 的模型，并且在很多情况下接近甚至超过了教师 LLM 的水平（图中的虚线）。\n\n数据集与模型 (Datasets &amp; Models)\n实验覆盖了多种类型的推理任务，以验证方法的普适性。\n\n数据集 (Datasets)：\n\n常识推理：CommonsenseQA (CSQA), OpenBookQA (OBQA)\n物理与社交互动推理：PIQA, SIQA\n数学推理：MultiArith, GSM8k\n综合能力评估：MATH, MMLU\n\n\n模型 (Models)：\n\n教师 LLM：Llama2-70b, Mixtral, GPT-3.5\n学生 SLM：Llama2-7b, Llama2-13b, Mistral-7b\n\n\n\n\n可复现性 (Reproducibility)\n\n代码：论文明确表示代码已开源，并提到使用了 Hugging Face 的 DPO_trainer 库，这为复现提供了便利。\n算力：实验在一台配备了 4 块 NVIDIA RTX A6000 GPU（每块 48GB VRAM）的工作站上进行。作者还提到了使用 QLoRA 技术在指令微调阶段节省显存，降低了硬件门槛。\n超参数：论文提供了两阶段训练过程中的学习率、优化器、批大小等关键超参数，细节清晰，有助于他人复现结果。\n\n\n可改进的几个点 (Potential Improvements)\n尽管该方法效果显著，但论文和方法本身也揭示了一些未来可探索的方向：\n\n多语言扩展：目前的研究完全基于英语。将该方法扩展到中文、西班牙语等其他语言，验证其跨语言的有效性，是一个重要的研究方向。\n优化推理步骤的内在逻辑：当前的 DPO 优化更多是基于最终答案的正确性来构建偏好对，并未深入到对推理步骤本身的逻辑严密性、简洁性进行建模。未来可以探索更细粒度的奖励机制，例如奖励更高效或更具逻辑性的推理路径。\n减少对教师模型的依赖：方法的第一步仍然依赖于强大的教师 LLM 生成高质量数据。如何让 SLM 在更少、甚至没有教师示例的情况下实现自我推理能力的引导和提升，是一个更具挑战性的问题。\n推理过程的可靠性与可信度：论文也提到了一个通用风险——模型可能生成一个“看似合理”但实际错误的推理过程。如何提升 CoT 推理的真实性和可靠性，避免模型“一本正经地胡说八道”，是整个领域需要解决的难题。\n探索不同优化算法：除了 DPO，还可以尝试其他强化学习或偏好对齐算法（如 PPO），看是否能在自优化阶段取得更好或更稳定的效果。\n\n\n可以被引用的一些结论 (Key Takeaways)\n这篇论文提供了多个坚实且有价值的结论，可以直接被后续研究引用：\n\n两阶段方法优于单一微调：结合“指令微调”和“基于DPO的自优化”的两阶段方法，在提升小模型推理能力方面，显著优于传统的、单一的监督微调方法。\n自优化能有效缩小大小模型差距：DPO 自优化阶段是弥合 SLM 和 LLM 推理能力差距的关键，能够使 SLM 在特定任务上的性能达到甚至超过其教师 LLM。\n自生成偏好数据是可行的：该研究证明了使用模型自生成的 CoT 答案作为偏好数据来驱动 DPO 是一种高效且低成本的自提升策略，摆脱了对昂贵的人工标注的依赖。\n跨领域泛化能力强：通过该方法学到的推理能力具有良好的泛化性，在一个领域（如通用问答）训练的模型，在另一个领域（如数学推理）也表现出性能提升。\n“异源”教师可能效果更佳：使用来自不同模型家族的“异源”教师（out-family，如用 GPT-3.5 训练 Llama 模型）进行教学，有时能培养出比“同源”教师（in-family）更具鲁棒性的学生模型。\n方法在低资源场景下依然有效：即使训练示例数量大幅减少（例如减少到25%），该方法依然能保持稳定的性能优势，展示了其数据效率。\n\n","categories":["paper"],"tags":["paper","LLM","refine","RL"]},{"title":"Precise Zero-Shot Dense Retrieval without Relevance Labels","url":"/paper/Precise-Zero-Shot-Dense-Retrieval-without-Relevance-Labels/","content":"问题 (Problem)\n传统的密集检索（Dense Retrieval）系统严重依赖大规模的、人工标注的“查询-文档”相关性数据进行训练。然而，在许多新的或特定的领域中，获取这样的标注数据成本高昂、耗时巨大，甚至是不可能的。因此，如何在没有任何相关性标签（即“零样本”或“完全无监督”）的情况下，构建一个开箱即用且性能强大的密集检索系统，是一个核心的挑战。\n\n方法 (Methodology)\n为了解决上述问题，论文提出了 HyDE (Hypothetical Document Embeddings，假设性文档嵌入) 方法。其核心思想是绕过对“查询-文档”相关性的直接建模，而是将其分解为两个独立的、更容易处理的任务。\n工作流程图解 (Figure 1)\n\n该图清晰地展示了 HyDE 的两步流程：\n\n生成 (Generation)：用户的查询 (query) 首先与一个任务指令 (instruction) 相结合，然后被送入一个遵循指令的大语言模型（如 GPT）。该模型会生成一个“假设性”的文档，这个文档虽然内容可能是虚构的（包含幻觉），但它在语义和结构上捕捉了真实相关文档的特征。\n编码与检索 (Encoding &amp; Retrieval)：\n\n生成的假设性文档被输入到一个无监督的文本编码器 (如 Contriever)中，该编码器将其压缩成一个向量。这个向量被视为原始查询的“代理”。\n利用这个向量，系统在预先编码好的真实文档库中进行相似度搜索，最终召回最相似的真实文档 (real document)。\n\n\n\n核心数学公式\nHyDE 的方法规避了传统密集检索中直接计算 sim(q, d) 的难题。\n\n\n文档编码器 (Document Encoder)：\nHyDE 使用一个通过无监督对比学习预训练的编码器 f=encd=encconf = enc_{d} = enc_{con}f=encd​=enccon​ 来处理所有真实文档和假设性文档。\nvd=f(d)v_d = f(d)\nvd​=f(d)\n这个编码器 fff 的作用是将任何文档（真实的或假设的）映射到一个向量空间中。\n\n\n查询向量的构建 (Query Vector Construction)：\n查询向量 vqv_qvq​ 不是直接由查询 qqq 编码而来，而是通过对生成的假设性文档进行编码得到的。给定一个查询 qijq_{ij}qij​ 和一个指令 INSTiINST_iINSTi​，生成模型 ggg 会产出一个假设性文档。查询向量是所有可能生成的假设性文档编码向量的期望值。\nE[vqij]=E[f(g(qij,INSTi))]\\mathbb{E}[v_{q_{ij}}] = \\mathbb{E}[f(g(q_{ij}, INST_i))]\nE[vqij​​]=E[f(g(qij​,INSTi​))]\n\n\n期望的估计 (Estimating the Expectation)：\n在实践中，期望值通过从生成模型 ggg 中采样 N 个假设性文档 dk^\\hat{d_k}dk​^​，然后将它们的编码向量取平均来估计。\nv^qij≈1N∑k=1Nf(dk^)wheredk^∼g(qij,INSTi)\\hat{v}_{q_{ij}} \\approx \\frac{1}{N} \\sum_{k=1}^{N} f(\\hat{d_k}) \\quad \\text{where} \\quad \\hat{d_k} \\sim g(q_{ij}, INST_i)\nv^qij​​≈N1​k=1∑N​f(dk​^​)wheredk​^​∼g(qij​,INSTi​)\n论文中还考虑将原始查询本身也作为一个“假设”，与N个生成文档的向量一起平均，以增强稳健性。\nv^qij=1N+1[∑k=1Nf(dk^)+f(qij)]\\hat{v}_{q_{ij}} = \\frac{1}{N+1} \\left[ \\sum_{k=1}^{N}f(\\hat{d_k}) + f(q_{ij}) \\right]\nv^qij​​=N+11​[k=1∑N​f(dk​^​)+f(qij​)]\n\n\n最终检索 (Final Retrieval)：\n最后，用这个构建好的查询向量 v^qij\\hat{v}_{q_{ij}}v^qij​​ 与语料库中所有真实文档的向量 vdv_dvd​ 计算内积（相似度），并召回得分最高的文档。\nsim(qij,d)=⟨v^qij,vd⟩sim(q_{ij}, d) = \\langle \\hat{v}_{q_{ij}}, v_d \\rangle\nsim(qij​,d)=⟨v^qij​​,vd​⟩\n\n\n\n基线模型 (Baselines)\n论文将 HyDE 与多类基线模型进行了比较：\n\n无监督模型:\n\nBM25: 传统的基于词频的稀疏检索方法。\nContriever / mContriever: HyDE 所使用的底层无监督密集编码器，单独使用作为对比。\n\n\n有监督模型 (作为参考):\n\nDPR, ANCE: 在大规模数据集 MS MARCO 上训练的监督式密集检索模型。\nContriever-ft / mContriever-ft: 在 MS MARCO 上进行微调后的 Contriever 模型，代表了当前先进的监督式方法。\nmDPR, mBERT, XLM-R: 用于多语言任务比较的监督式模型。\n\n\n\n\n数据集 (Datasets)\n实验覆盖了多种任务和语言，以验证 HyDE 的通用性。\n\n网页搜索: TREC DL19 和 TREC DL20，两者都基于 MS MARCO 数据集。\n低资源检索 (BEIR  benchmark): 包含7个不同领域的任务，如 Scifact (科学事实核查), Arguana (论点检索), TREC-COVID (新冠科研文献), FiQA (金融问答), DBPedia (实体检索), TREC-NEWS (新闻检索) 和 Climate-Fever (气候变化事实核查)。\n多语言检索 (Mr.TyDi): 涵盖斯瓦希里语 (sw)、韩语 (ko)、日语 (ja) 和孟加拉语 (bn) 四种语言。\n\n\n可复现性 (Reproducibility)\n\n代码: 论文在首页明确指出其代码是开源的，并提供了 GitHub 链接。实验使用了 Pyserini 工具包进行检索。\n算力/模型:\n\n生成模型: 主要使用了 OpenAI 的 text-davinci-003 API。同时也测试了 Cohere (52B 参数) 和 FLAN-T5-xxl (11B 参数)。\n编码器模型: 使用了 Contriever (BERT-base, 110M 参数) 和 GTR-XL (T5-XL, 1.2B 参数)。\n论文附录中详细列出了所用模型的大小、来源和许可证信息。\n\n\n\n\n一个可改进的点\n指令的精细化与自适应 (Instruction Refinement and Adaptation)。\n论文在分析部分提到，在 FiQA（金融）和 DBPedia（实体）这两个任务上，HyDE 的性能与微调后的模型（Contriever-ft）差距较为明显。作者推测，这可能是因为用于这些任务的指令“欠指定”（under-specification）。他们认为，设计更精巧、更具针对性的指令（more elaborate prompts）可能会提升性能。\n这是一个非常直接的改进方向：\n\n可以研究如何根据查询的意图或领域，自动生成或选择最优的指令。例如，可以训练一个小模型来判断查询属于哪个领域（金融、科学、日常问答等），然后从一个预设的指令库中选择最匹配的指令模板。\n甚至可以探索让大语言模型本身来“反思”和“优化”指令。例如，可以先用一个通用指令生成假设性文档，然后分析其与检索结果的初步匹配情况，再让大语言模型根据该反馈，生成一个更精确的指令，进行第二轮检索。这可以使 HyDE 具备一定的自适应能力。\n\n","categories":["paper"],"tags":["paper","zero-shot","Dense Retrieval","Retrieval"]},{"title":"Unsupervised Dense Retrieval with Relevance-Aware Contrastive Pre-Training","url":"/paper/Unsupervised-Dense-Retrieval-with-Relevance-Aware-Contrastive-Pre-Training/","content":"这是一篇关于如何改进无监督密集检索模型的论文。密集检索（Dense Retrieval）通过将查询（Query）和文档（Passage）都编码成向量，然后在向量空间中寻找最相似的文档来工作。\n问题 (Problem)\n传统的密集检索模型严重依赖大量的人工标注数据，这使得它们在新领域的应用成本高昂且泛化能力不足。\n为了解决这个问题，研究界提出了无监督的对比预训练方法，例如 Contriever 模型。这类方法通过在无标签的文档中自动构建正样本对（例如，从同一篇文档中随机裁剪出两段文字作为“查询”和“相关文档”）来进行学习。\n然而，这种自动构建的方式存在一个核心缺陷：“假正例”（False Positives）问题。如下图1所示，一篇文档中相邻的两个句子也可能在语义上毫不相关。如果模型被强制认为它们是相关的，就会学习到错误的表示，从而损害检索性能。\n\n图1：论文中的一个例子，源自维基百科。高亮的两个句子虽然来自同一篇文章，但内容上几乎没有关联。随机裁剪很容易将它们构造成一个“假正例”对。\n本文提出的 ReContriever 模型，旨在解决无监督预训练中的“假正例”问题。\n方法 (Method)\nReContriever 的核心思想是让模型在训练过程中学会区分“真”正例和“假”正例，并对前者赋予更高的学习权重。它建立在 Contriever 模型之上，并引入了两个关键组件：“一文多对”策略和**“相关性感知对比损失”**。\n一文多对 (One-Document-Multi-Pair)\n传统的对比学习方法通常从一个文档中只生成一个正样本对 (q, d+)。为了更充分地利用数据并为后续的相关性评估提供基础，ReContriever 采用“一文多对”策略。\n具体来说，对于一个文档 T，它会首先裁剪出一个固定的“查询”段落 q，然后再从该文档中裁剪出 n 个不同的“候选正例”段落 d_1+, d_2+, ..., d_n+。这样，就从单个文档中生成了 n 个正样本对。保持 q 不变是关键，因为它为后续比较这 n 个候选段落与 q 的相关性提供了一个统一的基准。\n相关性感知对比损失 (Relevance-Aware Contrastive Loss)\n这是该方法的核心。标准的对比学习损失函数是 InfoNCE，其目标是拉近正样本对的距离，推远负样本对的距离。其公式如下：\nLInfoNCE(q,d+)=−log⁡exp⁡(s(q,d+)/τ)exp⁡(s(q,d+)/τ)+∑i=1Dexp⁡(s(q,di−)/τ)\\mathcal{L}_{\\text{InfoNCE}}(q, d^+) = -\\log \\frac{\\exp(s(q, d^+) / \\tau)}{\\exp(s(q, d^+) / \\tau) + \\sum_{i=1}^{D} \\exp(s(q, d_i^-) / \\tau)}\nLInfoNCE​(q,d+)=−logexp(s(q,d+)/τ)+∑i=1D​exp(s(q,di−​)/τ)exp(s(q,d+)/τ)​\n其中：\n\ns(q,d+)s(q, d^+)s(q,d+) 是查询 q 和正例 d+ 的相似度得分（如点积）。\ndi−{d_i^-}di−​ 是一系列负例文档。\nτ\\tauτ 是一个温度超参数。\n\n传统方法中，一个批次的总损失是所有样本对 InfoNCE 损失的平均值。\nReContriever 对此进行了改进，引入了相关性权重。它利用模型自身在当前训练阶段的能力来充当一个“不完美的预言家”，评估每个正样本对的“可信度”。改进后的损失函数 LrelevanceL_{\\text{relevance}}Lrelevance​ 如下：\nLrelevance=1m∑i=1m∑j=1nsθ(qi,dij+)∑k=1nsθ(qi,dik+)⏟相关性权重⋅LInfoNCE(qi,dij+)\\mathcal{L}_{\\text{relevance}} = \\frac{1}{m} \\sum_{i=1}^{m} \\sum_{j=1}^{n} \\underbrace{\\frac{s_{\\theta}(q_i, d_{ij}^+)}{\\sum_{k=1}^{n} s_{\\theta}(q_i, d_{ik}^+)}}_{\\text{相关性权重}} \\cdot \\mathcal{L}_{\\text{InfoNCE}}(q_i, d_{ij}^+)\nLrelevance​=m1​i=1∑m​j=1∑n​相关性权重∑k=1n​sθ​(qi​,dik+​)sθ​(qi​,dij+​)​​​⋅LInfoNCE​(qi​,dij+​)\n其中：\n\nm 是一个批次中的文档数量，n 是每个文档生成的正样本对数量。\nsθ(qi,dij+)s_{\\theta}(q_i, d_{ij}^+)sθ​(qi​,dij+​) 是模型（参数为 θ\\thetaθ）计算出的第 i 个文档的查询 qiq_iqi​ 与其第 j 个正例 dij+d_{ij}^+dij+​ 之间的相似度。\n相关性权重部分是关键：对于源自同一个文档 i 的所有 n 个正样本对，模型会计算它们各自的相似度，然后进行归一化。模型认为更相关的样本对（即 sθs_{\\theta}sθ​ 得分更高）会获得更大的权重，从而在损失计算中占据主导地位。反之，那些可能是“假正例”的样本对权重会很小，对模型更新的影响也随之减弱。\n\n通过这种方式，ReContriever 能够自适应地关注那些质量更高的正样本对，从而减轻“假正例”带来的负面影响。\nBaseline (对比模型)\n论文将 ReContriever 与多个强大的基线模型进行了比较：\n\n传统稀疏检索模型: BM25。\n无监督密集检索模型:\n\n基于对比学习: Contriever (主要对比对象), SimCSE, coCondenser, Spider。\n基于自编码: RetroMAE。\n\n\n有监督密集检索模型 (作为参考): DPR。\n\n数据集 (Datasets)\n模型在两个主流的评测基准上进行了评估：\n\nBEIR: 一个异构的信息检索评测基准，包含了15个不同的公开数据集，用于评估模型的零样本（zero-shot）泛化能力。\n开放域问答 (Open-Domain QA): 三个代表性的问答检索数据集：Natural Questions (NQ), TriviaQA, 和 WebQuestions (WQ)。\n\n此外，还在4个特定领域的语料库上进行了进一步预训练的实验，以测试模型的实际应用潜力。\n可复现性 (Reproducibility)\n\n代码: 论文明确指出代码是公开的（在GitHub上：Yibin-Lei/ReContriever）。代码基于 Pytorch、Huggingface-Transformers 和 Contriever 的官方代码实现，具有较好的可复现基础。\n算力:\n\n预训练: 在 16 块 NVIDIA A100 GPU 上进行，批处理大小为 2048，训练了 500,000 步。\n领域内预训练: 在 8 块 NVIDIA A100 GPU 上进行。\n少样本实验: 在单块 NVIDIA A100 GPU 上进行。\n模型从 BERT-base-uncased 初始化。这些详细的配置和算力需求为复现工作提供了清晰的指引。\n\n\n\n可改进的几个点 (Potential Improvements)\n论文在局限性部分也坦诚地指出了几个未来可以改进的方向：\n\n通用性仍有不足: 尽管 ReContriever 表现出色，但在零样本、通用场景下，其性能仍然落后于经典的 BM25 模型。这意味着在面对一个全新的领域时，如果想获得最佳性能，可能仍需要基于该领域的语料进行额外的预训练，这在一定程度上限制了它的即开即用性。\n潜在的社会偏见: 模型基于 BERT-base 初始化，因此不可避免地会继承其预训练语料中可能存在的社会偏见（如性别、种族歧视等），这在使用时需要特别注意，并可能需要采取去偏见措施。\n\n可以被引用的一些结论 (Citable Conclusions)\n\n核心贡献: 通过“一文多对”和“相关性感知对比损失”，ReContriever 有效缓解了无监督对比学习中的“假正例”问题，为密集检索提供了一个更鲁棒的预训练范式。\n性能超越 SOTA: 在 BEIR 基准的15个任务中，ReContriever 在10个任务上优于之前的最强无监督模型 Contriever，平均排名第一。在开放域问答检索任务中，性能也几乎全面领先于所有无监督方法。\n实践应用价值高:\n\n领域适应性强: 在特定领域的语料上进行短暂的二次预训练后，ReContriever 能够稳定地超越强大的 BM25 基线，展现了很好的领域适应潜力。\n少样本学习能力强: 在只有少量（例如128个）标注样本的情况下进行微调，ReContriever 的性能可以媲美使用数千个样本训练的有监督 DPR 模型，这对于标注数据稀缺的场景非常有价值。\n\n\n方法组件的必要性: 消融实验证明，“一文多对”策略和“相关性感知损失”缺一不可。单独使用相关性感知损失甚至会导致性能下降，说明两者结合才能产生协同效应，共同提升模型性能。\n\n","categories":["paper"],"tags":["paper","Dense Retrieval","unsupervised"]},{"title":"Toolformer: Language Models Can Teach Themselves to Use Tools","url":"/paper/Toolformer-Language-Models-Can-Teach-Themselves-to-Use-Tools/","content":"问题 (Problem)\n大型语言模型（LLMs）虽然在文本生成和理解上表现出色，但存在一些固有的核心缺陷：\n\n知识过时与幻觉：LLMs的知识被冻结在训练数据的时间点，无法获取最新信息，并且有编造事实（幻觉）的倾向。\n缺乏精确计算能力：LLM不擅长进行精确的数学运算，容易在算术问题上犯错。\n时间感知能力弱：模型对当前日期和时间没有概念，无法回答与时间流逝相关的问题。\n低资源语言处理能力不足：在处理数据稀少的语言时表现不佳。\n\n现有的解决方案通常依赖大量的人工标注来教模型如何使用工具，或者只能在特定任务的设定下使用工具，缺乏通用性。因此，本文旨在解决一个核心问题：如何让语言模型以一种自监督的、通用的方式，学会自己决定何时、如何以及使用何种外部工具来弥补自身缺陷？\n方法 (Method)\nToolformer的核心思想是：一个有用的API调用，应该能帮助模型更好地预测未来的文本。基于此，作者提出了一种全新的、自监督的学习范式，让模型“教会自己”使用工具。该方法主要分为三个步骤，如下图2所示：\n\n\n图例解读：上图以一个问答（QA）工具为例，展示了如何为一个句子 “Pittsburgh is also known as the Steel City.” 自动生成和筛选API调用。\n\n输入文本 (x): 分为上下文 x1:i−1x_{1:i-1}x1:i−1​ (“Pittsburgh is also known as”) 和后续文本 xi:nx_{i:n}xi:n​ (“the Steel City”)。\n步骤1 (Sample API Calls): 在位置 i，模型可能会产生多个API调用候选，例如 ci1c_i^1ci1​（问匹兹堡的别名）和 ci2c_i^2ci2​（问匹兹堡在哪个国家）。\n步骤2 (Execute API Calls): 执行这些调用，得到结果 ri1r_i^1ri1​ (“Steel City”) 和 ri2r_i^2ri2​ (“United States”)。\n步骤3 (Filter API Calls): 通过一个损失函数（L）来判断哪个API调用是有用的。\n\n调用 ci1c_i^1ci1​ 及其结果 ri1r_i^1ri1​ (“Steel City”) 显著降低了模型预测后续文本 “the Steel City” 的难度（损失减小），因此这个API调用被保留。\n调用 ci2c_i^2ci2​ 及其结果 ri2r_i^2ri2​ (“United States”) 对于预测 “the Steel City” 毫无帮助（损失没有减小），因此被丢弃。\n\n\n最终输出 (x*): 将被保留的、有用的API调用及其结果整合回原始文本，形成一条新的、增强的训练数据。\n\n\n\n\n方法详解\n第一步：采样API调用 (Sampling API Calls)\n\n构建Prompt：为每一种工具（如QA、计算器）人工撰写一个包含少量示例的Prompt。这个Prompt会引导模型在新的文本中生成类似的API调用。\n生成候选：将 Prompt + 原始文本x 输入给LM。模型会在文本的每个位置 i 评估生成一个特殊token &lt;API&gt; 的概率。\n选择位置和内容：选取概率超过阈值 τs\\tau_sτs​ 的位置作为API调用的候选位置。在这些位置，让模型续写生成完整的API调用，例如 [QA(&quot;What other name is Pittsburgh known by?&quot;)]。\n\n第二步：执行API调用 (Executing API Calls)\n这是一个简单的步骤，将上一步生成的API调用（如 Calculator(400/1400)）发送给对应的外部工具（如Python解释器）并获取返回结果（如 0.29）。\n第三步：过滤API调用 (Filtering API Calls)\n这是整个方法的核心和精髓，通过一个自监督的损失函数来判断API调用的“有效性”。\n\n\n定义损失函数：\n作者定义了一个加权的交叉熵损失 Li(z)L_i(z)Li​(z)，用于衡量在给定前缀 z 的条件下，模型预测后续文本 xi,...,xnx_{i}, ..., x_{n}xi​,...,xn​ 的难度。\nLi(z)=−∑j=inwj−i⋅log⁡pM(xj∣z,x1:j−1)L_{i}(z) = - \\sum_{j=i}^{n} w_{j-i} \\cdot \\log p_{M}(x_{j} | \\mathbf{z}, x_{1:j-1})\nLi​(z)=−j=i∑n​wj−i​⋅logpM​(xj​∣z,x1:j−1​)\n其中，wj−iw_{j-i}wj−i​ 是一个权重函数，它使得离API调用位置更近的token具有更高的权重。\n\n\n定义过滤标准：\n对于一个在位置 i 的API调用 cic_ici​ 及其返回结果 rir_iri​，模型会比较两种情况下的损失：\n\nLi+L_i^+Li+​: 提供了API调用及其完整结果时的损失。前缀 zzz 是包含API调用和结果的序列 e(ci,ri)e(c_i, r_i)e(ci​,ri​)。\nLi−L_i^-Li−​: 不提供帮助或提供不完整帮助时的最小损失。它取以下两种情况的较小值：\n\n完全不进行API调用时的损失（前缀 zzz 为空序列 ϵ\\epsilonϵ）。\n进行了API调用但不提供其结果时的损失（前缀 zzz 是 e(ci,ϵ)e(c_i, \\epsilon)e(ci​,ϵ)）。\n\n\n\n一个API调用被认为是“有用的”，当且仅当它能显著降低模型的预测损失。其数学表达式为：\nLi−−Li+≥τfL_i^- - L_i^+ \\ge \\tau_f\nLi−​−Li+​≥τf​\n其中 τf\\tau_fτf​ 是一个预先设定的过滤阈值。只有满足这个条件的API调用才会被保留。\n\n\n最后一步：模型微调 (Model Finetuning)\n将所有文本中经过筛选的有用的API调用及其结果，插入到原始文本中，形成一个增强的数据集 C∗C^*C∗。然后，用这个新的数据集 C∗C^*C∗ 对原始的语言模型M进行标准的语言模型任务微调。\n通过这个过程，模型学会了在什么上下文中、应该生成什么样的API调用来帮助自己更好地完成语言建模任务。\nBaseline模型\n为了验证Toolformer的性能，论文主要与以下几类模型进行了比较：\n\nGPT-J (6.7B): 作为Toolformer的基础模型，未经任何微调。\nGPT-J + CC: 在与Toolformer相同的CCNet子集上进行微调，但数据中不包含任何API调用。这是一个对照组，用来验证性能提升不仅仅来自于额外的微调。\nToolformer (disabled): 训练好的Toolformer模型，但在推理时不让它调用API。这用来检验模型自身的知识和能力是否因学习使用工具而得到提升。\n更大规模的模型: OPT (66B) 和 GPT-3 (175B)，用来证明Toolformer作为一个小得多的模型，在工具的帮助下可以达到甚至超越这些巨型模型的性能。\n\n数据集\n\n\n训练数据集:\n\n使用了一个庞大的网络文本数据集 CCNet 的子集作为基础训练语料。Toolformer的方法将这个纯文本数据集转换为了一个包含API调用的增强数据集。\n\n\n\n评测数据集 (下游任务):\n\n事实问答 (LAMA): 评测模型回忆事实知识的能力。\n数学推理 (ASDiv, SVAMP, MAWPS): 评测模型的数学计算和逻辑推理能力。\n开放域问答 (WebQS, NQ, TriviaQA): 评测模型利用外部知识回答问题的能力。\n多语言问答 (MLQA): 评测模型的跨语言理解能力。\n时间敏感问答 (TEMPLAMA, DATESET): 评测模型对时间变化的感知能力。\n语言建模能力 (WikiText, CCNet): 评测模型在加入API调用后，其核心的语言建模能力是否受损。\n\n\n\n可复现性 (Reproducibility)\n\n代码: 论文没有明确提供开源代码，但详细描述了其实现方法。底座模型是公开的GPT-J，所使用的工具（如BM25检索、Atlas模型等）也大多是基于现有研究或开源工具构建的。\n算力: 论文中提到，微调过程使用了 8块 NVIDIA A100 40GB GPUs。这是一个相当大的算力需求，对于个人研究者或小型实验室来说有一定门槛，但对于企业或大型研究机构是可行的。\n超参数: 论文在附录中详细列出了采样阈值 τs\\tau_sτs​、过滤阈值 τf\\tau_fτf​、学习率、批量大小等关键超参数，为复现实验提供了重要参考。\n\n可改进的几个点 (Limitations &amp; Future Work)\n论文作者坦诚地指出了当前方法的几个局限性，这些也构成了未来研究的潜在方向：\n\n链式工具使用 (Chained Tool Use): 当前模型无法将一个工具的输出作为另一个工具的输入（例如，先用搜索工具找到信息，再把信息交给计算器处理）。这是因为API调用是独立生成的，训练数据中没有这种链式调用的样本。\n与工具的交互性: 模型只能进行一次性的API调用，无法与工具进行多轮交互，例如根据初步搜索结果调整搜索关键词，或者浏览多个搜索结果页面。\n对输入措辞的敏感性: 和其他LLM一样，Toolformer在决定是否调用API时，对输入的具体措辞非常敏感。微小的变化可能导致完全不同的行为。\n样本效率问题: 对于某些工具（尤其是计算器），需要处理海量的文本数据才能筛选出少量有用的API调用样本，效率较低。\n缺乏成本意识: 模型在调用API时，没有考虑现实世界中可能存在的计算成本、时间成本或金钱成本。\n\n可以被引用的一些结论\n\n自监督学习工具使用的可行性: 论文最有价值的结论是，语言模型完全可以通过一种自监督的方式，教会自己使用外部工具，而无需大量昂贵的人工标注。\n“有用性”的可量化: 将“API调用的有用性”定义为“对预测未来文本的损失降低程度”，是一个非常巧妙且有效的自监督信号。\n小模型+工具 &gt; 大模型: Toolformer (6.7B) 在多个需要事实、计算或实时信息的任务上，其性能显著超越了比它大10倍甚至25倍的OPT (66B)和GPT-3 (175B)。这证明了让模型学会使用工具是比单纯扩大模型规模更高效的提升能力的路径。\n工具使用是一种涌现能力: 如下图4所示，模型学习使用工具的能力与其规模正相关。在模型参数达到约775M（0.7B）之后，使用工具带来的性能提升才开始变得显著。这表明，有效利用工具本身也是大模型的一种“涌现能力”。\n通用性与核心能力保持: Toolformer学习使用工具的过程不会损害其核心的语言建模能力（在标准LM benchmark上困惑度没有增加），并且它学会的是一种通用的、在推理时（zero-shot）自主决策的能力，而不是针对特定任务的死记硬背。\n\n","categories":["paper"],"tags":["paper","LLM"]},{"title":"Tree","url":"/DataStru-Algo/Tree/","content":"SCNU-Turing-Class CLRS Discussion Week4-5\n\n\nCiallo～(∠・ω&lt;)⌒★！Ciallo～(∠・ω&lt; )⌒★！Ciallo～(∠・ω&lt;)⌒★！​\n​右子树就是柚子树\n​柚子厨蒸鹅心\nCiallo～(∠・ω&lt;)⌒★！Ciallo～(∠・ω&lt; )⌒★！Ciallo～(∠・ω&lt;)⌒★！\n\n二叉树 Binary Tree\n二叉树的定义\n​一种非线性数据结构，代表“祖先”与“后代”之间的派生关系，体现了“一分为二”的分治逻辑。\n​每个节点包含的属性有：\n1. left指针，指向左子节点\n2. right指针，指向右子节点\n3. prev指针，指向父节点（书上这么写的，但实际构建可能不写这个。当然可以构建一个prev指针指向父节点，形成一个双向二叉树）\n4. key关键字，节点的值\n构建代码如下：\n//c++struct TreeNode &#123;    int val;          // 节点值    TreeNode *left;   // 左子节点指针    TreeNode *right;  // 右子节点指针    TreeNode *prev;   // 父节点指针    TreeNode(int x) : val(x), left(nullptr), right(nullptr), prev(nullptr) &#123;&#125;&#125;;\n二叉树常用术语\n\n「根节点 root noderoot\\ noderoot node 」：位于二叉树顶层的节点，没有父节点。\n「叶节点 leaf nodeleaf\\ nodeleaf node 」：没有子节点的节点，其两个指针均指向 None 。\n节点的「度 degreedegreedegree 」：节点的子节点的数量。在二叉树中，度的取值范围是 0、1、2 。\n二叉树的「高度 heightheightheight 」：从根节点到最远叶节点所经过的边的数量。\n节点的「深度 depthdepthdepth 」：从根节点到该节点所经过的边的数量。\n节点的「高度 heightheightheight 」：从距离该节点最远的叶节点到该节点所经过的边的数量。\n\n常见二叉树类型\n\n\n完美二叉树 perfect binary treeperfect\\ binary\\ treeperfect binary tree\n\n所有层的节点都被完全填满。\n在完美二叉树中，叶节点的度为 0，其余所有节点的度都为2\n若树的高度为 ℎ ，则节点总数为 2^ℎ+1^−1 ，呈现标准的指数级关系。\n\n\n\n完全二叉树 complete binary treecomplete\\ binary\\ treecomplete binary tree\n\n只有最底层的节点未被填满\n最底层节点尽量靠左填充。\n\n\n\n完满二叉树 full binary treefull\\ binary\\ treefull binary tree\n\n所有节点都有两个子节点（除了叶节点）。\n\n\n\n平衡二叉树 balanced binary treebalanced\\ binary\\ treebalanced binary tree\n\n任意节点的左子树和右子树的高度之差的绝对值不超过 1 。\n\n\n\n二叉树的退化\n​ 当二叉树的每层节点都被填满时，达到“完美二叉树”；而当所有节点都偏向一侧时，二叉树退化为“链表”。\n\n完美二叉树是理想情况，可以充分发挥二叉树“分治”的优势。\n链表则是另一个极端，各项操作都变为线性操作，时间复杂度退化至O(n)O(n)O(n)​。\n\n\n\n\n\n完美二叉树\n链表\n\n\n\n\n第iii层的节点数量\n2i−12^{i-1}2i−1\n111\n\n\n高度为hhh的树的叶节点数量\n2h−12^{h-1}2h−1\n111\n\n\n高度为hhh的树的节点总数\n2h−1−12^{h-1}-12h−1−1\nh+1h+1h+1\n\n\n节点总数为nnn的树的高度\nlog⁡2(n+1)−1\\log_{2}{(n+1)}-1log2​(n+1)−1\nn−1n-1n−1\n\n\n\n二叉搜索树 Binary Search Tree\n\n一棵 x.left.key≤\\le≤ x.right.key 的二叉树\n\n二叉树的遍历\n深度优先搜索 Depth-first traversal\n\n\n二叉树的遍历在「深度优先搜索 DFT」中分为三种：\n\n\n前序遍历\n\n\n中序遍历\n\n\n后续遍历\n\n\n\n\n\n代码实现\nINORDER-TREE-WALK(x)  if x≠NIL  INORDER-TREE-WALK(x.left)  print x.key INORDER-TREE-WALK(x.right)\n/* 前序遍历 */void preOrder(TreeNode *root) &#123;    if (root == nullptr)        return;    // 访问优先级：根节点 -&gt; 左子树 -&gt; 右子树    vec.push_back(root-&gt;val);    preOrder(root-&gt;left);    preOrder(root-&gt;right);&#125;/* 中序遍历 */void inOrder(TreeNode *root) &#123;    if (root == nullptr)        return;    // 访问优先级：左子树 -&gt; 根节点 -&gt; 右子树    inOrder(root-&gt;left);    vec.push_back(root-&gt;val);    inOrder(root-&gt;right);&#125;/* 后序遍历 */void postOrder(TreeNode *root) &#123;    if (root == nullptr)        return;    // 访问优先级：左子树 -&gt; 右子树 -&gt; 根节点    postOrder(root-&gt;left);    postOrder(root-&gt;right);    vec.push_back(root-&gt;val);&#125;\n复杂度分析\n\n时间复杂度：Θ(n)\\Theta(n)Θ(n)\n\n对于渐进下界：\n​ 遍历需要访问 BSTBSTBST 的全部节点。所以有：\nT(n)=Ω(n)T(n)=\\Omega(n)\nT(n)=Ω(n)\n接下来证明渐进上界：\n​ 对于一棵空树，调用遍历函数需要一个极小的常数时间。所以设T(0)=cT(0)=cT(0)=c，ccc为常数​\n​ 对于一棵节点数n&gt;0n&gt;0n&gt;0 的树，设左子树节点数 k&gt;0k&gt;0k&gt;0 ，右子树节点数为 n−k−1n-k-1n−k−1 ，额外开销为ddd，则递归不等式为：\nT(n)≤T(k)+T(n−k−1)+d，d为常数T(n)\\le T(k)+T(n-k-1)+d，d为常数\nT(n)≤T(k)+T(n−k−1)+d，d为常数\n​ 我们不妨假设 ∃ c ∀ R\\exists{\\ c\\ \\forall\\ R}∃ c ∀ R ，当 ccc 很大时，有：T(n)≤cnT(n)\\le cnT(n)≤cn\n​ 则上式可化为：\nT(n)≤ck+c(n−k−1)+d=cn−(c−d)T(n) \\le ck+c(n-k-1)+d=cn-(c-d)\nT(n)≤ck+c(n−k−1)+d=cn−(c−d)\n​ 当 c−d≤0c-d\\le 0c−d≤0 时，有 T(n)=O(n)T(n)=O(n)T(n)=O(n)​ 。\n综上所述，==T(n)=Θ(n)T(n)=\\Theta (n)T(n)=Θ(n)==​​​\n\n空间复杂度：O(n)O(n)O(n)\n\n​ 在最差情况下，即树退化为链表时，递归深度达到 nnn ，系统占用  O(n)O(n)O(n)​ 栈帧空间。\n广度优先遍历 Breadth-first traversal\n\n「层序遍历 level-order traversal」\n\n​ 从顶部到底部逐层遍历二叉树，并在每一层按照从左到右的顺序访问节点。\n代码实现\n​ 广度优先遍历通常借助“队列”来实现。队列遵循“先进先出”的规则，而广度优先遍历则遵循“逐层推进”的规则，两者背后的思想是一致的。\n/* 层序遍历 */vector&lt;int&gt; levelOrder(TreeNode *root) &#123;    // 初始化队列，加入根节点    queue&lt;TreeNode *&gt; queue;    queue.push(root);    // 初始化一个列表，用于保存遍历序列    vector&lt;int&gt; vec;    while (!queue.empty()) &#123;        TreeNode *node = queue.front();        queue.pop();              // 队列出队        vec.push_back(node-&gt;val); // 保存节点值        if (node-&gt;left != nullptr)            queue.push(node-&gt;left); // 左子节点入队        if (node-&gt;right != nullptr)            queue.push(node-&gt;right); // 右子节点入队    &#125;    return vec;&#125;\n复杂度分析\n\n时间复杂度：Θ(n)\\Theta (n)Θ(n)​\n\n​ 所有节点被访问一次，使用 Θ(n)\\Theta (n)Θ(n) 时间，其中 nnn 为节点数量。\n\n空间复杂度：O(n)O(n)O(n)​\n\n​ 在最差情况下，即满二叉树时，遍历到最底层之前，队列中最多同时存在 (n+1)/2(n+1)/2(n+1)/2 个节点，占用 O(n)O(n)O(n) 空间。\n二叉搜索树的查找\n​ 给定目标节点值 num ，可以根据二叉搜索树的性质来查找。如图 7-17 所示，我们声明一个节点 cur ，从二叉树的根节点 root 出发，循环比较节点值 cur.val 和 num 之间的大小关系。\n\n若 cur.val &lt; num ，说明目标节点在 cur 的右子树中，因此执行 cur = cur.right 。\n若 cur.val &gt; num ，说明目标节点在 cur 的左子树中，因此执行 cur = cur.left 。\n若 cur.val = num ，说明找到目标节点，跳出循环并返回该节点。\n\n代码实现\n/* 查找节点 */TreeNode *search(int num) &#123;    TreeNode *cur = root;    // 循环查找，越过叶节点后跳出    while (cur != nullptr &amp;&amp; cur-&gt;val != num) &#123;        // 目标节点在 cur 的右子树中        if (cur-&gt;val &lt; num)            cur = cur-&gt;right;        // 目标节点在 cur 的左子树中        else            cur = cur-&gt;left;    &#125;    // 返回目标节点    return cur;&#125;\n最大关键字元素和最小关键字元素\n/* 最小关键字-迭代 */TreeNode *Minimun(TreeNode *x)&#123; while(x-&gt;left != nullptr)        x = x-&gt;left;    return x;&#125;\n/* 最小关键字-递归 */TreeNode *Minimun(TreeNode *x)&#123;    if(x-&gt;left != nullptr)        return Minimun(x-&gt;left);    else        return x;&#125;\n/* 最大关键字 */TreeNode *Maximun(TreeNode *x)&#123; while(x != nullptr)        x = x-&gt;right;    return x;&#125;\n/* 最大关键字-递归 */TreeNode *Maximun(TreeNode *x)&#123;    if(x-&gt;right != nullptr)        return Minimun(x-&gt;right);    else        return x;&#125;\n后继和前驱\n\nTREE-SUCCESSOR(x) if x.right ≠ NIL   return TREE-MINIMUM(x.right)  y = x.p  while y ≠ NIL and x == y.right   x = y   y = y.p  return y\n/* c++版后继 */TreeNode *Successor(TreeNode *x)&#123;    //如果结点x的右子树非空，那么x的后继恰是x右子树中的最左结点    if(x-&gt;right != nullptr)        return Minimun(x-&gt;right);    TreeNode *y=x-&gt;prev;    //如果x的右子树为空并有一个后继，那么向上回溯直到**x节点**是其父节点的左孩子    while(y != nullptr &amp;&amp; x-&gt;val == y-&gt;right-&gt;val)&#123;        x = y;        y = y-&gt;prev;    &#125;    return y;&#125;\n/* c++版前驱 */TreeNode *Predecessor(TreeNode *x)&#123;    //如果结点x的左子树非空，那么x的前驱恰是x左子树中的最右结点    if(x-&gt;left != nullptr)        return Maximun(x-&gt;left);    TreeNode *y=x-&gt;prev;    //如果x的左子树为空并有一个前驱，那么向上回溯直到**x节点**是其父节点的右孩子    while(y != nullptr &amp;&amp; x-&gt;val == y-&gt;left-&gt;val)&#123;        x = y;        y = y-&gt;prev;    &#125;    return y;&#125;\n二叉搜索树的插入\n​ 给定一个待插入元素 num ，为了保持二叉搜索树“左子树 &lt; 根节点 &lt; 右子树”的性质，插入操作流程如下所示：\n\n查找插入位置：与查找操作相似，从根节点出发，根据当前节点值和 num 的大小关系循环向下搜索，直到越过叶节点（遍历至 None ）时跳出循环。\n在该位置插入节点：初始化节点 num ，将该节点置于 None 的位置。\n\n代码实现\n/* 伪代码 */TREE-INSERT(T,z)  y = NIL x = T.root  while x ≠ NIL   y = x   if z.key &lt; x.key    x = x.left  else    x = x. right  z.p = y  if y == NIL   T. root= z   // tree T was empty  elseif z.key &lt; y.key   y.left= z  else y.right = z\n/* 插入节点-迭代版 */void insert(int num) &#123;    // 若树为空，则初始化根节点    if (root == nullptr) &#123;        root = new TreeNode(num);        return;    &#125;    TreeNode *cur = root, *pre = nullptr;    // 循环查找，越过叶节点后跳出    while (cur != nullptr) &#123;        // 找到重复节点，直接返回        if (cur-&gt;val == num)            return;        pre = cur;        // 插入位置在 cur 的右子树中        if (cur-&gt;val &lt; num)            cur = cur-&gt;right;        // 插入位置在 cur 的左子树中        else            cur = cur-&gt;left;    &#125;    // 插入节点    TreeNode *node = new TreeNode(num);    if (pre-&gt;val &lt; num)        pre-&gt;right = node;    else        pre-&gt;left = node;&#125;\n/* 插入节点-递归版 */void insert(TreeNode* &amp;cur, int num) &#123;    // 如果当前节点为空，则创建新节点并返回    if (cur == nullptr) &#123;        cur = new TreeNode(num);        return;    &#125;    // 如果值已存在，则直接返回    if (cur-&gt;val == num)        return;    // 如果要插入的值比当前节点的值大，则插入到右子树中    if (cur-&gt;val &lt; num)        insert(cur-&gt;right, num);    // 如果要插入的值比当前节点的值小，则插入到左子树中    else        insert(cur-&gt;left, num);&#125;\n复杂度分析\n\n时间复杂度：O(log n)O(log\\ n)O(log n) ，n为节点数\n空间复杂度：O(1)O(1)O(1)\n\n二叉搜索树的删除\n​ 先在二叉树中查找到目标节点，再将其删除。与插入节点类似，我们需要保证在删除操作完成后，二叉搜索树的“左子树 &lt; 根节点 &lt; 右子树”的性质仍然满足。\n​ 因此，我们根据目标节点的子节点数量，分 0、1 和 2 三种情况，执行对应的删除节点操作。\n\n\n若 zzz 有0个子节点，则简单地将 zzz 删除，并修改 zzz 的父节点，用 NILNILNIL​ 作为孩子来替换 zzz 。\n\n\n若 zzz 有1个子节点，则将这个子节点提升到 zzz 的位置，并修改 zzz 的父节点，用 zzz 的孩子来替换 zzz 。（图a、b）\n\n\n若 zzz 有2个子节点，找 zzz 的后继 yyy ，然后 yyy 的右孩子（若有）取代 yyy 的位置，最后用 yyy 取代 zzz 。 （图c、d）\n\n\n\n\n\n\n代码实现\n/* 将以u为根的子树用以v为根的子树代替 */TRANSPLANT(T,u,v) /*若u为根节点*/ if u.p == NIL   T.root=v  /*若u是其父节点的左孩子*/ else if u == u.p.left  u.p.left = v /*若u是其父节点的右孩子*/ else  u.p.right = v if v ≠ NIL  v.p=u.p\n/* 删除结点z */TREE-DELETE(T,z) /* 若z没有左孩子-图a */ if z.left == NIL  TRANSPLANT(T,z,z.right) /* 以z的右子树代替z */ /* 若z没有右孩子-图b */ else if z.right == NIL  TRANSPLANT(T,z,z.left) /* 以z的左子树代替z */ /* z有两个孩子 */ else  y = TREE-MINIMUM(z.right) /* y为z的后继，y必然没有左节点 */  /* 若y不是z的孩子-图d */  if y.p ≠ z   TRANSPLANT(T,y,y.right) /* 将y的位置替换为y的右子树 */   y.right = z.right /* 将y的右子树设置为z的右子树 */   y.right.p = y /* 将y的右子树的父节点设置为y */  /* 若y是z的右孩子-图c 以及图d的后续 */  TRANSPLANT(T,z,y) /* 用y替换z */  y.left = z.left   y.left.p = y\n/* 删除节点 */void remove(int num) &#123;    // 若树为空，直接提前返回    if (root == nullptr)        return;    TreeNode *cur = root, *pre = nullptr;    // 循环查找，越过叶节点后跳出    while (cur != nullptr) &#123;        // 找到待删除节点，跳出循环        if (cur-&gt;val == num)            break;        pre = cur;        // 待删除节点在 cur 的右子树中        if (cur-&gt;val &lt; num)            cur = cur-&gt;right;        // 待删除节点在 cur 的左子树中        else            cur = cur-&gt;left;    &#125;    // 若无待删除节点，则直接返回    if (cur == nullptr)        return;        // 子节点数量 = 0 or 1    if (cur-&gt;left == nullptr || cur-&gt;right == nullptr) &#123;        // 当子节点数量 = 0 or 1 时， child = nullptr or 该子节点        TreeNode *child = cur-&gt;left != nullptr ? cur-&gt;left : cur-&gt;right;        // 删除节点 cur        if (cur != root) &#123;            if (pre-&gt;left == cur)                pre-&gt;left = child;            else                pre-&gt;right = child;        &#125;         else &#123;            // 若删除节点为根节点，则重新指定根节点            root = child;        &#125;        // 释放内存        delete cur;    &#125;    // 子节点数量 = 2    else &#123;        // 获取中序遍历中 cur 的下一个节点        TreeNode *tmp = cur-&gt;right;        while (tmp-&gt;left != nullptr) &#123;            tmp = tmp-&gt;left;        &#125;        int tmpVal = tmp-&gt;val;        // 递归删除节点 tmp        remove(tmp-&gt;val);        // 用 tmp 覆盖 cur        cur-&gt;val = tmpVal;    &#125;&#125;\n复杂度分析\n\n时间复杂度：O(log n)O(log\\ n)O(log n) ，n为节点数\n空间复杂度：O(1)O(1)O(1)​\n\n*随机构建二叉搜索树 Randomly built binary search trees\n​ 定义 nnn​ 个关键字的一棵随机构建二叉搜索树为按随机次序插入这些关键字到一棵初始的空树中而生成的树，这里输入关键字的 n!n!n! 个排列中的每个都是等可能地出现。那么对于这棵树的期望高度，则有如下定理：\n\n定理 一棵有 nnn 个不同关键字的随机构建二叉搜索树的期望高度为 ==O(log n)O(log\\ n)O(log n)== 。\n证明如下：\n\n​ 我们先定义如下三个随机变量：\n​  1. XnX_{n}Xn​ 为一棵有 nnn 个不同关键字的随机构建二叉搜索树的高度。\n​  2. Yn=2XnY_{n}=2^{X_{n}}Yn​=2Xn​ 为指数高度，定义 Y0=0Y_{0}=0Y0​=0 和 Y1=20=1Y_{1}=2^{0}=1Y1​=20=1 。\n​  3. RnR_{n}Rn​ 为一个随机变量，选择一个关键字作为树根，其为这个关键字在 nnn 个关键字集合中的秩（rankrankrank），表示这个关键字在排好序后应占据的位置。RnR_{n}Rn​ 的值对于集合 {1,2,...,n}\\{1, 2,...,n\\}{1,2,...,n} 中的任何元素都是等可能的。\n​  若 Rn=iR_{n}=iRn​=i ，那么根的左子树是一棵有 i−1i-1i−1 个关键字的随机构建二叉搜索树，并且右子树是一棵有 n−in-in−i 个关键字的随机构建搜二叉索树。\n​  因为二叉树的高度为：\nXn=max(leftSubtree.height,rightSubtree.height)+1X_{n} =max(leftSubtree.height,rightSubtree.height)+1\nXn​=max(leftSubtree.height,rightSubtree.height)+1\n​  所以有：\nYn=2⋅max(Yi−1,Yn−i)Y_{n}=2\\cdot max(Y_{i-1},Y_{n-i})\nYn​=2⋅max(Yi−1​,Yn−i​)\n​  接下来，我们定义指示器随机变量（chapter 5.2chapter\\ 5.2chapter 5.2）Zn,1Z_{n,1}Zn,1​ ，Zn,2Z_{n,2}Zn,2​ ，......... ，Zn,nZ_{n,n}Zn,n​ ，其中：\nZn,i=I{Rn=i}={ 1n=i 0n≠i①Z_{n,i}=I\\{R_{n}=i\\}=\\begin{cases}\n\\ 1     &amp;n=i\\\\\n\\ 0     &amp;n\\ne i\n\\end{cases}\n\\qquad ①\nZn,i​=I{Rn​=i}={ 1 0​n=in=i​①\n​  又因为 Rn=iR_{n}=iRn​=i 对于集合中每一个元素取值的概率是相同的，所以：\nP{Rn=i}=1n ,(i=1,2,...,n)②P\\{R_{n}=i\\}=\\frac{1}{n}\\ ,(i=1,2,...,n) \\qquad ②\nP{Rn​=i}=n1​ ,(i=1,2,...,n)②\n​  结合 ① ②①\\ ②① ② ，得到：\nE[Zn,i]=1⋅P{Rn=i}=1n ,(i=1,2,...,n)③E[Z_{n,i}]=1 \\cdot P\\{R_{n}=i\\}=\\frac{1}{n} \\ ,(i=1,2,...,n) \\quad ③\nE[Zn,i​]=1⋅P{Rn​=i}=n1​ ,(i=1,2,...,n)③\n​  由于 Zn,iZ_{n,i}Zn,i​ 恰有一个值为 111 ，其余所有值为 000 ，因此：\nYn=∑i=1nZn,i(2⋅max(Yi−1,Yn−i))Y_{n}=\\sum_{i=1}^{n}{Z_{n,i}(2\\cdot max(Y_{i-1},Y_{n-i}))}\nYn​=i=1∑n​Zn,i​(2⋅max(Yi−1​,Yn−i​))\n​  由于 Zn,iZ_{n,i}Zn,i​ 独立于 Yi−1,Yn−iY_{i-1},Y_{n-i}Yi−1​,Yn−i​ 的值，上式两边取期望，得：\nE[Yn]=E[∑i=1nZn,i(2⋅max⁡(Yi−1,Yn−i))]=∑i=1nE[Zn,i(2⋅max⁡(Yi−1,Yn−i))](期望的线性性质)=∑i=1nE[Zn,i]⋅E[2⋅max⁡(Yi−1,Yn−i)](独立性)=∑i=1n1n⋅E[2⋅max⁡(Yi−1,Yn−i)](③式)=2n∑i=1nE[max⁡(Yi−1,Yn−i)](期望的线性性质)≤2n∑i=1n(E[Yi−1]+E[Yn−i])(补充证明1)\\begin{align*}\nE[Y_{n}] &amp;= E\\left[\\sum_{i=1}^{n}{Z_{n,i}(2 \\cdot \\max(Y_{i-1},Y_{n-i}))}\\right] \\\\\n&amp;=\\sum_{i=1}^{n}E[{Z_{n,i}}(2 \\cdot \\max(Y_{i-1},Y_{n-i}))] &amp;(期望的线性性质)\\\\\n&amp;=\\sum_{i=1}^{n}E[{Z_{n,i}}]\\cdot E[2 \\cdot \\max(Y_{i-1},Y_{n-i})] &amp;(独立性)\\\\\n&amp;=\\sum_{i=1}^{n}\\frac{1}{n}\\cdot E[2 \\cdot \\max(Y_{i-1},Y_{n-i})] &amp;(③式)\\\\\n&amp;=\\frac{2}{n}\\sum_{i=1}^{n}E[\\max(Y_{i-1},Y_{n-i})] &amp;(期望的线性性质)\\\\\n&amp;\\le\\frac{2}{n}\\sum_{i=1}^{n}(E[Y_{i-1}]+E[Y_{n-i}]) &amp;(补充证明1)\\\\\n\\end{align*}\nE[Yn​]​=E[i=1∑n​Zn,i​(2⋅max(Yi−1​,Yn−i​))]=i=1∑n​E[Zn,i​(2⋅max(Yi−1​,Yn−i​))]=i=1∑n​E[Zn,i​]⋅E[2⋅max(Yi−1​,Yn−i​)]=i=1∑n​n1​⋅E[2⋅max(Yi−1​,Yn−i​)]=n2​i=1∑n​E[max(Yi−1​,Yn−i​)]≤n2​i=1∑n​(E[Yi−1​]+E[Yn−i​])​(期望的线性性质)(独立性)(③式)(期望的线性性质)(补充证明1)​\n​  因为 E[Y0]E[Y_{0}]E[Y0​] ，E[Y1]E[Y_{1}]E[Y1​]，......... ，E[Yn−1]E[Y_{n-1}]E[Yn−1​] 每一项各出现两次，一次作为 E[Yi−1]E[Y_{i-1}]E[Yi−1​] ，另一次为 E[Yn−1]E[Y_{n-1}]E[Yn−1​] 。\n​  所以上述公式可化为：\nE[Yn]≤4n∑i=0n−1E[Yi]E[Y_{n}] \\le\\frac{4}{n}\\sum_{i=0}^{n-1}E[Y_{i}]\nE[Yn​]≤n4​i=0∑n−1​E[Yi​]\n​  使用替换法，证明 E[Y(n)]=O(n3)E[Y(n)]=O(n^3)E[Y(n)]=O(n3) ：\n4n∑i=0n−1E[Yi]≤4n∑i=0n−1cn3≤4n⋅cn44=cn3\\begin{align*}\n\\frac{4}{n}\\sum_{i=0}^{n-1}E[Y_{i}] &amp;\\le \\frac{4}{n}\\sum_{i=0}^{n-1}cn^3\\\\\n&amp;\\le \\frac{4}{n} \\cdot \\frac{cn^4}{4}=cn^3\n\\end{align*}\nn4​i=0∑n−1​E[Yi​]​≤n4​i=0∑n−1​cn3≤n4​⋅4cn4​=cn3​\n​  所以，E[Y(n)]=O(n3)E[Y(n)]=O(n^3)E[Y(n)]=O(n3) ，即E[2X(n)]=O(n3)E[2^{X(n)}]=O(n^3)E[2X(n)]=O(n3)。\n​  由 JensenJensenJensen 不等式（补充证明2）：\n2E[X(n)]≤E[2X(n)]=O(n3)2^{E[X(n)]}\\le E[2^{X(n)}]=O(n^3)\n2E[X(n)]≤E[2X(n)]=O(n3)\n​  两边取对数：\nE[X(n)]=O(lgn)E[X(n)]=O(lgn)\nE[X(n)]=O(lgn)\n​ 综上所述，一棵有 nnn 个不同关键字的随机构建二叉搜索树的期望高度为 ==O(log n)O(log\\ n)O(log n)​== 。\n\n补充证明1：\n要证明 E[max⁡(X,Y)]≤E[X]+E[Y]E[\\max(X,Y)] \\leq E[X] + E[Y]E[max(X,Y)]≤E[X]+E[Y]，我们可以考虑分两种情况：\n\n当 X≤YX \\leq YX≤Y 时，有 max⁡(X,Y)=Y\\max(X,Y) = Ymax(X,Y)=Y。\n当 X&gt;YX &gt; YX&gt;Y 时，有 max⁡(X,Y)=X\\max(X,Y) = Xmax(X,Y)=X。\n\n因此，我们可以写出以下不等式：\nE[max⁡(X,Y)]=E[max⁡(X,Y)∣X≤Y]⋅P(X≤Y)+E[max⁡(X,Y)∣X&gt;Y]⋅P(X&gt;Y)≤E[Y]⋅P(X≤Y)+E[X]⋅P(X&gt;Y)=E[Y⋅1X≤Y]+E[X⋅1X&gt;Y]\\begin{align*}\nE[\\max(X,Y)] &amp;= E[\\max(X,Y) \\mid X \\leq Y] \\cdot P(X \\leq Y) + E[\\max(X,Y) \\mid X &gt; Y] \\cdot P(X &gt; Y) \\\\\n&amp;\\leq E[Y] \\cdot P(X \\leq Y) + E[X] \\cdot P(X &gt; Y) \\\\\n&amp;= E[Y \\cdot \\mathbb{1}_{X \\leq Y}] + E[X \\cdot \\mathbb{1}_{X &gt; Y}]\n\\end{align*}\nE[max(X,Y)]​=E[max(X,Y)∣X≤Y]⋅P(X≤Y)+E[max(X,Y)∣X&gt;Y]⋅P(X&gt;Y)≤E[Y]⋅P(X≤Y)+E[X]⋅P(X&gt;Y)=E[Y⋅1X≤Y​]+E[X⋅1X&gt;Y​]​\n其中，1X≤Y\\mathbb{1}_{X \\leq Y}1X≤Y​ 和 1X&gt;Y\\mathbb{1}_{X &gt; Y}1X&gt;Y​ 是指示函数。现在，因为指示函数的期望是概率本身，所以我们有：\nE[Y⋅1X≤Y]=P(X≤Y)⋅E[Y]≤P(X≤Y)⋅E[X]+P(X≤Y)⋅E[Y]E[X⋅1X&gt;Y]=P(X&gt;Y)⋅E[X]≤P(X&gt;Y)⋅E[X]+P(X&gt;Y)⋅E[Y]E[Y \\cdot \\mathbb{1}_{X \\leq Y}] = P(X \\leq Y) \\cdot E[Y] \\leq P(X \\leq Y) \\cdot E[X] + P(X \\leq Y) \\cdot E[Y] \\\\\nE[X \\cdot \\mathbb{1}_{X &gt; Y}] = P(X &gt; Y) \\cdot E[X] \\leq P(X &gt; Y) \\cdot E[X] + P(X &gt; Y) \\cdot E[Y]\nE[Y⋅1X≤Y​]=P(X≤Y)⋅E[Y]≤P(X≤Y)⋅E[X]+P(X≤Y)⋅E[Y]E[X⋅1X&gt;Y​]=P(X&gt;Y)⋅E[X]≤P(X&gt;Y)⋅E[X]+P(X&gt;Y)⋅E[Y]\n将上述两个不等式代入前面的不等式中，我们得到：\nE[max⁡(X,Y)]≤E[Y]⋅P(X≤Y)+E[X]⋅P(X&gt;Y)≤E[X]+E[Y]E[\\max(X,Y)] \\leq E[Y] \\cdot P(X \\leq Y) + E[X] \\cdot P(X &gt; Y) \\leq E[X] + E[Y]\nE[max(X,Y)]≤E[Y]⋅P(X≤Y)+E[X]⋅P(X&gt;Y)≤E[X]+E[Y]\n因此，E[max⁡(X,Y)]≤E[X]+E[Y]E[\\max(X,Y)] \\leq E[X] + E[Y]E[max(X,Y)]≤E[X]+E[Y]​​，证毕。\n\n\n补充证明2：\n假设 fff 是一个 convexconvexconvex 函数（国外是凸函数，国内是凹函数）。我们想要证明对于任意随机变量 XXX 和权重 wiw_iwi​，满足 ∑iwi=1\\sum_{i} w_i = 1∑i​wi​=1，有：\nE[f(X)]≥f(E[X])E[f(X)] \\geq f(E[X])\nE[f(X)]≥f(E[X])\n为了证明这一点，我们首先需要定义 convexconvexconvex 函数的性质。一个函数 fff 被称为 convexconvexconvex 函数，如果对于任意 x1,x2x_1, x_2x1​,x2​ 和 0≤λ≤10 \\leq \\lambda \\leq 10≤λ≤1，都有：\nf(λx1+(1−λ)x2)≤λf(x1)+(1−λ)f(x2)f(\\lambda x_1 + (1 - \\lambda) x_2) \\leq \\lambda f(x_1) + (1 - \\lambda) f(x_2)\nf(λx1​+(1−λ)x2​)≤λf(x1​)+(1−λ)f(x2​)\n现在，我们来证明 Jensen 不等式：\n根据 convexconvexconvex 函数的定义，我们有：\nf(λE[X]+(1−λ)E[X])≤λf(E[X])+(1−λ)f(E[X])f(\\lambda E[X] + (1 - \\lambda) E[X]) \\leq \\lambda f(E[X]) + (1 - \\lambda) f(E[X])\nf(λE[X]+(1−λ)E[X])≤λf(E[X])+(1−λ)f(E[X])\n⇒f(E[X])≤λf(E[X])+(1−λ)f(E[X])\\Rightarrow f(E[X]) \\leq \\lambda f(E[X]) + (1 - \\lambda) f(E[X])\n⇒f(E[X])≤λf(E[X])+(1−λ)f(E[X])\n⇒f(E[X])≤f(E[X])\\Rightarrow f(E[X]) \\leq f(E[X])\n⇒f(E[X])≤f(E[X])\n这是显然成立的。因此，我们得出：\nE[f(X)]≥f(E[X])E[f(X)] \\geq f(E[X])\nE[f(X)]≥f(E[X])\n这就证明了 Jensen 不等式。\n\n:sweat_smile: 红黑树 Red-Black Tree :fearful:\n\n🥵Q宝速速给我抄抄😋\n\n红黑树的性质\n\n每个节点要么红，要么黑\n根节点永远为黑\n叶节点 (NIL)(NIL)(NIL) 为黑\n不会有两个相连的红节点（一个节点是红的，那么俩子节点是黑的）\n对每个节点，从该节点到其所有后代叶节点的简单路径上，均包含相同数目的黑色节点（即黑高 bh(x)bh(x)bh(x) 相同）\n(*)每一棵红黑树都对应着一棵 2−3−42-3-42−3−4 树（或 2−32-32−3 树）\n\n\n其最明显的优势：一棵有 nnn 个节点的红黑树的高度至多为 ==2log(n+1)2log( n+1)2log(n+1)==​ 。\n\n证明如下：\n​ 设 xxx 为一棵红黑树的根节点，bh(x)bh(x)bh(x)​ 为一棵红黑树某个节点的黑高（即一条简单路径上黑色节点的个数），hhh 为一棵红黑树的高度。\n​ 我们先证明以 xxx 为根的子树中至少包含 2bh(x)−12^{bh(x)}-12bh(x)−1 个内部节点\n​ 要证明此点，我们以数学归纳法进行证明：\n​  1. 当 x.depth=0x.depth=0x.depth=0 ，即 xxx 为根节点，此时子树中的节点满足 2bh(x)−1=20−1=02^{bh(x)}-1=2^{0}-1=02bh(x)−1=20−1=0​ 。\n​  2. 当 x.depth=kx.depth=kx.depth=k ，假设该命题成立，即以 xxx 为根的子树中至少包含 2bh(xk)−12^{bh(x_{k})}-12bh(xk​)−1 个内部节点，此时的黑高为 bh(xk)bh(x_{k})bh(xk​) 或 bh(xk)−1bh(x_{k})-1bh(xk​)−1 ，这取决于 x.colorx.colorx.color 是黑还是红。\n​  **3.**则当 x.depth=k+1x.depth=k+1x.depth=k+1 ，即 xxx​ 下一节点时，因为同一节点左右子树的黑高一致，所以有：\nbh(xk+1)=bh(xk)−1bh(x_{k+1})=bh(x_{k})-1\nbh(xk+1​)=bh(xk​)−1\n​   此时 x.depth=kx.depth=kx.depth=k 的左右子树节点数至少为：2bh(xk+1)−1=2bh(xk)−1−12^{bh(x_{k+1})}-1=2^{bh(x_{k})-1}-12bh(xk+1​)−1=2bh(xk​)−1−1 。\n​   所以内部总节点数至少为：(2bh(xk)−1−1)+(2bh(xk)−1−1)+1=2bh(xk)−1(2^{bh(x_{k})-1}-1)+(2^{bh(x_{k})-1}-1)+1=2^{bh(x_{k})}-1(2bh(xk​)−1−1)+(2bh(xk​)−1−1)+1=2bh(xk​)−1 。\n​   （这里只加一次 111 是因为考虑到了左右子树可能存在 NILNILNIL ）\n​  综上所述，以 xxx 为根的子树中至少包含 ==2bh(x)−12^{bh(x)}-12bh(x)−1== 个内部节点。\n​ 又因为根节点到叶节点至少有一半是黑节点，所以黑高至少为 ⌈h/2⌉\\left\\lceil h/2\\right\\rceil⌈h/2⌉ ，于是有：\nn≥2⌈h/2⌉−1n \\ge 2^{\\left\\lceil h/2\\right\\rceil}-1\nn≥2⌈h/2⌉−1\n​ 两边取对：\nlog(n+1)≥⌈h/2⌉log(n+1) \\ge \\left\\lceil h/2\\right\\rceil\nlog(n+1)≥⌈h/2⌉\n​ 即：\nh≤2log(n+1)h\\le 2log(n+1)\nh≤2log(n+1)\n红黑树的旋转\n在 CS61BCS61BCS61B​ 上，旋转的正式定义为：\n\nrotateLeft(G) : Let x be the right child of G. Make G the new left child of x.\n\n左旋：设 xxx 为 GGG 的右子结点，让 GGG 成为 xxx 的新左子结点。\n\n\nrotateRight(G): Let x be the left child of G. Make G the new right child of x.\n\n右旋：设 xxx 为 GGG 的左子结点，让 GGG 成为 xxx 的新右子结点。\n\n\n\n\n上面发生的事情的书面描述是这样的：\n\nGGG 的右子项 PPP 与 GGG 合并，并带来了它的孩子。\n然后 PPP 将其左子项传递给 GGG，GGG 向左下降成为 PPP 的左子项。\n\n可以看到树的结构及其高度发生了变化。我们也可以在非根节点上轮换。我们只是暂时断开节点与父节点的连接，旋转节点上的子树，然后重新连接新的根。\n代码实现\n/* 伪代码-左旋 */LEIT-ROTATE(T,x)  y = x.right  x.right = y.left  if y.left ≠ T.nil  y.left.p = x y.p = x.p  if x.p == T.nil /* 根节点 */  T.root=y  else if x == x.p.left   x.p.left = y  else x.p.right = y  y.left= x  X.p = y\n/* cs61b-java版 *//* 建议结合图来分析代码 */// make a right-leaning link lean to the leftprivate Node rotateLeft(Node h) &#123;    // assert (h != null) &amp;&amp; isRed(h.right);    Node x = h.right;     //将x设为h的右节点    h.right = x.left;     //将h的右节点设为x的左节点(即h.left.right)    x.left = h;           //将x的左节点设为h    return x;             //返回旋转后的根节点&#125;private Node rotateRight(Node h) &#123;    // assert (h != null) &amp;&amp; isRed(h.left);    Node x = h.left;    h.left = x.right;    x.right = h;    return x;&#125;\n/* c++版 */TreeNode *rotateLeft(TreeNode *h)&#123;    TreeNode *child = h-&gt;right;    h-&gt;right = child-&gt;left;    child-&gt;left = h;    return x;&#125;TreeNode *rotateRight(TreeNode *h)&#123;    TreeNode *child = h-&gt;left;    h-&gt;left = child-&gt;right;    child-&gt;right = h;    return x;&#125;\n复杂度分析\n\n\n时间复杂度：O(1)O(1)O(1)\n\n\n空间复杂度：O(1)O(1)O(1)\n\n\n红黑树的插入\n插入操作与 BSTBSTBST​ 基本一致。只是将插入节点染成红色，接着调用 RBT−Insert−FixupRBT-Insert-FixupRBT−Insert−Fixup 函数，修复红黑树的性质。\n代码实现\nRB-INSERT(T,z) y = T.nil x = T.root while x != T.nil   y = x   if z. key &lt; x. key    x = x.left   else   x = x.right  z.p = y if y == T.nil   T.root= z  elseif z.key &lt; y.key   y.left = z  else  y.right = z /* 以上是BST-Insert操作 */ z.left = T.nil z.right = T.nil z.color = RED RB-INSERT-FIXUP(T,z)\nfixupfixupfixup 原理分析\n接下来我们来分析一下 FixupFixupFixup​ 的原理：\n​ 执行 InsertInsertInsert 操作后，针对 RBTRBTRBT 的五个性质，我们不难发现：\n​  性质 111 和性质 333 以及性质 555 依然成立\n​  可能被破坏的是\n​   性质 222 （根节点为黑色）\n​   性质 444 （不会有两个相连的红节点）\n​   并且这两个性质至多只有一条被破坏。\n​ 因此我们从修复性质 444 入手，确定循环终止条件，即 zzz 的父节点是黑色时终止。\n​ 接下来我们以 z.pz.pz.p 是左孩子为例，右孩子的代码直接所有方向取反。\n​ 此时，有如下 333​ 种情况：\n​  case1case1case1：zzz 的叔节点 yyy 是红色的\n​   该情况在 zzz 和 z.pz.pz.p 都是红色时发生。因为 z.p.pz.p.pz.p.p 是黑色的，所以我们把 z.pz.pz.p 和 yyy 染成黑色，把 z.p.pz.p.pz.p.p 染成红色以保持性质 555 。然后我们以 z.p.pz.p.pz.p.p 作为新的 zzz 节点来重复循环（即 zzz 上移两个节点）。\n\n​  case2case2case2：zzz 的叔结点 yyy 是黑色的且 zzz 是一个右孩子（ zzz 和其父节点方向相反）\n​  case3case3case3：zzz 的叔结点 yyy 是黑色的且 zzz 是一个左孩子（ zzz 和其父节点方向一致）\n​   我们将情况 222 和 333​ 合并在一起来看。\n​   对于情况 222 ，我们立即对 z.pz.pz.p 执行一次左旋，将其转变为情况 333 。\n​   对于情况 333 ，我们能肯定的是，z.p.pz.p.pz.p.p 必定为黑色。为了修复性质，我们交换 z.pz.pz.p 和z.p.pz.p.pz.p.p 的颜色，并对 z.p.pz.p.pz.p.p 执行一次右旋。此时，z.pz.pz.p 的颜色是黑色，循环终止，修复成功。\n\n\nfixupfixupfixup 代码\nRB-INSERT-FIXUP(T,z)  while z.p.color == RED   if z.p == z.p.p.left    y = z.p.p.right /* y是z的叔节点 */   /* case1 */   if y.color == RED     z.p.color = BLACK     y.color = BLACK     z.p.p.color = RED     z = z.p.p    else    /* case2 */    if z == z. p. right      z = z.p      LEFT-ROTATE(T,z)    /* case3 */    z.p.color = BLACK     z.p.p.color = RED     RIGHT-ROTATE(T,z.p.p)  else(same as then clause with &quot;right&quot; and &quot;left&quot; exchanged)   ...... T.root.color= BLACK\n复杂度分析\n\n时间复杂度：O(log n)O(log\\ n)O(log n)​\n\n树高为 O(log n)O(log\\ n)O(log n) ，则插入操作需要 O(log n)O(log\\ n)O(log n) 的时间\n调用 FixupFixupFixup 时，仅当情况 111 发生时，zzz 才会沿树上升两层，此时需要O(log n)O(log\\ n)O(log n) 的时间，其他情况都是 O(1)O(1)O(1) 。\n因此是 O(log n)O(log\\ n)O(log n)\n\n\n空间复杂度：O(1)O(1)O(1)\n\n红黑树的删除\n删除操作跟 BSTBSTBST 的删除操作类似，只是多了一个记录节点 xxx 以及在最后添加了一个 RBT−Delete−FixupRBT-Delete-FixupRBT−Delete−Fixup 的操作\n代码实现\n/* 参考BST的删除 *//* 用以v为根的子树替换以u为根的子树 */RB-TRANSPLANT(T, u, v)  if u.p == T.nil  /* 若u为根节点 */  T.root=v  else if u == u.p.left  /* 若u是其父节点的左孩子 */  u.p.left = v   else  /* 若u是其父节点的右孩子 */  u.p.right = v  v.p = u.p\n/* 参考BST的删除 *//* attention:第9、13、24行传递的第三个参数与x相同，x只是引用作用，不参与实际删除操作，只是给后续的fixup操作提供参数 */RB-DELETE(T, z)  y = z  y-original-color = y.color  /* 若z的左树为空 */ if z.left == T.nil   x = z.right   RB-TRANSPLANT(T, z, z.right)  /* 用z的右孩子代替z */ /* 若z的右树为空 */ else if z.right == T.nil   x = z.left   RB-TRANSPLANT(T, z, z.left)   /* 用z的左孩子代替z */ /* 若z有两个孩子 */ else   y = TREE-MINIMUM(z.right)     /* y是z的后继 */  y-original-color = y.color   x = y.right  /* 若y是z的孩子 */  if y.p == z    x.p = y  /* 若y不是z的孩子 */  else   RB-TRANSPLANT(T, y, y.right)  /* 用y的右孩子代替y */   y.right = z.right             /* 令z的右孩子 */   y.right.p = y                 /* 成为y的右孩子 */  RB-TRANSPLANT(T, z, y)        /* 用后继y代替z */  y.left = z.left               /* 将z的左孩子给 */  y.left.p = y                  /* (肯定)没有左孩子的y */  y.color = z.color if y-original-color == BLACK      /* y是红的不用管 不会影响红黑树的性质 */  RB-DELETE-FIXUP(T, x)\nfixupfixupfixup 原理分析\n下面重点讲 FixupFixupFixup 的原理：\n​ 记录节点 xxx ，其为我们替换 yyy 所需的节点。\n​ 因为其引用的节点替换 yyy 后可能会违反红黑树的性质，所以我们要对 xxx 及其子树向上进行 FixupFixupFixup 操作。\n​ 如果 yyy 是黑色的话，会产生三个问题：\n​  1. 若 yyy 是原先的根节点，而其一个红色的孩子代替了 yyy ，则会违反==性质2==，即根节点不能为红色。\n​  2. 若 xxx 和 x.px.px.p 是红色的话，则违反了==性质4==，即两个红色节点不能相连。\n​  **3.**若移动 yyy 后导致之前包含 yyy 的简单路径的 bh(x)bh(x)bh(x) 少了 111 ，则 yyy 的任何祖先都不满足性质5，即 bh(x)bh(x)bh(x) 相等。\n​   解决这一问题的方法就是将现在占有 yyy 原先位置的 xxx 视作还有额外一层黑色，即黑黑色或红黑色。\n​   （但 x.colorx.colorx.color​ 不变，只是视作有两种颜色，这里则变为违反==性质1==，但是后面会进行修复）\n​ 所以问题转变为 FixupFixupFixup 操作需要修复性质 1、2、41、2、41、2、4 。\n​ whilewhilewhile 循环的目标就是要把额外的黑色沿树上移，直到：\n​  1. xxx​​ 指向红黑节点，此时将其染成单个黑色\n​  2. xxx 指向根节点，此时可以简单地&quot;移除&quot;额外的黑色\n​  3. 执行适当的旋转及重新染色，退出循环\n​ 在循环里， xxx 是黑黑色的~~（如果是红黑色，那就直接染成黑色了，瞎进什么循环）~~，下面设 xxx 是其父节点左孩子， www 是 xxx 的兄弟节点。\n则会有如下几种情况：\n​ case1case1case1：xxx 的兄弟节点 www 是红色的  兄弟兄弟，你怎么是红色的(疾旋鼬.jpg)\n​  因为 www 是红色，其必有俩黑色子节点，所以可以改变 www 以及 w.pw.pw.p 的颜色，并做一次旋转。\n​  这样就将 case1case1case1 转换为接下来的三种情况处理。\n\n​ case2case2case2：xxx 的兄弟节点 www 是黑色的且 www​ 的两个孩子都是黑色\n​  因为 www 是黑色的，所以从 xxx 和 www 上各去掉一层黑色，即 www 变为红色，xxx 上移。为补偿删掉的一层黑色，将新的 xxx 加一层黑色，继续循环，直到满足条件。\n​  若从 case1case1case1 过来的，此时新的 xxx 是红黑色的，满足条件，直接将新的 xxx​ 染成黑色，结束循环，修复完成。\n\n​ case3case3case3：xxx 的兄弟节点 www 是黑色的且 www​ 的右节点是黑色，左节点是红色\n​  此情况需要转换为 case4case4case4 来解决，所以我们先交换 www 和 w.leftw.leftw.left 的颜色，然后对 www 进行右旋。\n​ case4case4case4：xxx 的兄弟节点 www 是黑色的且 www 的右节点是红色\n​  此情况下，将 w.rightw.rightw.right 变为黑色，x.px.px.p 与 www 交换颜色，并对 x.px.px.p 进行一次左旋，消除 xxx 的额外黑色，最后将 xxx 设为根节点，跳出循环。（ xxx 的那一层黑色给到了原来 www​ 的右节点）\n\nfixupfixupfixup 代码\nRB-DELETE-FIXUP(T, x)  while x != T. root and x.color == BLACK   if x == x.p.left    w = x.p.right    /* case1 */   if w.color == RED     w.color = BLACK     x.p.color = RED     LEFT-ROTATE(T, x.p)     w = x.p.right    /* case2 */   if w.left.color == BLACK and w.right.color == BLACK     w.color = RED     x = x.p   else    /* case3 */   if w.right.color == BLACK     w.left.color = BLACK     w.color = RED     RIGHT-ROTATE(T,w)     w = x.p.right   /* case4- */   w.color = x.p.color    x.p.color = BLACK    w.right.color = BLACK    LEFT-ROTATE(T, x.p)    x = T.root   else (same as then clause with &quot;right&quot; and &quot;left&quot; exchanged)    ...... x.color = BLACK\n复杂度分析\n\n\n时间复杂度：O(log n)O(log\\ n)O(log n) ​\n\n不调用 FixupFixupFixup 时需要 O(log n)O(log\\ n)O(log n) ，因为树高是 O(log n)O(log\\ n)O(log n)\n而调用FixupFixupFixup ，case1、3、4case1、3、4case1、3、4 进行常数次数操作，case2case2case2 至多循环到根节点，也即树高 O(log n)O(log\\ n)O(log n) ​\n因此是 O(log n)O(log\\ n)O(log n)\n\n\n\n空间复杂度：O(1)O(1)O(1)\n\n\nB树 B-tree\n\n未完工喵~:yum:\nshmmshmmshmm 速速🤺\n\n","categories":["DataStru&Algo"],"tags":["Data Structure","Algorithm","SCNU Turing Discussion","BST","RBT"]},{"title":"Zero-Shot Dense Retrieval with Embeddings from Relevance Feedback","url":"/paper/Zero-Shot-Dense-Retrieval-with-Embeddings-from-Relevance-Feedback/","content":"问题 (Problem)\n在信息检索领域，密集检索（Dense Retrieval）系统通常需要大量的标注数据（即“查询-相关文档”对）进行训练才能达到良好效果。但在许多场景下，这种标注数据是稀缺或不存在的。因此，如何在没有标注数据的情况下（即零样本 Zero-Shot 场景）构建高效的密集检索系统，是一个核心挑战。\n现有的SOTA（State-of-the-art）方法，如HyDE，尝试使用大语言模型（LLM）来解决这个问题。它的思路是：针对一个用户查询，让LLM生成一篇“假想的”（hypothetical）相关文档，然后用这篇假想文档的向量去寻找语料库中内容最相似的真实文档。\n然而，这种依赖LLM生成假想文档的方法存在三个主要缺陷：\n\n知识局限性：该方法严重依赖LLM自身的参数化知识。如果查询涉及特定或专有领域（如公司内部文档），LLM可能无法生成高质量、有事实依据的假想文档。\n效率低下：对于每个查询，LLM都需要生成一篇完整的文档（包含大量token），这个生成过程非常耗时，导致检索延迟很高。\n内容不可靠：即使给LLM提供一些参考文档作为上下文，它在生成内容时也可能出现幻觉、忽略或曲解已有信息等问题。\n\n因此，本文要解决的问题是：如何在零样本场景下，构建一个既有效又高效的密集检索系统，同时避免上述依赖LLM生成完整文档所带来的问题？\n\n方法 (Method)\n本文提出了一种名为 ReDE-RF (Real Document Embeddings from Relevance Feedback) 的新方法。其核心思想非常巧妙：不再让LLM“创作”内容，而是让它扮演“裁判”的角色。作者将任务从“生成假想文档”重新定义为“评估已有文档的相关性”，并利用这个相关性反馈来优化查询。\nReDE-RF的完整流程如下图所示，可以分为四个主要步骤：\n\n\n图例解读：上图清晰地展示了整个流程。一个关于“史上最伟大的NBA球员”的查询，首先通过一个基础的混合检索系统（BM25 + Contriever）召回初步的文档列表（Doc 1, Doc 2, Doc 3…）。然后，ReDE-RF模块中的LLM逐一判断这些文档是否相关。例如，LLM判断Doc 1不相关，而Doc 2和Doc 3相关。接着，系统只使用被判断为“相关”的文档（Doc 2, Doc 3）的向量来更新原始查询向量。最后，用这个更新后的、更精确的查询向量去Contriever索引中进行最终的相似度搜索，得到排序更优的最终结果列表（如“勒布朗·詹姆斯”、“迈克尔·乔丹”、“科比·布莱恩特”等）。\n\n下面是每个步骤的详细技术解读：\n1. 初步检索 (Initial Retrieval)\n对于给定的用户查询q，首先使用一个完全无监督的混合检索系统（如传统的BM25稀疏检索 + Contriever稠密检索）从大规模文档库中召回一个初步的候选文档集D（例如，Top-20的文档）。这一步的目的是快速筛选出一批可能相关的文档，为后续LLM的精选提供素材。\n2. LLM相关性反馈 (Relevance Feedback with LLMs)\n这是ReDE-RF的核心。系统会遍历上一步召回的每一个候选文档d_i∈Dd\\_i \\in Dd_i∈D，并使用一个特定的提示（Prompt）去请求LLM（被称为LLMRel−JudgeLLM_{Rel-Judge}LLMRel−Judge​）判断该文档与原始查询q的相关性。这个提示要求LLM只输出一个token：“1”代表相关，“0”代表不相关。\n这个过程极大地提升了效率，因为LLM不再需要生成长篇大论，只需进行一个简单的分类判断。最终，LLM会筛选出一个被认为是相关的文档子集 Dr=dr1,dr2,...,drk∗D_r = {d_{r_1}, d_{r_2}, ..., d_{r_{k^*}}}Dr​=dr1​​,dr2​​,...,drk∗​​，其中 k∗k^*k∗ 是相关文档的数量。\n3. 更新查询表示 (Updating the Query Representation)\n在获得相关文档集 DrD_rDr​ 后，ReDE-RF会用这些真实文档的向量来优化原始的查询向量。这一步的数学公式如下：\nv^qReDE=1k∗+1(f(q)+∑i=1k∗CE[dri])\\hat{v}_{q_{ReDE}} = \\frac{1}{k^*+1} \\left( f(q) + \\sum_{i=1}^{k^*} C_E[d_{r_i}] \\right)\nv^qReDE​​=k∗+11​(f(q)+i=1∑k∗​CE​[dri​​])\n\n公式解读：\n\nf(q)f(q)f(q) 是原始查询q经过Contriever编码器得到的初始查询向量。\ndrid_{r_i}dri​​ 是被LLM判断为相关的第i个真实文档。\nCE[dri]C_E[d_{r_i}]CE​[dri​​] 表示从预先计算好的文档向量索引库中，直接提取文档 drid_{r_i}dri​​ 的向量。这是一个非常高效的操作，因为所有文档的向量在建索引时已经离线计算好了。\n整个公式的含义是：将原始查询向量与所有被LLM认证为相关的真实文档向量进行平均，从而生成一个全新的、信息更丰富、更贴近语料库内容分布的查询向量 v^qReDE\\hat{v}_{q_{ReDE}}v^qReDE​​。\n\n\n\n4. 最终检索 (Final Retrieval)\n使用这个优化后的查询向量 v^qReDE\\hat{v}_{q_{ReDE}}v^qReDE​​，在Contriever的文档向量索引中进行最终的ANN（近似最近邻）搜索，得到最终的排序结果。\n特殊情况处理：如果在初步检索的文档中，LLM没有找到任何相关的文档（即 DrD_rDr​ 为空集），ReDE-RF提供了两种备选（Default）策略：\n\n退回至Contriever：直接使用原始查询向量 f(q)f(q)f(q) 进行检索。\n退回至HyDEPRFHyDE_{PRF}HyDEPRF​：在这种困难情况下，才启用计算成本较高的HyDE方法（使用初步召回的文档作为上下文）来生成假想文档进行检索。\n\n方法蒸馏 (DistillReDE)\n为了进一步解决LLM在推理时带来的延迟问题，论文还提出了一种名为DistillReDE的蒸馏方法。其目标是将ReDE-RF的强大性能“蒸馏”到一个更小的、无需LLM参与的Contriever模型中。具体做法是：\n\n离线使用大量合成查询，并通过完整的ReDE-RF流程计算出最优的查询向量。\n将这些最优向量作为“教师”信号，来微调一个标准的Contriever查询编码器。\n优势在于，这个过程只更新查询编码器，无需重新为整个语料库建立文档向量索引，大大节约了成本。\n\n\nBaseline\n为了全面评估ReDE-RF的性能，论文选取了三类基线模型进行对比：\n\n\n无监督检索模型 (不使用LLM)\n\nBM25: 经典的稀疏检索模型。\nContriever: SOTA的无监督稠密检索模型。\nHybrid (BM25 + Contriever): 结合稀疏和稠密的混合模型。\nContriever AvgPRF: 一种伪相关性反馈方法，它不经过LLM筛选，直接将初步召回的所有文档向量进行平均来更新查询。\n\n\n\n零样本密集检索模型 (使用LLM且无需训练)\n\nHyDE: 核心对比对象，使用LLM生成假想文档。\nHyDEPRFHyDE_{PRF}HyDEPRF​: HyDE的变种，在生成假想文档时，将初步召回的文档作为上下文。\nPromptReps: 另一种利用LLM进行零样本检索的方法，它通过提示LLM生成一个描述性token来代表查询和文档。\n\n\n\n有监督密集检索模型 (使用标注数据进行微调)\n\nDPR, ANCE, ContrieverFT: 这些是经典的、在大量标注数据上训练过的模型，用于展示零样本方法与有监督方法之间的性能差距。\n\n\n\n\n数据集 (Datasets)\n实验在两大类共9个公开数据集上进行，覆盖了不同领域和资源量。评价指标统一使用 NDCG@10。\n\n\n高资源网页搜索数据集:\n\nTREC DL19 和 TREC DL20: 来自TREC深度学习赛道的标准测试集。\n\n\n\n低资源BEIR基准数据集:\n\n新闻检索: TREC-News, Robust04\n金融问答: FiQA\n实体检索: DBpedia\n生物医学: TREC-Covid, NFCorpus\n事实核查: SciFact\n\n\n\n\n可复现性 (Reproducibility)\n论文提供了非常详细的实现细节，可复现性较高。\n\n代码和框架: 基于公开的 Pyserini 和 HuggingFace Transformers 库实现。\n模型:\n\n核心LLM (LLMRel−JudgeLLM_{Rel-Judge}LLMRel−Judge​) 使用的是 Mistral-7B-Instruct-v0.2。\n稠密检索器使用的是 facebook/contriever。\n论文还在消融实验中测试了Mixtral、Gemma、Llama等多种不同大小的开源LLM。\n\n\n算力: 实验在单张A100 GPU上完成。论文还详细报告了不同方法的平均查询延迟，这对于评估实际部署的可行性至关重要。\n超参数和提示: 论文附录中给出了复现HyDE和ReDE-RF所使用的全部提示语（Prompts），以及蒸馏过程的训练参数（如学习率、批大小等），为复现工作提供了便利。\n\n\n可改进的几个点 (Potential Improvements)\n论文在局限性（Limitations）部分诚实地指出了几个未来可以改进的方向：\n\n对初步检索的依赖：ReDE-RF的性能上限受限于初步检索的结果。如果第一阶段召回的文档质量很差，完全不包含相关文档，那么LLM也无能为力。未来的工作可以研究如何使模型对第一阶段的检索结果不那么敏感，或者设计一种动态机制，当顶层结果不佳时自动扩大检索范围。\n对LLM判断准确性的依赖：虽然LLM做判断题比做问答题简单，但它仍然可能出错。一个错误的“相关”判断可能会引入噪声，污染查询向量。如何设计更鲁棒的机制来处理或过滤LLM的潜在错误判断，是一个值得探索的方向。\n延迟问题：尽管ReDE-RF比HyDE快得多，但在推理时引入LLM仍然比纯粹的稠密检索要慢。论文提出的蒸馏方法DistillReDE是一个很好的离线解决方案。对于在线服务，可以进一步探索使用更小、更专用的模型（例如，专门为相关性判断任务微调的小模型）来替代通用的7B大模型，以实现性能和速度的极致平衡。\n与重排序（Reranking）的结合：实验表明，ReDE-RF（优化召回）和LLM重排序（优化排序）是互补的。可以设计一个更一体化的框架，将两者有机结合，而不是作为两个独立的串联步骤，可能会带来更大的提升。\n\n\n可以被引用的一些结论 (Citable Conclusions)\n这篇论文提供了多个有价值的、可供引用的发现和结论：\n\n“判断”优于“生成”：对于零样本密集检索任务，将LLM的角色从“内容生成者”转变为“相关性裁判”，不仅在低资源领域显著提升了检索效果（最高提升6%-14%），而且大幅降低了查询延迟（最高提速11倍）。\n真实文档向量更可靠：使用语料库中真实文档的向量来增强查询，可以使查询表示更“接地气”，更好地适应特定领域的知识和语言风格，避免了生成式模型可能带来的领域知识缺乏或内容幻觉问题。\nLLM相关性反馈的有效性：简单地将初步召回的文档向量全部平均（AvgPRF）并不能提升性能，这证明了使用LLM进行精准筛选，剔除不相关文档是ReDE-RF成功的关键。\n性能可被有效蒸馏：ReDE-RF的性能增益可以被成功地“蒸馏”到一个标准的稠密检索模型中，从而在不牺牲太多性能的前提下，完全移除推理阶段对LLM的依赖，实现高效部署。\n召回优化与排序优化的互补性：ReDE-RF旨在提升第一阶段召回的候选项质量（能找到更多好结果），而LLM重排序则专注于优化排序（把最好的结果排到最前面）。两者功能不同但目标一致，结合使用效果更佳。\n提示工程的重要性：在设计用于相关性判断的提示时，明确地给出“相关性”的定义（例如，“文档是否能直接回答查询”）比模糊的提问能带来更好的性能。\n\n","categories":["paper"],"tags":["paper","zero-shot","Dense Retrieval","Retrieval","Embedding"]}]