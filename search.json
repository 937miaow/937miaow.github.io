[{"title":"ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction","url":"/paper/ColBERTv2-Effective-and-Efficient-Retrieval-via-Lightweight-Late-Interaction/","content":"问题 (Problem)\n传统的神经信息检索（Neural IR）模型主要分为两类，各自有明显的优缺点：\n\n\n单向量（Single-Vector）模型:\n\n工作方式: 将查询（query）和文档（document）分别编码成一个高维向量，然后通过计算这两个向量的点积来评估相关性。\n优点: 存储开销小，检索速度快。\n缺点: 表达能力有限且脆弱。模型需要将查询和文档的所有复杂语义关系压缩到一个单一的向量点积中，这对编码器提出了极高的要求。\n\n\n\n晚期交互（Late Interaction）模型 (如 ColBERTv1):\n\n工作方式: 将查询和文档的每个词元（token）都编码成一个向量，形成多向量表示。相关性计算分解为词元级别的计算，例如计算每个查询词元向量与所有文档词元向量的最大相似度之和 [: 20]。\n优点: 效果好，表达能力强，因为它将复杂的匹配任务交给了交互机制，减轻了编码器的负担。\n缺点: 空间占用巨大。由于需要为每个词元存储一个向量，其索引大小比单向量模型大一个数量级，这在网络规模的语料库上是难以接受的。\n\n\n\n核心问题: 如何在保持晚期交互模型强大效果（Effectiveness）的同时，解决其巨大的空间占用问题，提升其效率（Efficiency）？\n\n方法 (Method)\nColBERTv2 通过两大创新来解决上述问题：**降噪监督（Denoised Supervision）**用于提升模型质量，**残差压缩（Residual Compression）**用于减小空间占用。\n模型架构 (Modeling)\nColBERTv2 沿用了 ColBERT 的晚期交互架构。\n\n\n\n图例解读 (Figure 1)\n\n编码 (Encoding): 查询（Question）和文档（Passage）分别通过独立的BERT编码器（Question Encoder, Passage Encoder）。编码器的输出是每个词元（token）的上下文向量表示。\n交互 (Interaction): 交互过程是“晚期”的，即在编码之后发生。对于查询中的每一个词元向量，都会计算它与文档中所有词元向量的相似度，并取其中的最大值（MaxSim操作）。\n评分 (Scoring): 最终的查询-文档相关性分数是所有查询词元的 MaxSim 分数之和（Σ 操作）。\n\n\n\n数学公式解读\n模型的核心评分公式如下:\nSq,d=∑i=1Nmaxj=1MQi⋅DjTS_{q,d}=\\sum_{i=1}^{N}max_{j=1}^{M}Q_{i}\\cdot D_{j}^{T}\nSq,d​=i=1∑N​maxj=1M​Qi​⋅DjT​\n其中：\n\nQQQ 是一个 N×dN \\times dN×d 的矩阵，代表查询 qqq 的 NNN 个词元向量。\nDDD 是一个 M×dM \\times dM×d 的矩阵，代表文档 ddd 的 MMM 个词元向量。\nQiQ_iQi​ 是查询的第 iii 个词元向量。\nmaxj=1MQi⋅DjTmax_{j=1}^{M}Q_{i}\\cdot D_{j}^{T}maxj=1M​Qi​⋅DjT​ 计算的是查询词元 QiQ_iQi​ 与文档中所有词元向量的最大余弦相似度。\n∑i=1N\\sum_{i=1}^{N}∑i=1N​ 将每个查询词元找到的最佳匹配分数相加，得到最终总分。\n\n\n\n降噪监督 (Denoised Supervision)\n为了提升模型质量，超越那些经过高度优化的单向量模型，ColBERTv2 采用了一种更强的监督策略。\n\n生成高质量标签: 使用一个初步训练好的 ColBERT 模型从训练集中为每个查询检索 top-k 个候选段落。\n教师模型重排: 将这些（查询，段落）对送入一个更强大但更慢的交叉编码器（cross-encoder）教师模型进行重排序，得到更精确的相关性分数。\n知识蒸馏: 使用 KL 散度损失函数，将交叉编码器的分数蒸馏到 ColBERTv2 的架构中。这使得 ColBERTv2 能学习到教师模型的精细判断能力。\n难负例挖掘: 训练过程中使用的负例是经过初步模型和教师模型筛选后的高分负例（hard negatives），这使得训练更具挑战性也更有效。\n\n残差压缩 (Residual Compression)\n这是解决空间占用的核心技术。其基本假设是：特定词元在不同上下文中的向量表示会聚集在语义空间的特定区域内。\n\n聚类中心: 首先，对样本向量进行 k-means 聚类，得到一组聚类中心（centroids）CCC。\n编码: 对于语料库中的任意一个词元向量 vvv，它的压缩表示由两部分组成：\n\n离它最近的聚类中心的索引 ttt。\n残差向量 r=v−Ctr = v - C_tr=v−Ct​ 的量化版本 r~\\tilde{r}r~。残差的每个维度被量化为1或2个比特。\n\n\n解码: 在检索时，通过 v~=Ct+r~\\tilde{v} = C_t + \\tilde{r}v~=Ct​+r~ 来近似重构原始向量。\n\n对于 b−bitb-bitb−bit 编码的 nnn 维向量，每个向量需要 $\\left \\lceil log|C| \\right \\rceil + bn $ bits的空间。\n\n\n\n通过这种方式，原本需要256字节（128维，16位精度）存储的向量，现在只需要20或36字节（4字节索引 + 16/32字节量化残差），实现了6-10倍的空间压缩。\n\nBaseline (基线模型)\n论文与多种先进的检索模型进行了比较，可以分为几类：\n\n单向量模型:\n\n无蒸馏: RepBERT, DPR, ANCE。\n有蒸馏/特殊预训练: TAS-B, PAIR, coCondenser, RocketQAv2。\n\n\n稀疏/词汇模型: SPLADEv2，这是一个非常强的基线，它也将相关性计算分解到词元级别，但生成的是稀疏的词汇权重向量。\n晚期交互模型: ColBERT (v1)。\n传统模型: BM25。\n\n\n数据集 (Datasets)\n论文在多种数据集上进行了广泛的评估，以验证模型的性能和泛化能力。\n\n\n训练:\n\nMS MARCO Passage Ranking: 业界标准的 IR 训练和评测数据集。\n\n\n\n域内（In-Domain）评测:\n\nMS MARCO Dev Set: 在训练数据所在的领域进行评测。\n\n\n\n域外（Out-of-Domain）评测:\n\nBEIR: 一个包含13个不同领域（如生物医学、金融、科学）检索任务的异构评测基准，用于测试模型的零样本（zero-shot）泛化能力。\nWikipedia Open-QA: 包括 Natural Questions (NQ), TriviaQA (TQ), SQUAD 数据集，用于评测开放域问答的段落检索任务。\nLoTTE (论文新贡献): 全称 Long-Tail Topic-stratified Evaluation，是一个专注于长尾主题和自然用户查询的新评测基准。它源于 StackExchange 社区，包含 “Search” (来自Google搜索) 和&quot;Forum&quot; (来自帖子标题) 两种查询类型，覆盖写作、科技、生活等5大领域。\n\n\n\n\n可复现性 (Reproducibility)\n\n代码/模型: 论文明确指出代码、模型和 LoTTE 数据集都在 GitHub 上开源维护，这为复现工作提供了极大的便利。\n算力:\n\n训练: 需要4块80GB的A100 GPU，训练约5天。\n推理: 使用4块12GB的Titan V GPU。\n总计: 作者估计整个项目（包括开发、实验、评估）大约消耗了20个GPU月。\n结论: 复现单个模型的训练对于有高端GPU资源的实验室或公司是可行的，但完成所有实验的成本较高。\n\n\n\n\n可改进的点 (Potential Improvements)\n论文在结尾处也探讨了未来可能的研究方向：\n\n多语言支持: 目前所有实验均在英文上进行。将该方法扩展并验证到其他语言是一个重要的方向。\n训练数据依赖: 所有域外评测的模型都是在 MS MARCO 上训练的。在其他（尤其是较小的）数据集上训练时的表现有待探索。\n更优的压缩算法: 论文采用了相对简单的残差压缩。未来可以探索更复杂的量化或压缩技术，以期在不损失质量的前提下获得更高的压缩率。\n训练流程简化: 带有知识蒸馏的训练流程比原版 ColBERT 更复杂、成本更高。研究如何简化训练流程，降低训练成本，将是一个有价值的方向。\n系统级优化: 当前实现是基于Python和PyTorch。通过C++或CUDA进行底层优化，可以进一步降低检索延迟。\n\n\n可引用的结论 (Citable Conclusions)\n这篇论文提供了一些非常有价值和可引用的结论：\n\n晚期交互模型的潜力: 通过降噪监督（知识蒸馏和难负例挖掘），晚期交互模型的性能可以被大幅提升，显著超越了当前最先进的单向量模型和稀疏模型。\n效率与效果的统一: ColBERTv2 提出的残差压缩机制，可以在几乎不损失模型检索质量的情况下，将索引大小减少6-10倍，成功解决了晚期交互模型的空间占用瓶颈，使其在存储上与单向量模型相当。\n强大的泛化能力: 无论是在域内（MS MARCO）还是在多个域外基准（BEIR, Open-QA, LoTTE）上，ColBERTv2 都取得了SOTA（state-of-the-art）的成绩，证明了其架构和训练方法的鲁棒性和泛化能力。\n词元表示的语义结构: 实验分析表明，ColBERT 的词元级向量天然具有语义聚集性，即同一词语的不同“意义”会对应到向量空间中的不同簇，这是残差压缩有效性的根本原因。\n长尾主题检索的重要性: 论文新提出的LoTTE数据集，填补了现有评测基准在长尾、专业领域自然查询方面的空白，为未来检索模型的泛化能力评测提供了新的视角和资源。\n\n","categories":["paper"],"tags":["paper","Retrieval","Neural IR"]},{"title":"Gecko: Versatile Text Embeddings Distilled from Large Language Models","url":"/paper/Gecko-Versatile-Text-Embeddings-Distilled-from-Large-Language-Models/","content":"问题 (Problem)\n论文旨在解决当前文本嵌入模型领域的核心挑战：如何创建一个既紧凑又通用的文本嵌入模型。\n现有方法存在以下痛点：\n\n通用性差：许多模型在特定任务（如语义相似度）上表现优异，但在跨任务、跨领域（如信息检索、分类、聚类等）的泛化能力上表现不佳。\n数据依赖严重：要构建一个覆盖多领域、多任务的通用模型，通常需要海量的、高质量的人工标注数据。这个过程不仅成本高昂、耗时费力，而且难以覆盖所有场景。\n模型效率问题：为了追求高性能，模型往往变得越来越大（例如参数量超过70亿），嵌入维度也越来越高（例如超过4000维），这给实际部署和应用带来了巨大的计算和存储开销。\n\n因此，本文的核心问题是：我们能在多大程度上直接利用大型语言模型（LLM）中蕴含的丰富世界知识，来蒸馏出一个紧凑、高效且在多种任务上都表现出色的通用文本嵌入模型？\n方法 (Method)\n本文提出了 Gecko，一个通过两步式LLM知识蒸馏流程训练得到的文本嵌入模型。其核心是创建了一个名为 FRet (Few-shot Prompted Retrieval dataset) 的高质量合成数据集。\n\nFRet：两步式LLM蒸馏数据集生成\n该方法不直接使用LLM生成文本内容，而是利用LLM的理解和判断能力来生成高质量的训练标签。\n第一步：LLM生成多样化的任务和查询 (Diverse Query Generation)\n\n起点：从一个巨大的、无标签的网页语料库中随机抽取一个“种子段落” (pseedp_{seed}pseed​)。\n过程：使用一个经过小样本（few-shot）提示（Prompt）的LLM，让它读取这个种子段落，并为其生成一个相关的任务描述 (ttt) 和一个符合该任务的查询 (qqq)。\n\n数学表示为：LLM(PQG,pseed)→(t,q)LLM(\\mathbb{P}_{QG}, p_{seed}) \\rightarrow (t, q)LLM(PQG​,pseed​)→(t,q)\n其中 PQG\\mathbb{P}_{QG}PQG​ 是一个包含了指令和若干示例的固定Prompt。\n\n\n多样性来源：\n\n网页语料库本身内容丰富，涵盖博客、新闻、百科等多种风格和主题。\n通过在Prompt中设计多样化的任务示例（如“问答”、“事实核查”、“寻找相似句子”等），引导LLM生成形式多样的任务和查询。\n\n\n\n图解：如下图上半部分所示，LLM读取一个关于“Phastos…创造原子弹”的段落后，可以生成“问答”任务及对应查询“谁制造了原子弹？”，也可以生成“事实核查”任务及对应查询“Phastos创造了原子弹”。\n\n图：FRet数据集生成流程图\n第二步：LLM挖掘正负样本 (Positive and Negative Mining)\n\n核心假设：种子段落 (pseedp_{seed}pseed​) 不一定是其生成的查询 (qqq) 的最佳答案。语料库中可能存在更相关、更直接的段落。\n过程：\n\n检索：对于上一步生成的每个查询 (qqq)，使用一个预训练好的检索器，从网页语料库中检索出Top-N个最相关的候选段落 P=p(1),...,p(N)P={p^{(1)},...,p^{(N)}}P=p(1),...,p(N)。\n重排序 (Re-ranking)：利用同一个LLM对这N个候选段落进行打分和排序。为了提高排序的鲁棒性，论文融合了两种LLM打分方法：\n\n查询似然度 (Query Likelihood, QL)：计算在给定段落 ppp 的条件下，生成查询 qqq 的对数似然概率，即 LLM(q∣p,PQL)LLM(q|p, \\mathbb{P}_{QL})LLM(q∣p,PQL​)。\n相关性分类 (Relevance Classification, RC)：计算在给定查询 qqq 和段落 ppp 的条件下，生成“相关”标签的对数似然概率，即 LLM(label∣q,p,PRC)LLM(label|q,p,\\mathbb{P}_{RC})LLM(label∣q,p,PRC​)。\n融合：使用倒数排序融合 (Reciprocal Rank Fusion, RRF) 算法将QL和RC的排序结果结合起来，得到最终的排序函数 R(q,p)R(q,p)R(q,p)。\n\n\n重新标注：\n\n正样本 (p+p^+p+)：选择LLM重排序后排名第一的段落，即 p+=p1p^+ = p_1p+=p1​。实验发现，大约有15%的情况下，p1p_1p1​ 并不等于原始的 pseedp_{seed}pseed​，证明了重标注的必要性。\n难负样本 (p−p^-p−)：选择LLM重排序后排名最低的段落（例如pNp_NpN​），或者从排名靠后的段落中随机采样。\n\n\n\n\n\n通过以上两步，最终生成了包含660万个样本的FRet数据集，每个样本都包含(任务, 查询, 正样本, 负样本)四元组。\nGecko 模型训练流程\nGecko的训练分为两个阶段，基于一个12亿参数的预训练Transformer模型。\n预微调 (Pre-finetuning)\n\n\n目的：让模型接触大量不同类型的文本，学习通用的文本表示能力。\n\n\n数据：大规模的社区问答对（如论坛问答）和网页标题-正文对。\n\n\n查询向量的生成(q_i): 它的特殊之处在于，它不仅考虑了查询本身，还融入了任务描述（task）。\nqi=mean_pool∣t∣+∣qi∣[M(t⊕qi)∈R(∣t∣+∣qi∣)×d]∈Rdq_i = mean\\_ pool_{|t|+|q_i|} \\left[ \\mathcal{M}(t \\oplus q_i) \\in \\mathbb{R}^{(|t|+|q_i|) \\times d} \\right] \\in \\mathbb{R}^d\nqi​=mean_pool∣t∣+∣qi​∣​[M(t⊕qi​)∈R(∣t∣+∣qi​∣)×d]∈Rd\n\nt⊕qit \\oplus q_it⊕qi​: 这一步是文本拼接。\n\nttt 代表任务描述，例如“问答”或“事实核查”。\nqiq_iqi​ 是用户的实际查询内容，例如“谁发明了灯泡”。\n⊕\\oplus⊕ 符号表示将这两个字符串连接在一起，形成一个新的输入，如：“任务：问答 | 查询：谁发明了灯泡”。这样做是为了让模型知道它当前需要执行哪种类型的任务。\n\n\nM(...)\\mathcal{M}(...)M(...): M\\mathcal{M}M 代表一个预训练的语言模型（例如Transformer）。\n\n这个模型会读取拼接后的文本，并为其中的每一个词元（token） 都生成一个上下文相关的向量。\n输出结果是一个矩阵，维度是 (词元数量)×d(词元数量) \\times d(词元数量)×d。其中 ddd 是预设的向量维度（比如768维），词元数量是 ∣t∣+∣qi∣|t|+|q_i|∣t∣+∣qi​∣。\n\n\n$mean_pool[…]: 这是平均池化操作。\n\n它会取模型输出的那个矩阵，然后将所有词元的向量相加再取平均值。\n这就好比把一句话里每个词的“含义向量”都融合起来，得到一个代表整句话核心含义的“平均向量”。\n\n\n最终结果 (qi∈Rdq_i \\in R^dqi​∈Rd): 经过平均池化后，原来代表每个词的向量矩阵就被压缩成了一个单一的、维度为 ddd 的向量 qiq_iqi​。这个向量就是最终代表整个“任务+查询”的文本嵌入。\n\n\n\n段落向量的生成(p_i): 这个公式是为段落（passage），比如一篇文章或一个回答，生成向量。它的流程和上面类似，但更简单一些。\npi=mean_pool∣pi∣[M(pi)∈R∣pi∣×d]∈Rdp_i = mean\\_ pool_{|p_i|}[\\mathcal{M}(p_i) \\in \\mathbb{R}^{|p_i| \\times d}] \\in \\mathbb{R}^d\npi​=mean_pool∣pi​∣​[M(pi​)∈R∣pi​∣×d]∈Rd\n\n\n目标函数：使用带有批内负样本（in-batch negatives）的对比学习目标函数。对于批次中的一个查询 qiq_iqi​，其对应的段落 pip_ipi​ 是正样本，批次内所有其他的段落 pjp_jpj​ (j≠i) 都被视为负样本。\nLpre=−1B∑i=1Blog⁡esim(qi,pi)/τ∑j=1Besim(qi,pj)/τL_{pre} = -\\frac{1}{B} \\sum_{i=1}^{B} \\log \\frac{e^{sim(q_i, p_i)/\\tau}}{\\sum_{j=1}^{B} e^{sim(q_i, p_j)/\\tau}}\nLpre​=−B1​i=1∑B​log∑j=1B​esim(qi​,pj​)/τesim(qi​,pi​)/τ​\n其中:\n\nsim(x,y)=x⊤y∣∣x∣∣⋅∣∣y∣∣sim(x,y) = \\frac{x^{\\top}y}{||x|| \\cdot ||y||}sim(x,y)=∣∣x∣∣⋅∣∣y∣∣x⊤y​ 是余弦相似度函数\nτ\\tauτ 是温度超参数。\n\n\n\n微调 (Fine-tuning)\n\n目的：利用高质量的FRet和公开数据集，对模型进行多任务、指令化的微调。\n数据：一个统一的混合数据集，包含FRet以及多个学术数据集（如Natural Questions, SNLI, MNLI等），所有数据都被统一格式化为(任务, 查询, 正样本, 负样本)。\n目标函数：一个更强的批内对比学习损失函数，它不仅包含来自其他样本的正段落作为负样本，还创新性地引入了“同塔负样本 (same-tower negatives)”，即将批次内其他的查询也作为负样本。Lmain=1B∑i=1B[−log⁡esim(qi,pi+)/τ∑j=1B(esim(qi,pj+)/τ+1[j≠i]esim(qi,qj)/τ)+esim(qi,pi−)/τ]L_{main} = \\frac{1}{B} \\sum_{i=1}^{B} \\left [ -\\log \\frac{e^{sim(q_i, p_i^+)/\\tau}}{\\sum_{j=1}^{B} (e^{sim(q_i, p_j^+)/\\tau} + \\mathbb{1_{[j \\ne i]}} e^{sim(q_i, q_j)/\\tau}) + e^{sim(q_i, p_i^-)/\\tau}} \\right ]\nLmain​=B1​i=1∑B​[−log∑j=1B​(esim(qi​,pj+​)/τ+1[j=i]​esim(qi​,qj​)/τ)+esim(qi​,pi−​)/τesim(qi​,pi+​)/τ​]\n\npi+p_i^+pi+​ 是 qiq_iqi​ 的正样本。\npi−p_i^-pi−​ 是 qiq_iqi​ 的难负样本。\npj+p_j^+pj+​ (j≠i) 是批内其他样本的正样本。\nqjq_jqj​ (j≠i) 是同塔负样本，对于对称性任务（如语义相似度）尤其有效。\n\n\n多维度支持：采用了套娃表示学习 (Matryoshka Representation Learning, MRL) 损失，使同一个模型能够输出不同维度的嵌入向量（如768维和256维），增加了模型的灵活性。\n\nBaseline\nGecko与当前主流的文本嵌入模型进行了对比，可以分为两类：\n\n更大规模的模型 (≥7B 参数, &gt;3k 维度)：\n\ngritlm-8x7b, e5-mistral-7b-instruct, gritlm-7b, text-embedding-3-large (OpenAI)\n\n\n相似规模的模型 (≤5B 参数, ≤1k 维度)：\n\ngtr-t5-xxl, gtr-t5-xl, instructor-xl, text-embedding-3-large-256 (OpenAI)\n\n\n\n实验结果显示，Gecko-1B-768 (12亿参数，768维) 的平均性能（66.31）不仅全面超越了所有相似规模的模型，甚至能够与比它大7倍、维度高5倍的模型（如gritlm-7b的66.76）相媲美。Gecko-1B-256 (256维) 的性能也远超同维度的其他模型。\n数据集 (Datasets)\n\n训练数据：\n\n预微调：社区问答网站数据、网页标题-正文数据。\n微调：\n\nFRet (本文贡献)：基于网页语料库生成的660万合成数据。\n公开学术数据：Natural Questions, HotpotQA, FEVER (检索与问答), SNLI, MNLI (自然语言推断), MedMCQA (医疗问答), MIRACL (多语言检索), 以及多个Huggingface上的分类数据集。\n\n\n\n\n评测数据：\n\nMTEB (Massive Text Embedding Benchmark)：一个包含56个数据集的大规模评测基准，覆盖7大类任务：分类、聚类、成对分类、重排序、检索、语义文本相似度(STS)和摘要。\nMIRACL：一个包含18种语言的多语言检索评测基准。\n\n\n\n可复现性 (Reproducibility)\n\n代码：论文中未提及开源代码。\n算力/模型：\n\n模型依赖：训练基于一个12亿参数的Transformer模型，微调这样一个模型需要强大的计算资源（如多个高端GPU或TPU）。\n数据生成依赖：FRet数据集的生成依赖于一个未指定但性能强大的内部大语言模型（推测为Google自家的模型）。外部研究者难以获取同样性能的LLM。\n语料库依赖：生成FRet所用的网页语料库是内部数据，未公开。\n\n\n结论：由于核心的数据集生成方法依赖于非公开的LLM和数据，且没有提供源代码，该工作的外部可复现性很低。\n\n可改进的几个点\n\n降低LLM依赖：方法的核心在于强大的LLM。未来可以探索使用更小、更易于获取的开源LLM（如Llama 3, Mistral等）是否能生成类似质量的数据集，从而让该方法更具通用性和可复现性。\n数据生成效率：目前“生成-检索-重排序”的两步流程计算成本非常高。可以研究能否设计出更高效的单步流程，例如让LLM直接生成高质量的(查询, 正样本, 负样本)三元组。\n任务多样性的系统化生成：当前任务多样性依赖于Prompt中的示例。可以探索更系统化的方法，比如先让LLM生成一个包含上百种任务类型的清单，再基于这个清单去生成数据，以确保任务的广度和均衡性。\n更优的难负样本挖掘策略：论文中使用了排序最低的样本或随机采样作为难负样本。可以探索更高级的策略，例如挖掘那些与查询在语义上高度相似但在关键事实上相悖的样本，为模型提供更具挑战性的学习信号。\nFRet的多语言扩展：目前的FRet数据集是纯英文的。尽管它对多语言模型也有帮助，但如果能直接用LLM生成多种语言的FRet数据，有望在非英语任务上取得更大的性能提升。\n\n可以被引用的一些结论\n\nLLM蒸馏是创建高效嵌入模型的有效途径：通过从LLM中蒸馏知识，一个12亿参数的紧凑模型（Gecko）可以在性能上媲美甚至超越大7倍（&gt;70亿参数）的巨型模型，证明了“小模型+高质量合成数据”路线的巨大潜力。\nLLM重标注对于合成数据质量至关重要：原始用于生成查询的“种子段落”往往不是最佳正样本。利用LLM对检索到的候选集进行重排序并重新选择正样本，能显著提升模型性能。这一“重选”操作发生在约15%的数据上。\n纯合成数据具有强大的零样本泛化能力：仅在合成的FRet数据集上训练的模型，在MTEB这个对它来说是纯零样本（zero-shot）的评测基准上，表现依然非常强劲，甚至超过了许多在人工标注数据上训练的基线模型。\n任务多样性是通用嵌入模型的关键：与在单一合成任务（如纯问答）上训练相比，在一个由LLM生成的多任务混合数据集上训练，能显著提升模型在各类下游任务上的泛化能力。\n低维嵌入也能达到顶尖性能：有效的知识蒸馏使得低维度（如256维）的Gecko模型能够超越许多高维度（如768维）的强大对手，这对于资源受限的应用场景意义重大。\n融合多种LLM打分策略可提升数据标注的鲁棒性：通过RRF融合查询似然度和相关性分类两种LLM打分方式，可以获得一个在不同任务上都表现稳健的重排序器，这对于生成覆盖多种任务的高质量数据集至关重要。\n\n","categories":["paper"],"tags":["paper","Embedding","LLM"]},{"title":"Making LLMs A Better Foundation For Dense Retrieval","url":"/paper/Making-LLMs-A-Better-Foundation-For-Dense-Retrieval/","content":"这篇论文的核心贡献是提出了一种名为 LLaRA (LLM adapted for dense RetrivAl) 的新方法，旨在解决大型语言模型（LLMs）在直接应用于稠密检索任务时的根本性问题。它通过一个高效的“事后适应”（post-hoc adaptation）阶段，显著提升了LLM作为检索模型基座（backbone）的能力。\n\n问题 (Problem)\n大型语言模型（LLMs）虽然在语义理解上能力强大，但其预训练方式与稠密检索的需求存在天然的“鸿沟” 。\n\nLLM的预训练目标：LLM（如GPT系列）主要通过自回归的文本生成任务进行预训练，其目标是预测下一个词元（token）。这使得模型生成的文本嵌入（text embedding）更侧重于捕捉**局部和短期（local and near-future）**的语义信息，以便生成连贯的下文。\n稠密检索的需求：稠密检索需要将查询（query）和文档（document）映射到一个语义空间中，并通过向量相似度来判断其相关性。这要求文本嵌入能够高度概括和表示**全局（global）**的语义信息 。\n核心矛盾：直接使用LLM（尤其是decoder-only架构）最后一个词元的输出嵌入作为全局表征，其效果会受到限制，因为这个嵌入本质上是为了生成任务优化的，而不是为了全局表示。\n\n\n方法 (Method)\n为了解决上述问题，作者提出了LLaRA，它通过两个精心设计的预训练任务（pretext tasks）来调整LLM，使其生成的嵌入更适合稠密检索。\n\n\n核心思想：通过让LLM的文本嵌入去完成“预测整个句子”而非“预测下一个词”的任务，来迫使其学习全局语义表示。\n\n\n模型框架 (Framework - Figure 1 解读)\n\n\n该框架展示了LLaRA如何处理一段文本（例如来自维基百科的两句话）。\n输入: “Norwegian forest cat is a breed cat originating in Northern Europe. This natural breed is adapted to a very cold climate.”\n处理: 模型将第一句话作为输入，并拼接上两个特殊的提示（prompt）：“the original sentence:” 和 “the next sentence:”。\n生成嵌入: LLM处理这个拼接后的序列，并在两个提示语后的特殊token（&lt;\\s&gt;）位置分别生成两个文本嵌入，一个用于EBAE任务，一个用于EBAR任务。\n任务:\n\nEBAE (绿色箭头): 使用第一个嵌入来重构（预测）原始输入句子“Norwegian forest cat…”。\nEBAR (蓝色箭头): 使用第二个嵌入来重构（预测）原始输入的下一句“This natural breed…”。\n\n\n\n\n\n两个预训练任务 (Two Pretext Tasks)\n\n\nEBAE (Embedding-Based Auto-Encoding):\n\n目标：让文本嵌入能够表示输入文本自身的全局语义。\n做法：使用LLM生成的文本嵌入 ete_tet​ 来预测输入句子本身的所有词元。如果一个嵌入能还原整个输入，那么它必然蕴含了完整的全局信息。\n应用场景：这个任务训练出的嵌入能力，天然适合处理“相似性搜索”场景，比如查找与输入文本语义上最接近的内容。\n\n\n\nEBAR (Embedding-Based Auto-Regression):\n\n目标：让文本嵌入学会关联查询（query）和文档（doc）。\n做法：使用文本嵌入 ete_tet​ 来预测输入文本的下一个句子。作者认为，一个相关的文档（如问题的答案）可以被看作是查询的一种“合理的下一句”。\n应用场景：这个任务训练出的嵌入能力，更适合处理“问答”这类需要推理和关联匹配的场景。\n\n\n\n\n\n数学公式 (Mathematical Formulas)\n\n稠密检索基础: 查询 qqq 和文档 ddd 的相关性通过嵌入向量的相似度计算：sim(q,d)=⟨eq,ed⟩sim(q, d) = \\langle e_q, e_d \\ranglesim(q,d)=⟨eq​,ed​⟩。\nLLaRA嵌入生成:\n\n为了提升效率，两个任务的提示语被合并成一个联合提示（joint prompt）：&quot;[输入文本] The original sentence: &lt;\\s&gt; The next sentence: &lt;\\s&gt;&quot;。并通过修改注意力掩码（attention mask）使两个任务的计算相互独立，在一个前向传播中同时得到两个嵌入。\nEBAE 嵌入: etα←LLaMA(T,SELF,⟨\\s⟩)[−1]e_{t}^{\\alpha} \\leftarrow LLaMA(T, SELF, \\langle\\backslash s\\rangle)[-1]etα​←LLaMA(T,SELF,⟨\\s⟩)[−1]，其中 SELF 代表提示 “The original sentence:”。\nEBAR 嵌入: etβ←LLaMA(T,NEXT,⟨s⟩)[−1]e_{t}^{\\beta} \\leftarrow LLaMA(T, NEXT, \\langle s\\rangle)[-1]etβ​←LLaMA(T,NEXT,⟨s⟩)[−1]，其中 NEXT 代表提示 “The next sentence:”。\n\n\n训练目标 (Training Objective):\n\nLLaRA的训练目标是让文本嵌入（etαe_t^\\alphaetα​ 或 etβe_t^\\betaetβ​）通过一个线性投影层来预测目标句子中的每一个词元。这被构建为一个多分类问题，其目标函数如下：min⁡∑t∈Texp⁡(eTWt)∑v∈Vexp⁡(eTWv)\\min \\sum_{t \\in \\mathcal{T}} \\frac{\\exp(e^T W_t)}{\\sum_{v \\in V} \\exp(e^T W_v)}\nmint∈T∑​∑v∈V​exp(eTWv​)exp(eTWt​)​\n\n公式解读:\n\neee: 指的是文本嵌入 etαe_t^\\alphaetα​ 或 etβe_t^\\betaetβ​。\nW∈R∣V∣×dW \\in \\mathbb{R}^{|V| \\times d}W∈R∣V∣×d: 是一个线性投影矩阵，将维度为 ddd 的文本嵌入映射到整个词表空间。VVV 是词表空间。\nT\\mathcal{T}T: 是目标句子的词元集合。对于EBAE，T\\mathcal{T}T是输入句子；对于EBAR，T\\mathcal{T}T是下一句。\n这个公式本质上是计算用单个全局嵌入 eee 去预测目标句子 T\\mathcal{T}T 中所有词元的交叉熵损失之和。\n\n\n\n\n\n\n\n\nBaseline\n论文与多种基线模型进行了全面对比：\n\n稀疏检索: BM25。\n基于BERT的稠密检索模型: ANCE, RetroMAE, SimLM等。这些是基于中等规模预训练模型（~110M参数）的代表性工作。\n基于LLM的稠密检索模型:\n\nGTR-XXL (4.8B), SGPT (5.8B), OpenAI-Ada-002。\nRepLLaMA (7B): 这是最重要的一个基线，因为它同样使用了LLaMA-2-7B模型和相同的微调方法（hard negatives），但没有经过LLaRA的适应性训练 。因此，与RepLLaMA的直接对比可以最干净地证明LLaRA方法本身的有效性。\n\n\n\n\n数据集 (Datasets)\n\n适应性训练 (Adaptation Training): 在DPR整理的无标签维基百科语料库上进行。这说明LLaRA不需要任何有监督标注数据，成本较低。\n下游任务微调 (Fine-tuning): MS MARCO 通道检索（passage retrieval）数据集。\n评测 (Evaluation):\n\nMS MARCO: 通道检索和文档检索（document retrieval）任务。\nBEIR benchmark: 一个异构的、包含多种检索场景（如问答、事实核查等）的基准测试集，用于评估模型的零样本（zero-shot）泛化能力。\n\n\n\n\n可复现性 (Reproducibility)\n\n代码: 论文承诺模型和源代码将在BGE的GitHub仓库中公开发布（https://github.com/FlagOpen/FlagEmbedding）。\n算力:\n\n模型基座: LLaMA-2-7B (base)。\n适应性训练: 在维基百科语料上训练了10,000步，批大小（batch size）为256，序列长度为1024。这对算力有一定要求。\n微调: 微调阶段采用了LoRA（Low-Rank Adaptation）技术，这是一种参数高效的微调方法，可以显著降低LLM微调的资源消耗。\n\n\n\n\n可改进的 1 个点\n\n与更先进的微调方法结合：\n\n论文中明确提到，LLaRA在微调阶段仅使用了简单的“ANN hard negatives”方法。作者自己也指出，如果未来能够利用更先进的微调技术，LLaRA的性能很可能被进一步提升。\n例如，可以尝试将LLaRA适应后的模型与知识蒸馏（knowledge distillation，从更强大的交叉编码器cross-encoder中学习）、更复杂的负采样策略（如SimANS）或其他对比学习框架结合，可能会在现有基础上取得更好的效果。\n\n\n\n\n可以被引用的一些结论\n\nLLM与检索任务的不匹配性: 大型语言模型因其自回归的生成式预训练目标，其输出嵌入天然地偏向于捕捉局部语义，这与稠密检索所需的全局语义表示存在明显的不匹配。\nLLaRA适应训练的有效性: LLaRA提出的事后适应方法能显著提升LLM的文本嵌入能力。与使用相同基座模型但未经适应的RepLLaMA相比，LLaRA在MS MARCO通道检索MRR@10上提升了1.9%，在文档检索MRR@100上提升了1.9%，在BEIR零样本评测NDCG@10上提升了1.0%。\nLLM作为检索基座的巨大潜力: 从基于BERT的编码器转向基于LLM的编码器，为稠密检索带来了巨大的性能飞跃。例如，在MS MARCO通道检索任务上，LLaRA相比之前的SOTA（如RetroMAE）在MRR@10指标上高出近4个百分点。\nLLM显著提升检索模型的泛化能力: 相比于在零样本场景下表现不佳的BERT类模型，基于LLM的检索器（特别是经过LLaRA优化的模型）在BEIR基准测试上展现了强大的泛化能力，平均性能远超BM25和BERT基线。\n\n","categories":["paper"],"tags":["paper","Retrieval","LLM","Dense Retrieval"]},{"title":"How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval","url":"/paper/How-to-Train-Your-DRAGON-Diverse-Augmentation-Towards-Generalizable-Dense-Retrieval/","content":"问题 (Problem)\n传统的稠密检索（Dense Retrieval, DR）模型普遍存在一个核心问题：在监督式评测和零样本（Zero-shot）评测之间存在明显的性能权衡（trade-off）。具体来说，一个在特定数据集（如 MS MARCO）上通过监督学习训练得很好的模型，在从未见过的新领域（零样本场景）上往往表现不佳，反之亦然。\n如 Figure 1 所示，大多数现有的检索器（除了参数量巨大（4.8B）的 GTR-XXL）都分布在一条倾斜的直线上，显示了“监督式评测”得分（X轴）和“零样本评测”得分（Y轴）之间的负相关关系。当时的普遍观点认为，要打破这种权衡，必须大幅增加模型（如BERT-base）的容量。\n本文的核心目标：挑战上述观点，证明在不增加模型大小（仍使用 BERT-base 级别）的前提下，通过一种更优的训练方法，可以训练出一个在监督式和零样本场景下均达到顶尖水平（State-of-the-Art, SOTA）的通用稠密检索器。\nFigure 1 解读:\n\n\n坐标轴:\n\nX轴: Supervised evaluation (MS MARCO Dev: RR@10) - 衡量模型在见过的、有监督数据上的性能。\nY轴: Zero-shot evaluation (BEIR 13 subsets: nDCG@10) - 衡量模型在未见过的新领域的泛化能力。\n\n\n数据点: 图中的每个点代表一个检索模型。不同形状代表不同类型的检索器（如圆形代表稠密检索，方形代表稀疏检索）。\n蓝色趋势线: 清晰地揭示了大多数模型存在的性能权衡。\nDRAGON (红色点): 本文提出的模型，显著地位于趋势线的右上方，表明它同时在监督和零样本任务上取得了高分，成功打破了这种权衡。\n\n\n方法 (Method)\n作者提出了一个名为 DRAGON (Dense Retriever trained with diverse AuGmentatiON) 的模型。其核心思想是构建一个统一的数据增强（Data Augmentation, DA）框架，通过多样化的查询增强和渐进式的相关性标签增强来训练模型。\n统一的数据增强框架\n作者将许多现有的稠密检索改进方法（如知识蒸馏、对比预训练、伪查询生成）都归纳到数据增强的框架下，主要分为两类：\n\n查询增强 (Query Augmentation): 增加训练查询的数量和多样性。\n相关性标签增强 (Relevance Label Augmentation): 为每个查询提供更丰富、更准确的正负样本标签。\n\n查询增强\nDRAGON 探索了两种主要的查询增强方式：\n\n句子裁剪 (Sentence Cropping): 一种计算成本低廉的方法，直接从语料库的文档中裁剪出句子作为查询。这种方法可以轻松地将查询规模扩大到千万级别。\n伪查询生成 (Pseudo Query Generation, GenQ): 使用大型语言模型（如 T5）为文档生成类似人类提问的查询。这种方法生成的查询质量高，但计算成本昂贵。\n\n最终，DRAGON 采用混合方式，将一半的裁剪句子和一半的生成式查询混合作为训练数据，以兼顾数量、多样性和质量。\n多样化和渐进式的标签增强\n这是 DRAGON 方法的核心创新点。传统的标签增强通常依赖于单个、强大的“教师模型”（如交叉编码器 Cross-Encoder, CE），但作者认为单个教师的视角是有限的。\n核心假设：不同的检索器（如稀疏、稠密、多向量模型）捕捉到的文本相关性信号是不同的（例如，词汇匹配 vs. 语义匹配）。为了让学生模型学到更通用的匹配能力，应该从多个具有不同优势的教师那里学习。\n具体实现：渐进式监督 (Progressive Supervision)\nDRAGON 不会一次性将所有教师的知识都灌输给模型，而是采用一种类似课程学习（Curriculum Learning）的策略，分阶段、逐步地引入新的教师。\n\n教师团队: DRAGON 使用了一组多样化的教师模型，包括：\n\n稀疏检索: uniCOIL, SPLADE++\n稠密检索: Contriever, GTR-XXL\n多向量检索: ColBERTv2\n\n\n训练过程: 训练分为多个迭代（Iteration）。\n\n第一阶段: 只使用第一个教师（如 uniCOIL）生成的排序列表来为增强查询构建训练样本（正负例）。\n第二阶段: 引入第二个教师（如 Contriever），此时，对于每个查询，模型会从第一和第二个教师的排序列表中均匀采样一个作为监督信号。\n后续阶段: 逐步引入更多的教师，在第 TTT 个阶段，模型会从前 TTT 个教师的监督信号中均匀采样。\n\n\n\nFigure 2 图解:\n\n该图直观地展示了渐进式监督的过程。\n\nIteration 1: 只有红色类别的标签（来自教师1）。模型从这些正负样本中学习。\nIteration 2: 在原有基础上，增加了绿色类别的标签（来自教师2）。此时训练数据的“视野”扩大了，模型需要学习兼容两种不同的相关性判断。\nIteration 3: 进一步增加了蓝色类别的标签（来自教师3）。模型学习的监督信号变得更加丰富和复杂。\n这个过程引导模型从简单、单一的视角逐步过渡到复杂、多元的视角，从而更有效地学习通用性。\n\n数学公式\nDRAGON 的训练遵循标准的稠密检索对比学习框架。\n\n相似度分数: 查询 qqq 和文档 ddd 之间的相似度通过它们向量表示的点积计算：\n\ns(q,d)≜Eq(q)⋅Ed(d)s(q, d) \\triangleq E_q(q) \\cdot E_d(d)\ns(q,d)≜Eq​(q)⋅Ed​(d)\n其中 EqE_qEq​ 和 EdE_dEd​ 是查询和文档的编码器（通常是 BERT），输出的是 [CLS] token 的向量表示。\n\n损失函数: 使用 InfoNCE 损失函数。对于一个查询 qqq、其相关的正样本 d+d^+d+ 和一组负样本 dj−j=1k{d_j^-}_{j=1}^kdj−​j=1k​，目标是最小化以下损失：\n\nL=−log⁡exp⁡(s(q,d+))exp⁡(s(q,d+))+∑j=1kexp⁡(s(q,dj−))L = -\\log \\frac{\\exp(s(q, d^+))}{\\exp(s(q, d^+)) + \\sum_{j=1}^k \\exp(s(q, d_j^-))}\nL=−logexp(s(q,d+))+∑j=1k​exp(s(q,dj−​))exp(s(q,d+))​\n这个损失函数旨在拉近查询与正样本的距离，同时推远与负样本的距离。在 DRAGON 的训练中，d+d^+d+ 和 dj−d_j^-dj−​ 是从教师模型生成的排序列表中采样得到的。\n\nBaseline\nDRAGON 与多种先进的检索模型进行了比较，涵盖了不同的架构和训练技术：\n\n不同架构模型:\n\nSPLADE++: 强大的稀疏检索模型。\nColBERTv2: 高效的轻量级晚期交互（multi-vector）模型。\nGTR-XXL: 参数量巨大的稠密检索模型（4.8B）。\n\n\n基线稠密检索模型 (BERT-base):\n\n知识蒸馏: RocketQAv2, CL-DRD。\n对比预训练: coCondenser, Contriever, COCO-DR。\n掩码自编码预训练: COT-MAE, RetroMAE。\n无监督领域自适应: GPL, PTR。\n\n\n\n\n数据集 (Datasets)\n\n训练数据来源:\n\nMS MARCO Passage Corpus (8.8M passages): 用于进行查询增强和标签增强，是生成所有训练数据的唯一语料库。\n\n\n评测数据集:\n\n监督式评测:\n\nMS MARCO Dev: 标准的领域内（in-domain）评测集。\nTREC Deep Learning (DL) 2019 &amp; 2020: 拥有更精细人工标注的监督评测集。\n\n\n零样本评测:\n\nBEIR: 一个包含18个不同领域、不同任务的异构零样本评测基准。\nLoTTE: 包含多个 StackExchange 社区问答数据的零样本评测集。\n\n\n\n\n\n\n可复现性 (Reproducibility)\n\n代码: 作者在论文中提供了 GitHub 仓库链接：https://github.com/facebookresearch/dpr-scale，包含了代码和模型 checkpoints。\n算力: 训练成本非常高昂。论文在“局限性”部分明确指出，训练一个完整的 DRAGON 模型需要：\n\n32 块 A100 (40 GB) GPUs\n耗时 5 天\n此外，还需要预先准备好所有训练好的教师模型，并使用它们对千万级别的增强查询进行推理以生成标签，这本身也是一个巨大的计算开销。\n\n\n\n\n可改进的几个点 (Potential Improvements)\n论文自身也指出了几个未来可以探索的方向：\n\n降低训练成本: 当前的训练数据规模高达 2800 万，其中可能包含大量重复或低质量的查询。设计一种有效的数据筛选或去重方法，可以在不牺牲性能的前提下大幅降低训练成本。\n与领域自适应结合: DRAGON 已经具备了很强的零样本能力。如果将其作为基础模型，再结合 GPL、PTR 等领域自适应技术进行微调，可能会在特定目标领域上取得更好的性能。\n与其他预训练方法结合: DRAGON+ 的实验表明，掩码自编码预训练（如 RetroMAE）与本文的对比学习方法是互补的。未来可以进一步探索如何更好地融合生成式和对比式预训练方法。\n特定领域的知识增强: 在 LoTTE 数据集的科技主题上，DRAGON 的表现不如 SPLADE++ 和 ColBERTv2。这表明，在训练中引入特定领域的语料库（如科学文献）可能会弥补这一不足。\n\n\n可以被引用的结论 (Key Takeaways)\n\n成功打破性能权衡: 本文首次证明，一个标准的 BERT-base 大小的稠密检索器，无需增加模型参数，就能同时在监督式和零样本评测中达到 SOTA 水平，打破了两者之间的固有权衡。\n多样化监督优于单一强监督: 与普遍认为“应使用最强的交叉编码器作为教师”的观点相反，本文发现，使用来自多个不同类型检索器（稀疏、稠密、多向量）的多样化监督信号，对于训练模型的泛化能力更为关键。\n渐进式学习策略的有效性: 渐进式监督（Progressive Supervision）作为一种课程学习策略，能有效引导模型学习复杂且多样的相关性信号，其效果优于一次性混合或融合所有监督信号。\n廉价数据增强的意外价值: 实验惊人地发现，通过句子裁剪这种廉价方式生成的增强查询，在提升模型泛化能力方面，效果甚至优于使用昂贵语言模型生成的“类人”查询（GenQ）。这为未来低成本、大规模训练通用检索器提供了新的思路。\n统一的数据增强框架: 论文提出的数据增强（DA）框架，为理解和归类稠密检索领域的各种训练技巧（如知识蒸馏、对比学习等）提供了一个统一且有价值的视角。\n\n","categories":["paper"],"tags":["paper","Retrieval","Dense Retrieval","zero-shot","supervised"]},{"title":"Multilingual E5 Text Embeddings","url":"/paper/Multilingual-E5-Text-Embeddings/","content":"核心问题 (Problem)\n现有的文本嵌入（Text Embedding）模型大多只在英文语料上进行训练，这极大地限制了它们在多语言场景下的应用。为了解决这一问题，微软的研究人员开发了一系列名为mE5（multilingual E5）的开源多语言文本嵌入模型，旨在提供在多种语言上都表现出色的高质量文本表示能力。\n\n核心方法 (Method)\n该论文的核心方法沿用了其英文版E5模型的两阶段训练流程：弱监督对比学习预训练 + 监督微调。此外，还引入了一个创新的**指令微调（instruction-tuned）**版本。\n模型架构与初始化\n研究人员发布了三种不同规模的模型，以平衡效果和效率：\n\nmE5-small: 基于multilingual-MiniLM初始化。\nmE5-base: 基于xlm-roberta-base初始化。\nmE5-large: 基于xlm-roberta-large初始化。\n\n第一阶段：弱监督对比学习预训练 (Weakly-supervised Contrastive Pre-training)\n此阶段的目标是让模型从海量无标注或弱标注数据中学习通用的多语言文本表示。\n\n\n训练数据: 使用了从多个来源收集的约10亿个多语言文本对。这些数据对的形式多样，例如（章节标题，段落内容）、（问题，回答）、（标题，新闻正文）等。\n\n\n数据源构成 (见下表):\n\n\n\n数据源\n样本量\n\n\n\n\nWikipedia\n1.5亿\n\n\nmC4\n1.6亿\n\n\nMultilingual CC News\n1.6亿\n\n\nNLLB\n1.6亿\n\n\nReddit\n1.6亿\n\n\nS2ORC\n5000万\n\n\nStackexchange\n5000万\n\n\nxP3\n8000万\n\n\nMisc. SBERT Data\n1000万\n\n\n总计\n约10亿\n\n\n\n\n\n\n\n训练目标: 采用标准的InfoNCE对比学习损失函数。对于一个给定的文本（锚点），模型需要将其对应的正例（如配对的标题和内容）的相似度拉近，同时将其与批次内所有其他不相关的文本（负例）的相似度推远。其数学公式如下：\nLInfoNCE=−log⁡exp⁡(sim(q,p+)/τ)∑i=1Nexp⁡(sim(q,pi)/τ)\\mathcal{L}_{\\text{InfoNCE}} = -\\log \\frac{\\exp(\\text{sim}(q, p_{+})/\\tau)}{\\sum_{i=1}^{N} \\exp(\\text{sim}(q, p_{i})/\\tau)}\nLInfoNCE​=−log∑i=1N​exp(sim(q,pi​)/τ)exp(sim(q,p+​)/τ)​\n其中，qqq 是查询（query）文本的嵌入，p+p_{+}p+​ 是其对应的正例（positive）文本的嵌入，pip_{i}pi​ 包含了正例和所有负例（in-batch negatives），τ\\tauτ 是温度超参数，用于调节相似度分布的平滑度。\n\n\n第二阶段：监督微调 (Supervised Fine-tuning)\n在预训练之后，模型会在少量高质量的标注数据集上进行微调，以进一步提升其在特定任务上的表现。\n\n\n训练数据: 使用了约160万个来自不同任务的高质量标注数据对。\n\n\n数据源构成 (见下表):\n\n\n\n数据源\n样本量（约）\n\n\n\n\nMS-MARCO Passage &amp; Document\n57万\n\n\nNQ, TriviaQA, SQUAD\n22万\n\n\nNLI\n27.5万\n\n\nELI5\n10万\n\n\nNLLB\n10万\n\n\nDuReader Retrieval\n8.6万\n\n\nFever\n7万\n\n\nHotpotQA\n7万\n\n\nQuora Duplicate Questions\n1.5万\n\n\nMr. TyDi\n5万\n\n\nMIRACL\n4万\n\n\n总计\n约160万\n\n\n\n\n\n\n\n优化技巧: 除了使用批内负例外，此阶段还引入了难负例挖掘（mined hard negatives）和来自交叉编码器（cross-encoder）模型的知识蒸馏（knowledge distillation），以增强嵌入的判别能力。\n\n\n指令微调模型 (Instruction-Tuned Model)\n为了让模型能更好地理解任务意图，研究者还训练了一个特殊的指令微调模型mE5-large-instruct。\n\n核心思想: 在输入文本前加入描述任务的自然语言指令（如“检索相关的段落”），让模型根据指令生成更具任务针对性的嵌入。\n特殊数据: 在监督微调的数据基础上，额外加入了由GPT-3.5/4生成的50万个合成数据。这些数据包含了15万个独特的指令，覆盖了93种语言，极大地增强了模型的泛化能力和多语言能力。\n\n\n基线模型 (Baseline)\n论文在多个基准上与当时最先进的多语言和单语言（英文）模型进行了对比。\n\n英文能力基准 (MTEB):\n\nLaBSE: 一个专门在翻译对上训练的多语言模型。\nCohere-multilingual-v3: 商业化的多语言模型。\nBGE-large-en-v1.5: 当时一个表现很强的纯英文模型。\n\n\n多语言检索基准 (MIRACL):\n\nBM25: 传统的稀疏检索算法。\nmDPR: 一个在MIRACL训练集上微调过的稠密检索模型。\n\n\n双语文本挖掘 (Bitext Mining):\n\nmContriever: 一个基于对比学习的检索模型。\nLaBSE: 在该任务上表现出色的强基线。\n\n\n\n\n数据集 (Datasets)\n训练数据集:\n\n预训练阶段: 详细构成见上方“核心方法”部分的表格，总量约为10亿对。\n微调阶段: 详细构成见上方“核心方法”部分的表格，总量约为160万对。mE5-large-instruct额外使用了50万合成数据。\n\n评估基准 (Evaluation Benchmarks):\n\nMTEB (Massive Text Embedding Benchmark): 用于评估模型在英文世界的综合能力，涵盖了分类、聚类、排序、检索、语义相似度等多种任务（共56个数据集）。\nMIRACL (Multilingual Information Retrieval Across a Continuum of Languages): 一个多语言检索基准，论文中评估了其在16种不同语言上的表现。\nBitext Mining: 跨语言相似度搜索任务，用于评估模型在没有词汇重叠情况下匹配语义相似句子的能力。使用了BUCC 2018（4种语言）和Tatoeba（112种语言）两个数据集。\n\n\n可复现性 (Reproducibility)\n\n代码与模型: 论文中明确指出模型权重和相关信息已在GitHub上公开发布：https://github.com/microsoft/unilm/tree/master/e5。这使得社区可以轻松使用和复现这些模型。\n算力与超参数:\n\n预训练: 使用了高达32,000的批处理大小（batch size），训练了30,000步。这需要非常强大的计算资源（通常是多机多卡的GPU集群）。\n微调: 批处理大小为512，训练2个周期（epoch）。\n学习率: 针对不同尺寸的模型和不同训练阶段设置了不同的学习率（预训练为{3,2,1}×10−4\\{3,2,1\\}\\times10^{-4}{3,2,1}×10−4，微调为{3,2,1}×10−5\\{3,2,1\\}\\times10^{-5}{3,2,1}×10−5，分别对应small/base/large模型）。\n复现门槛: 普通研究者或开发者可以轻松下载模型进行微调和推理，但从头开始复现整个预训练过程的成本极高。\n\n\n\n\n可改进的几个点 (Potential Improvements)\n\n合成数据的质量与偏差: mE5-large-instruct的卓越性能部分归功于GPT生成的合成数据。然而，这些数据可能继承了大型语言模型的偏见或事实性错误，对其进行更深入的分析和清洗可能会进一步提升模型鲁棒性。\n对极低资源语言的覆盖: 尽管模型覆盖了超过100种语言，但对于那些在预训练数据中出现频率极低的语言，其性能可能仍然有限。未来的工作可以探索如何通过迁移学习或更有效的数据采样策略来提升这些语言的效果。\n模型效率与压缩: 论文提到了小尺寸模型在效率上的优势，但性能有所牺牲。可以进一步研究模型压缩技术（如量化、剪枝）来减小mE5-large模型的存储和推理开销，同时尽可能保持其高性能。\n指令的泛化能力: 指令微调是一个很有前景的方向。可以探索更复杂、更多样化的指令形式，甚至让模型能理解零样本（zero-shot）的未知指令，从而提升其在更广泛任务上的泛化能力。\n跨语言知识的对齐: 虽然模型在多语言任务上表现出色，但其内部如何对齐不同语言的语义空间仍值得深入探究。更显式地进行跨语言对齐（cross-lingual alignment）的训练或许能带来性能提升。\n\n\n可以被引用的一些结论 (Citable Conclusions)\n\n两阶段训练范式的有效性: “弱监督对比学习预训练 + 监督微调”的训练范式是构建高性能多语言文本嵌入模型的有效路径。\n指令微调的巨大潜力: 通过使用包含任务指令的合成数据进行微调，mE5-large-instruct模型的性能得到了显著提升，甚至在英文基准上超越了同等规模的强力纯英文模型（如BGE-large-en-v1.5）。\n多语言能力的领先水平: 在多语言检索基准MIRACL上，mE5系列模型显著优于经过特定任务微调的mDPR模型。在跨语言文本匹配任务（Bitext Mining）上，mE5-large-instruct也超越了专为此类任务设计的LaBSE模型。\n一个模型，多种用途: mE5模型不仅在多语言检索任务上表现优异，也在语义相似度、文本分类和聚类等多种NLP任务中展现出强大的、可迁移的特征提取能力。\n开源模型的价值: 通过开源不同规模的mE5模型，该工作为学术界和工业界在多语言信息检索、检索增强生成（RAG）等领域提供了强大且易于使用的基础工具。\n\n","categories":["paper"],"tags":["paper","Embeddings"]},{"title":"Precise Zero-Shot Dense Retrieval without Relevance Labels","url":"/paper/Precise-Zero-Shot-Dense-Retrieval-without-Relevance-Labels/","content":"问题 (Problem)\n传统的密集检索（Dense Retrieval）系统严重依赖大规模的、人工标注的“查询-文档”相关性数据进行训练。然而，在许多新的或特定的领域中，获取这样的标注数据成本高昂、耗时巨大，甚至是不可能的。因此，如何在没有任何相关性标签（即“零样本”或“完全无监督”）的情况下，构建一个开箱即用且性能强大的密集检索系统，是一个核心的挑战。\n\n方法 (Methodology)\n为了解决上述问题，论文提出了 HyDE (Hypothetical Document Embeddings，假设性文档嵌入) 方法。其核心思想是绕过对“查询-文档”相关性的直接建模，而是将其分解为两个独立的、更容易处理的任务。\n工作流程图解 (Figure 1)\n\n该图清晰地展示了 HyDE 的两步流程：\n\n生成 (Generation)：用户的查询 (query) 首先与一个任务指令 (instruction) 相结合，然后被送入一个遵循指令的大语言模型（如 GPT）。该模型会生成一个“假设性”的文档，这个文档虽然内容可能是虚构的（包含幻觉），但它在语义和结构上捕捉了真实相关文档的特征。\n编码与检索 (Encoding &amp; Retrieval)：\n\n生成的假设性文档被输入到一个无监督的文本编码器 (如 Contriever)中，该编码器将其压缩成一个向量。这个向量被视为原始查询的“代理”。\n利用这个向量，系统在预先编码好的真实文档库中进行相似度搜索，最终召回最相似的真实文档 (real document)。\n\n\n\n核心数学公式\nHyDE 的方法规避了传统密集检索中直接计算 sim(q, d) 的难题。\n\n\n文档编码器 (Document Encoder)：\nHyDE 使用一个通过无监督对比学习预训练的编码器 f=encd=encconf = enc_{d} = enc_{con}f=encd​=enccon​ 来处理所有真实文档和假设性文档。\nvd=f(d)v_d = f(d)\nvd​=f(d)\n这个编码器 fff 的作用是将任何文档（真实的或假设的）映射到一个向量空间中。\n\n\n查询向量的构建 (Query Vector Construction)：\n查询向量 vqv_qvq​ 不是直接由查询 qqq 编码而来，而是通过对生成的假设性文档进行编码得到的。给定一个查询 qijq_{ij}qij​ 和一个指令 INSTiINST_iINSTi​，生成模型 ggg 会产出一个假设性文档。查询向量是所有可能生成的假设性文档编码向量的期望值。\nE[vqij]=E[f(g(qij,INSTi))]\\mathbb{E}[v_{q_{ij}}] = \\mathbb{E}[f(g(q_{ij}, INST_i))]\nE[vqij​​]=E[f(g(qij​,INSTi​))]\n\n\n期望的估计 (Estimating the Expectation)：\n在实践中，期望值通过从生成模型 ggg 中采样 N 个假设性文档 dk^\\hat{d_k}dk​^​，然后将它们的编码向量取平均来估计。\nv^qij≈1N∑k=1Nf(dk^)wheredk^∼g(qij,INSTi)\\hat{v}_{q_{ij}} \\approx \\frac{1}{N} \\sum_{k=1}^{N} f(\\hat{d_k}) \\quad \\text{where} \\quad \\hat{d_k} \\sim g(q_{ij}, INST_i)\nv^qij​​≈N1​k=1∑N​f(dk​^​)wheredk​^​∼g(qij​,INSTi​)\n论文中还考虑将原始查询本身也作为一个“假设”，与N个生成文档的向量一起平均，以增强稳健性。\nv^qij=1N+1[∑k=1Nf(dk^)+f(qij)]\\hat{v}_{q_{ij}} = \\frac{1}{N+1} \\left[ \\sum_{k=1}^{N}f(\\hat{d_k}) + f(q_{ij}) \\right]\nv^qij​​=N+11​[k=1∑N​f(dk​^​)+f(qij​)]\n\n\n最终检索 (Final Retrieval)：\n最后，用这个构建好的查询向量 v^qij\\hat{v}_{q_{ij}}v^qij​​ 与语料库中所有真实文档的向量 vdv_dvd​ 计算内积（相似度），并召回得分最高的文档。\nsim(qij,d)=⟨v^qij,vd⟩sim(q_{ij}, d) = \\langle \\hat{v}_{q_{ij}}, v_d \\rangle\nsim(qij​,d)=⟨v^qij​​,vd​⟩\n\n\n\n基线模型 (Baselines)\n论文将 HyDE 与多类基线模型进行了比较：\n\n无监督模型:\n\nBM25: 传统的基于词频的稀疏检索方法。\nContriever / mContriever: HyDE 所使用的底层无监督密集编码器，单独使用作为对比。\n\n\n有监督模型 (作为参考):\n\nDPR, ANCE: 在大规模数据集 MS MARCO 上训练的监督式密集检索模型。\nContriever-ft / mContriever-ft: 在 MS MARCO 上进行微调后的 Contriever 模型，代表了当前先进的监督式方法。\nmDPR, mBERT, XLM-R: 用于多语言任务比较的监督式模型。\n\n\n\n\n数据集 (Datasets)\n实验覆盖了多种任务和语言，以验证 HyDE 的通用性。\n\n网页搜索: TREC DL19 和 TREC DL20，两者都基于 MS MARCO 数据集。\n低资源检索 (BEIR  benchmark): 包含7个不同领域的任务，如 Scifact (科学事实核查), Arguana (论点检索), TREC-COVID (新冠科研文献), FiQA (金融问答), DBPedia (实体检索), TREC-NEWS (新闻检索) 和 Climate-Fever (气候变化事实核查)。\n多语言检索 (Mr.TyDi): 涵盖斯瓦希里语 (sw)、韩语 (ko)、日语 (ja) 和孟加拉语 (bn) 四种语言。\n\n\n可复现性 (Reproducibility)\n\n代码: 论文在首页明确指出其代码是开源的，并提供了 GitHub 链接。实验使用了 Pyserini 工具包进行检索。\n算力/模型:\n\n生成模型: 主要使用了 OpenAI 的 text-davinci-003 API。同时也测试了 Cohere (52B 参数) 和 FLAN-T5-xxl (11B 参数)。\n编码器模型: 使用了 Contriever (BERT-base, 110M 参数) 和 GTR-XL (T5-XL, 1.2B 参数)。\n论文附录中详细列出了所用模型的大小、来源和许可证信息。\n\n\n\n\n一个可改进的点\n指令的精细化与自适应 (Instruction Refinement and Adaptation)。\n论文在分析部分提到，在 FiQA（金融）和 DBPedia（实体）这两个任务上，HyDE 的性能与微调后的模型（Contriever-ft）差距较为明显。作者推测，这可能是因为用于这些任务的指令“欠指定”（under-specification）。他们认为，设计更精巧、更具针对性的指令（more elaborate prompts）可能会提升性能。\n这是一个非常直接的改进方向：\n\n可以研究如何根据查询的意图或领域，自动生成或选择最优的指令。例如，可以训练一个小模型来判断查询属于哪个领域（金融、科学、日常问答等），然后从一个预设的指令库中选择最匹配的指令模板。\n甚至可以探索让大语言模型本身来“反思”和“优化”指令。例如，可以先用一个通用指令生成假设性文档，然后分析其与检索结果的初步匹配情况，再让大语言模型根据该反馈，生成一个更精确的指令，进行第二轮检索。这可以使 HyDE 具备一定的自适应能力。\n\n","categories":["paper"],"tags":["paper","Retrieval","Dense Retrieval","zero-shot"]},{"title":"Zero-Shot Dense Retrieval with Embeddings from Relevance Feedback","url":"/paper/Zero-Shot-Dense-Retrieval-with-Embeddings-from-Relevance-Feedback/","content":"问题 (Problem)\n在信息检索领域，密集检索（Dense Retrieval）系统通常需要大量的标注数据（即“查询-相关文档”对）进行训练才能达到良好效果。但在许多场景下，这种标注数据是稀缺或不存在的。因此，如何在没有标注数据的情况下（即零样本 Zero-Shot 场景）构建高效的密集检索系统，是一个核心挑战。\n现有的SOTA（State-of-the-art）方法，如HyDE，尝试使用大语言模型（LLM）来解决这个问题。它的思路是：针对一个用户查询，让LLM生成一篇“假想的”（hypothetical）相关文档，然后用这篇假想文档的向量去寻找语料库中内容最相似的真实文档。\n然而，这种依赖LLM生成假想文档的方法存在三个主要缺陷：\n\n知识局限性：该方法严重依赖LLM自身的参数化知识。如果查询涉及特定或专有领域（如公司内部文档），LLM可能无法生成高质量、有事实依据的假想文档。\n效率低下：对于每个查询，LLM都需要生成一篇完整的文档（包含大量token），这个生成过程非常耗时，导致检索延迟很高。\n内容不可靠：即使给LLM提供一些参考文档作为上下文，它在生成内容时也可能出现幻觉、忽略或曲解已有信息等问题。\n\n因此，本文要解决的问题是：如何在零样本场景下，构建一个既有效又高效的密集检索系统，同时避免上述依赖LLM生成完整文档所带来的问题？\n\n方法 (Method)\n本文提出了一种名为 ReDE-RF (Real Document Embeddings from Relevance Feedback) 的新方法。其核心思想非常巧妙：不再让LLM“创作”内容，而是让它扮演“裁判”的角色。作者将任务从“生成假想文档”重新定义为“评估已有文档的相关性”，并利用这个相关性反馈来优化查询。\nReDE-RF的完整流程如下图所示，可以分为四个主要步骤：\n\n\n图例解读：上图清晰地展示了整个流程。一个关于“史上最伟大的NBA球员”的查询，首先通过一个基础的混合检索系统（BM25 + Contriever）召回初步的文档列表（Doc 1, Doc 2, Doc 3…）。然后，ReDE-RF模块中的LLM逐一判断这些文档是否相关。例如，LLM判断Doc 1不相关，而Doc 2和Doc 3相关。接着，系统只使用被判断为“相关”的文档（Doc 2, Doc 3）的向量来更新原始查询向量。最后，用这个更新后的、更精确的查询向量去Contriever索引中进行最终的相似度搜索，得到排序更优的最终结果列表（如“勒布朗·詹姆斯”、“迈克尔·乔丹”、“科比·布莱恩特”等）。\n\n下面是每个步骤的详细技术解读：\n1. 初步检索 (Initial Retrieval)\n对于给定的用户查询q，首先使用一个完全无监督的混合检索系统（如传统的BM25稀疏检索 + Contriever稠密检索）从大规模文档库中召回一个初步的候选文档集D（例如，Top-20的文档）。这一步的目的是快速筛选出一批可能相关的文档，为后续LLM的精选提供素材。\n2. LLM相关性反馈 (Relevance Feedback with LLMs)\n这是ReDE-RF的核心。系统会遍历上一步召回的每一个候选文档d_i∈Dd\\_i \\in Dd_i∈D，并使用一个特定的提示（Prompt）去请求LLM（被称为LLMRel−JudgeLLM_{Rel-Judge}LLMRel−Judge​）判断该文档与原始查询q的相关性。这个提示要求LLM只输出一个token：“1”代表相关，“0”代表不相关。\n这个过程极大地提升了效率，因为LLM不再需要生成长篇大论，只需进行一个简单的分类判断。最终，LLM会筛选出一个被认为是相关的文档子集 Dr=dr1,dr2,...,drk∗D_r = {d_{r_1}, d_{r_2}, ..., d_{r_{k^*}}}Dr​=dr1​​,dr2​​,...,drk∗​​，其中 k∗k^*k∗ 是相关文档的数量。\n3. 更新查询表示 (Updating the Query Representation)\n在获得相关文档集 DrD_rDr​ 后，ReDE-RF会用这些真实文档的向量来优化原始的查询向量。这一步的数学公式如下：\nv^qReDE=1k∗+1(f(q)+∑i=1k∗CE[dri])\\hat{v}_{q_{ReDE}} = \\frac{1}{k^*+1} \\left( f(q) + \\sum_{i=1}^{k^*} C_E[d_{r_i}] \\right)\nv^qReDE​​=k∗+11​(f(q)+i=1∑k∗​CE​[dri​​])\n\n公式解读：\n\nf(q)f(q)f(q) 是原始查询q经过Contriever编码器得到的初始查询向量。\ndrid_{r_i}dri​​ 是被LLM判断为相关的第i个真实文档。\nCE[dri]C_E[d_{r_i}]CE​[dri​​] 表示从预先计算好的文档向量索引库中，直接提取文档 drid_{r_i}dri​​ 的向量。这是一个非常高效的操作，因为所有文档的向量在建索引时已经离线计算好了。\n整个公式的含义是：将原始查询向量与所有被LLM认证为相关的真实文档向量进行平均，从而生成一个全新的、信息更丰富、更贴近语料库内容分布的查询向量 v^qReDE\\hat{v}_{q_{ReDE}}v^qReDE​​。\n\n\n\n4. 最终检索 (Final Retrieval)\n使用这个优化后的查询向量 v^qReDE\\hat{v}_{q_{ReDE}}v^qReDE​​，在Contriever的文档向量索引中进行最终的ANN（近似最近邻）搜索，得到最终的排序结果。\n特殊情况处理：如果在初步检索的文档中，LLM没有找到任何相关的文档（即 DrD_rDr​ 为空集），ReDE-RF提供了两种备选（Default）策略：\n\n退回至Contriever：直接使用原始查询向量 f(q)f(q)f(q) 进行检索。\n退回至HyDEPRFHyDE_{PRF}HyDEPRF​：在这种困难情况下，才启用计算成本较高的HyDE方法（使用初步召回的文档作为上下文）来生成假想文档进行检索。\n\n方法蒸馏 (DistillReDE)\n为了进一步解决LLM在推理时带来的延迟问题，论文还提出了一种名为DistillReDE的蒸馏方法。其目标是将ReDE-RF的强大性能“蒸馏”到一个更小的、无需LLM参与的Contriever模型中。具体做法是：\n\n离线使用大量合成查询，并通过完整的ReDE-RF流程计算出最优的查询向量。\n将这些最优向量作为“教师”信号，来微调一个标准的Contriever查询编码器。\n优势在于，这个过程只更新查询编码器，无需重新为整个语料库建立文档向量索引，大大节约了成本。\n\n\nBaseline\n为了全面评估ReDE-RF的性能，论文选取了三类基线模型进行对比：\n\n\n无监督检索模型 (不使用LLM)\n\nBM25: 经典的稀疏检索模型。\nContriever: SOTA的无监督稠密检索模型。\nHybrid (BM25 + Contriever): 结合稀疏和稠密的混合模型。\nContriever AvgPRF: 一种伪相关性反馈方法，它不经过LLM筛选，直接将初步召回的所有文档向量进行平均来更新查询。\n\n\n\n零样本密集检索模型 (使用LLM且无需训练)\n\nHyDE: 核心对比对象，使用LLM生成假想文档。\nHyDEPRFHyDE_{PRF}HyDEPRF​: HyDE的变种，在生成假想文档时，将初步召回的文档作为上下文。\nPromptReps: 另一种利用LLM进行零样本检索的方法，它通过提示LLM生成一个描述性token来代表查询和文档。\n\n\n\n有监督密集检索模型 (使用标注数据进行微调)\n\nDPR, ANCE, ContrieverFT: 这些是经典的、在大量标注数据上训练过的模型，用于展示零样本方法与有监督方法之间的性能差距。\n\n\n\n\n数据集 (Datasets)\n实验在两大类共9个公开数据集上进行，覆盖了不同领域和资源量。评价指标统一使用 NDCG@10。\n\n\n高资源网页搜索数据集:\n\nTREC DL19 和 TREC DL20: 来自TREC深度学习赛道的标准测试集。\n\n\n\n低资源BEIR基准数据集:\n\n新闻检索: TREC-News, Robust04\n金融问答: FiQA\n实体检索: DBpedia\n生物医学: TREC-Covid, NFCorpus\n事实核查: SciFact\n\n\n\n\n可复现性 (Reproducibility)\n论文提供了非常详细的实现细节，可复现性较高。\n\n代码和框架: 基于公开的 Pyserini 和 HuggingFace Transformers 库实现。\n模型:\n\n核心LLM (LLMRel−JudgeLLM_{Rel-Judge}LLMRel−Judge​) 使用的是 Mistral-7B-Instruct-v0.2。\n稠密检索器使用的是 facebook/contriever。\n论文还在消融实验中测试了Mixtral、Gemma、Llama等多种不同大小的开源LLM。\n\n\n算力: 实验在单张A100 GPU上完成。论文还详细报告了不同方法的平均查询延迟，这对于评估实际部署的可行性至关重要。\n超参数和提示: 论文附录中给出了复现HyDE和ReDE-RF所使用的全部提示语（Prompts），以及蒸馏过程的训练参数（如学习率、批大小等），为复现工作提供了便利。\n\n\n可改进的几个点 (Potential Improvements)\n论文在局限性（Limitations）部分诚实地指出了几个未来可以改进的方向：\n\n对初步检索的依赖：ReDE-RF的性能上限受限于初步检索的结果。如果第一阶段召回的文档质量很差，完全不包含相关文档，那么LLM也无能为力。未来的工作可以研究如何使模型对第一阶段的检索结果不那么敏感，或者设计一种动态机制，当顶层结果不佳时自动扩大检索范围。\n对LLM判断准确性的依赖：虽然LLM做判断题比做问答题简单，但它仍然可能出错。一个错误的“相关”判断可能会引入噪声，污染查询向量。如何设计更鲁棒的机制来处理或过滤LLM的潜在错误判断，是一个值得探索的方向。\n延迟问题：尽管ReDE-RF比HyDE快得多，但在推理时引入LLM仍然比纯粹的稠密检索要慢。论文提出的蒸馏方法DistillReDE是一个很好的离线解决方案。对于在线服务，可以进一步探索使用更小、更专用的模型（例如，专门为相关性判断任务微调的小模型）来替代通用的7B大模型，以实现性能和速度的极致平衡。\n与重排序（Reranking）的结合：实验表明，ReDE-RF（优化召回）和LLM重排序（优化排序）是互补的。可以设计一个更一体化的框架，将两者有机结合，而不是作为两个独立的串联步骤，可能会带来更大的提升。\n\n\n可以被引用的一些结论 (Citable Conclusions)\n这篇论文提供了多个有价值的、可供引用的发现和结论：\n\n“判断”优于“生成”：对于零样本密集检索任务，将LLM的角色从“内容生成者”转变为“相关性裁判”，不仅在低资源领域显著提升了检索效果（最高提升6%-14%），而且大幅降低了查询延迟（最高提速11倍）。\n真实文档向量更可靠：使用语料库中真实文档的向量来增强查询，可以使查询表示更“接地气”，更好地适应特定领域的知识和语言风格，避免了生成式模型可能带来的领域知识缺乏或内容幻觉问题。\nLLM相关性反馈的有效性：简单地将初步召回的文档向量全部平均（AvgPRF）并不能提升性能，这证明了使用LLM进行精准筛选，剔除不相关文档是ReDE-RF成功的关键。\n性能可被有效蒸馏：ReDE-RF的性能增益可以被成功地“蒸馏”到一个标准的稠密检索模型中，从而在不牺牲太多性能的前提下，完全移除推理阶段对LLM的依赖，实现高效部署。\n召回优化与排序优化的互补性：ReDE-RF旨在提升第一阶段召回的候选项质量（能找到更多好结果），而LLM重排序则专注于优化排序（把最好的结果排到最前面）。两者功能不同但目标一致，结合使用效果更佳。\n提示工程的重要性：在设计用于相关性判断的提示时，明确地给出“相关性”的定义（例如，“文档是否能直接回答查询”）比模糊的提问能带来更好的性能。\n\n","categories":["paper"],"tags":["paper","Retrieval","Embedding","Dense Retrieval","zero-shot"]},{"title":"Maximum Flow","url":"/DataStru-Algo/Maximum-Flow/","content":"\n免责声明：code太难写了，本章就不提供code了，各位自行GPT吧（\n\n基本概念阐述\n问题阐述\n\n输入：一个有向有权图 G=(V,E)G=(V,E)G=(V,E) ，源节点 sss ，汇点 ttt​\n目标：从 sss 发送尽可能多的水到 ttt​\n约束：\n\n流要小于管道容量\n\n\n\n流网络和流\n\n\n流网络\n\nG=(V,E)G=(V,E)G=(V,E)​ 是一个有向有权图\n\n∣E∣≥∣V∣−1|E|\\ge |V|-1∣E∣≥∣V∣−1\n\n\n图中每一条边 (u,v)∈E(u,v)\\in E(u,v)∈E 有一个非负的容量值 c(u,v)≥0c(u,v)\\ge 0c(u,v)≥0\n\n若 (u,v)∉E(u,v)\\notin E(u,v)∈/E ，则定义 c(u,v)=0c(u,v)=0c(u,v)=0\n\n\n源节点 sss ，汇点 ttt\n\n\n\n流 flowflowflow\n\n\n在一个流网络 G=(V,E)G=(V,E)G=(V,E) 中，设容量函数为 ccc ，源节点 sss ，汇点 ttt\n\n\nGGG 中的流为一个实值函数 f:V×V→Rf:V \\times V \\to Rf:V×V→R ，满足以下两条性质​：\n\n容量限制：对于所有的节点，0≤f(u,v)≤c(u,v)0\\le f(u,v)\\le c(u,v)0≤f(u,v)≤c(u,v) ​\n流量守恒：对于除源节点、汇点外（V−{s,t}V-\\{s,t\\}V−{s,t}）的所有节点，∑v∈Vf(v,u)=∑v∈Vf(u,v)\\sum_{v \\in V}f(v,u)=\\sum_{v \\in V} f(u,v)∑v∈V​f(v,u)=∑v∈V​f(u,v)​\n\n即流入 uuu 的总流量等于从 uuu​ 流出的总流量\n前后两个 vvv 一般是不同的，所以我更倾向写成 ∑v∈Vf(v,u)=∑v′∈Vf(u,v′)\\sum_{v \\in V}f(v,u)=\\sum_{v&#x27; \\in V} f(u,v&#x27;)∑v∈V​f(v,u)=∑v′∈V​f(u,v′)\n当 (u,v)∉E(u,v)\\notin E(u,v)∈/E ，则 f(u,v)=0f(u,v)=0f(u,v)=0​\n\n\n\n\n\n一个流的值 ∣f∣|f|∣f∣ 定义为：\n∣f∣=∑v∈Vf(s,v)−∑v∈Vf(v,s)|f|=\\sum_{v\\in V}f(s,v)-\\sum_{v \\in V}f(v,s)\n∣f∣=v∈V∑​f(s,v)−v∈V∑​f(v,s)\n\n表示从源结点流入该节点的总流量减去流入源结点的总流量。\n一般而言，没有流入源节点的流，即第二项的值为 000​ ，所以也可以写成：\n\n\n\n∣f∣=∑v∈Vf(s,v)|f|=\\sum_{v\\in V}f(s,v)\n∣f∣=v∈V∑​f(s,v)\n\n\n\n\n\n\n残存网络 Residual GraphResidual\\ GraphResidual Graph​\n\n\n残存边 (u,v)(u,v)(u,v) ，残存容量 cf=c(u,v)−f(u,v)c_f=c(u,v)-f(u,v)cf​=c(u,v)−f(u,v)​​​\n回流边 (v,u)(v,u)(v,u) ，回流值大小 cf′=f′(v,u)=f(u,v)c_{f&#x27;}=f&#x27;(v,u)=f(u,v)cf′​=f′(v,u)=f(u,v) ——在 Ford−FulkersonFord-FulkersonFord−Fulkerson 方法里会用到\n\ncf′=∑f′(v,u)c_{f&#x27;}=\\sum f&#x27;(v,u)cf′​=∑f′(v,u) ，也即合并 MergeMergeMerge 操作，后面会给出证明\n\n\n\n增广路径 Augmenting pathsAugmenting\\ pathsAugmenting paths\n\n\n增广路径 ppp 是从源节点 sss 到汇点 ttt 的一条简单路径\n路径 ppp 的瓶颈容量 cf(p)=min{cf(u,v):(u,v)∈p}c_f(p)=min\\{c_f(u,v):(u,v)\\in p\\}cf​(p)=min{cf​(u,v):(u,v)∈p}​\n\n切割\n切割，字面意思，就是把图切割成 nnn 份。在本章中，我们主要讨论的是 S−T cut\\mathcal{S-T}\\ cutS−T cut ，即把图切割成两份。\n\n\nS−T cut\\mathcal{S-T}\\ cutS−T cut\n\n\n将节点集合 VVV 切割成两个子集：S\\mathcal{S}S 和 T\\mathcal{T}T​\n\nS∪T=V\\mathcal{S} \\cup \\mathcal{T}=VS∪T=V ，S∩T=∅\\mathcal{S} \\cap \\mathcal{T}=\\emptysetS∩T=∅\ns∈Ss \\in \\mathcal{S}s∈S ， t∈Tt \\in \\mathcal{T}t∈T​\n\n\n\nS\\mathcal{S}S 和 T\\mathcal{T}T 的二元组 (S,S)(\\mathcal{S},\\mathcal{S})(S,S) 被称为 S−T cut\\mathcal{S-T}\\ cutS−T cut​\n\n\n定义 S−T\\mathcal{S-T}S−T 的容量为离开集合 S\\mathcal{S}S​ 的边的权重之和，如下：\nc(S,T)=∑u∈S∑v∈Tc(u,v)c(\\mathcal{S},\\mathcal{T})=\\sum_{u \\in \\mathcal{S}}\\sum_{v \\in \\mathcal{T}}c(u,v)\nc(S,T)=u∈S∑​v∈T∑​c(u,v)\n\n例如下图的 c(S,T)=6c(\\mathcal{S},\\mathcal{T})=6c(S,T)=6\n\n\n\n定义切割的净流量 f(S,T)f(\\mathcal{S},\\mathcal{T})f(S,T) 如下：\nf(S,T)=∑u∈S∑v∈Tf(u,v)−∑u∈S∑v∈Tf(v,u)f(\\mathcal{S},\\mathcal{T})=\\sum_{u \\in \\mathcal{S}}\\sum_{v \\in \\mathcal{T}}f(u,v)-\\sum_{u \\in \\mathcal{S}}\\sum_{v \\in \\mathcal{T}}f(v,u)\nf(S,T)=u∈S∑​v∈T∑​f(u,v)−u∈S∑​v∈T∑​f(v,u)\n\n例如下图的 f(S,T)=f(v1,v3)+f(v1,v4)+f(v2,v4)−f(v4,s)=2+2+2−4=2f(\\mathcal{S},\\mathcal{T})=f(v_1,v_3)+f(v_1,v_4)+f(v_2,v_4)-f(v_4,s)=2+2+2-4=2f(S,T)=f(v1​,v3​)+f(v1​,v4​)+f(v2​,v4​)−f(v4​,s)=2+2+2−4=2\n\n\n\ne.g.\n\n\n \n     \n     \n \n\n注意，S−T cut\\mathcal{S-T}\\ cutS−T cut 不唯一\n\n \n     \n \n\n\n最小割 Min−CutMin-CutMin−Cut\n\n使容量最小化的切割被称为最小割 Min−CutMin-CutMin−Cut\ne.g.\n\n \n     \n     \n \n\n\nNaive AlgorithmNaive\\ AlgorithmNaive Algorithm 引入\n\nNaive 不是一个人名，它的意思是天真的；幼稚的。这里应译为朴素算法\n\n​\t这个算法是初步的算法，并不一定能找到最大流（只能找到阻塞流），只是大多数情况下可以找到最大流，但是这种方法很好理解，并且后面的更优的算法都是以此为基础进行优化的，所以我们先介绍这种算法。\n算法实现步骤\n\n创建一个残存图 (residual graph)(residual\\ graph)(residual graph) ，初始化残存容量\nwhilewhilewhile  可以找到增广路径 :\n\n找一条增广路径\n找出增广路径中的瓶颈容量 xxx\n更新这条增广路径上每一条边，cf=cf−xc_f=c_f-xcf​=cf​−x ，并删除饱和边\n\n\n\n图示\n\n\n初始化\n\n\n\n第一轮循环\n\n\n随便用什么遍历方法找到一条从 sss 到 ttt​​​ 的增广路径，找到瓶颈容量，更新这条路径上的每一条边\n  \n  \n  \n  \n\n\n此时有两条边的余量为0，说明这两条管道已经饱和了，于是把余量为0的边删除\n\n\n\n\n\n\n\n\n\n第二轮循环\n\n同样的操作\n\n\n\n\n\n\n\n\n\n第三轮循环\n\n同样的操作\n\n\n\n\n    \n    \n    \n\n\n循环结束\n\n用原始图减去最终的残存图，即可得到流量图\n实际流量可以从源节点 sss 去进行计算\n\n\n\n\n    \n    \n\n该简单算法局限性\n该算法在寻找路径的时候，如果找到的路径是错误的，则最终找到的有可能不是最大流\n\n    \n    \n\n\n\t\n\t\n\n\n    \n    \n\n\n阻塞流   blocking flowblocking\\ flowblocking flow\n\n\n如果从源节点 sss 到汇点 ttt ，不能找到更多的流汇入，则该流是阻塞流\n最大流是阻塞流\n\nFord−FulkersonFord-FulkersonFord−Fulkerson 算法\n​\t上面的简单算法一旦选择了 bad pathbad\\ pathbad path 后，不能修正，就只能找到阻塞流。\n​\t而下面我们介绍的算法，则是以上面的算法为基础进行优化的，Ford−FulkersonFord-FulkersonFord−Fulkerson 算法添加了回流这一步操作，假如选到了阻塞流，则可以进行回流修正，从而解决了问题。\n图示\n​\t我们以刚刚的基本算法的示例图来进行算法介绍\n\n依旧进行初始化\n\n\n    \n\n\n\n第一轮循环\n\n这三步和之前的 naive algorithmnaive\\ algorithmnaive algorithm​ 都是一样的，关键在于之后的一步\n\n  \n  \n  \n  \n \n - 在删除了饱和边之后，$Ford-Fulkerson\\ Algorithm$ 增加了一步回流法：画出该增广路径的回流边，即从 $t$ 到 $s$​ 的流向。并且这些回流边在下一轮选择路径试仍然可以被选中。\n- 为什么可以这么做？这样做对算法的正确性有无影响？这里先介绍完算法，后文会给出正确性证明。\n\n\n\n\n\n\n第二轮循环\n\n依旧如此，进行同样的操作\n\n  \n \t\n \t\n \t\n \n - 此时我们注意到：在 $v_1 \\to s$​ 这条路径，有两条回流边，此时我们很容易就会想到一个问题，这两条边，是否可以合并？\n    - 答案是肯定的，同样的，我们在算法正确性小节会给出证明\n    - 第二轮循环结束的图如下：\n \n\n\n第三轮循环\n\n第三轮循环即是刚刚简单算法失败的地方，我们来看 Ford−FulkersonFord-FulkersonFord−Fulkerson​ 算法是如何纠正的\n\n此图很清晰地说明了回流边的作用——给后面的纠错预留操作空间\n\n\n\n\n\n\n    \n    \n    \n    \n\n\n\n循环结束\n\n此时，图中没有从 sss 到 ttt​ 的路径了，于是算法终止。\n\n \n     \n \n\n因此，我们可以得到流量图，且该图的 Maximum FlowMaximum\\ FlowMaximum Flow 为 555 。\n\n \n     \n     \n \n\n\n算法实现步骤\n刚刚我们介绍了 Naive AlgorithmNaive\\ AlgorithmNaive Algorithm ，对于 Ford−FulkersonFord-FulkersonFord−Fulkerson 算法的步骤，也只需添加一句话：\n\n创建一个残存图 (residual graph)(residual\\ graph)(residual graph) ，初始化残存容量\nwhilewhilewhile  可以找到增广路径 :\n\n找一条增广路径\n找出增广路径中的瓶颈容量 xxx\n更新这条增广路径上每一条边，cf=cf−xc_f=c_f-xcf​=cf​−x​​ ，并删除饱和边\n==添加该增广路径的回流边==\n\n\n\n最坏时间复杂度分析\n\n\nwhilewhilewhile​​ 循环最坏情况\n​\t不难看出，该图的最大流为 200200200 ，若选择路径时，非常&quot;好运&quot;地选择了 s→v1→v2→ts \\to v_1 \\to v_2 \\to ts→v1​→v2​→t 这条路径，则会发生一些 amazingamazingamazing 的事情，即后面两张图。此时，循环就会进行 200200200 遍，即进行 Amount of MaxFlowAmount\\ of\\ MaxFlowAmount of MaxFlow 遍。\n\n\n\n    \n    \n    \n\n​\t\t以下给出证明：\n​\t\t\t由于每一轮循环中，流量值至少增加 111 ，FlowvalueFlowvalueFlowvalue 从 000 开始增长到 MaxFlowMaxFlowMaxFlow\n​\t\t\t所以，Iterations≤Amount of MaxFlowIterations \\le Amount\\ of\\ MaxFlowIterations≤Amount of MaxFlow\n​\t\t\t在这个例子中，Iterations=Amount of MaxFlowIterations = Amount\\ of\\ MaxFlowIterations=Amount of MaxFlow\n​\t\t综上所述，循环的次数为：O(f∗)O(f^*)O(f∗) ，其中 f∗f^*f∗ 代表最大流的大小。\n\n\n寻找路径最坏情况\n​\t假设有向图 GGG 中有 EEE 条边，VVV 个结点，并且所有的结点都至少有一条相邻边，则 E≥V2E \\ge \\frac{V}{2}E≥2V​\n​\t所以如果使用 bfsbfsbfs 或 dfsdfsdfs ，在一个残存网络中找到一条路径的时间可以化为：O(V+E′)=O(E)O(V+E&#x27;)=O(E)O(V+E′)=O(E)​\n\n\n最坏时间复杂度\n​\t综上所述，最坏时间复杂度为：O(Ef∗)O(Ef^*)O(Ef∗)​\n\n\n*算法正确性证明\n​\t在算法介绍中，我们得知了设置回流边的意义，现在，我们来探究为什么能设置回流边，以及我们对回流边所进行的操作，是否会对算法正确性有所影响。\n对于回流操作正确性的证明\n​\t我们知道，残存网络（residual graphresidual\\ graphresidual graph）是一个特殊的流网络，其允许反向边，也即平行边的存在。\n​\t那么对于残存网络中，这&quot;特殊&quot;的流 f′f&#x27;f′ ，我们定义 $f \\uparrow f’ $ 为流 f′f&#x27;f′ 对 fff 的递增操作，定义为：\n(f↑f′)(u,v)={f(u,v)+f′(u,v)−f′(v,u)若(u,v)∈E0其他(f\\uparrow f&#x27;)(u,v)=\\begin{cases}f(u,v)+f&#x27;(u,v)-f&#x27;(v,u) &amp;若(u,v)\\in E\n \\\\0 &amp;其他\n\\end{cases}\n(f↑f′)(u,v)={f(u,v)+f′(u,v)−f′(v,u)0​若(u,v)∈E其他​\n​\t在残存网络中将流量发送到反向边上等同于在原来的网络中缩减流量，所以将边 (u,v)(u,v)(u,v) 的流量增加 f′(u,v)f&#x27;(u,v)f′(u,v) ，但减少了 f′(v,u)f&#x27;(v,u)f′(v,u) 。在残存网络中，这种将流量推流回去也称为抵消操作（cancellationcancellationcancellation​）。\n​\n​\t对于上述操作，其满足容量限制性质以及流量守恒性质，证明如下：\n​\t\t先证明容量守恒性质，\n​\t\t\t设边 (u,v)∈E(u,v)\\in E(u,v)∈E ，则 cf(v,u)=f(u,v)c_f(v,u)=f(u,v)cf​(v,u)=f(u,v) ，且 f′(v,u)≤cf(v,u)=f(u,v)f&#x27;(v,u) \\le c_f(v,u)=f(u,v)f′(v,u)≤cf​(v,u)=f(u,v) ，因此有，\n(f↑f′)(u,v)=f(u,v)+f′(u,v)−f′(v,u)≥f(u,v)+f′(u,v)−f(u,v)=f′(u,v)≥0(f↑f′)(u,v)=f(u,v)+f′(u,v)−f′(v,u)≤f(u,v)+cf(u,v)=f(u,v)+c(u,v)−f(u,v)=c(u,v)\\begin{align*}\n(f \\uparrow f&#x27;)(u,v)\n&amp;=f(u,v)+f&#x27;(u,v)-f&#x27;(v,u)\\\\\n&amp;\\ge f(u,v)+f&#x27;(u,v)-f(u,v)\\\\\n&amp;=f&#x27;(u,v)\\\\\n&amp;\\ge 0\\\\\n\\\\\n(f \\uparrow f&#x27;)(u,v)\n&amp;=f(u,v)+f&#x27;(u,v)-f&#x27;(v,u)\\\\\n&amp;\\le f(u,v)+c_f(u,v)\\\\\n&amp;=f(u,v)+c(u,v)-f(u,v)\\\\\n&amp;=c(u,v)\n\\end{align*}\n(f↑f′)(u,v)(f↑f′)(u,v)​=f(u,v)+f′(u,v)−f′(v,u)≥f(u,v)+f′(u,v)−f(u,v)=f′(u,v)≥0=f(u,v)+f′(u,v)−f′(v,u)≤f(u,v)+cf​(u,v)=f(u,v)+c(u,v)−f(u,v)=c(u,v)​\n​\t\t\t所以，0≤(f↑f′)(u,v)≤c(u,v)0 \\le (f \\uparrow f&#x27;)(u,v) \\le c(u,v)0≤(f↑f′)(u,v)≤c(u,v) ，即该操作满足容量限制性质。\n​\t\t再证明流量守恒性质，\n​\t\t\t因为 fff 和 f′f&#x27;f′ 都遵循流量守恒性质，所以对于所有的节点 u∈V−{s,t}u \\in V-\\{s,t\\}u∈V−{s,t} ，有，\n∑v∈V(f↑f′)(u,v)=∑v∈V(f(u,v)+f′(u,v)−f′(v,u))=∑v∈Vf(u,v)+∑v∈Vf′(u,v)−∑v∈Vf′(v,u)=∑v∈Vf(v,u)+∑v∈Vf′(v,u)−∑v∈Vf′(u,v)=∑v∈V(f(v,u)+f′(v,u)−f′(u,v))=∑v∈V(f↑f′)(v,u)\\begin{align*}\n\\sum_{v \\in V}(f \\uparrow f&#x27;)(u,v)\n&amp;=\\sum_{v \\in V}(f(u,v)+f&#x27;(u,v)-f&#x27;(v,u))\\\\\n&amp;=\\sum_{v \\in V}f(u,v)+\\sum_{v \\in V}f&#x27;(u,v)-\\sum_{v \\in V}f&#x27;(v,u)\\\\\n&amp;=\\sum_{v \\in V}f(v,u)+\\sum_{v \\in V}f&#x27;(v,u)-\\sum_{v \\in V}f&#x27;(u,v)\\\\\n&amp;=\\sum_{v \\in V}(f(v,u)+f&#x27;(v,u)-f&#x27;(u,v))\\\\\n&amp;=\\sum_{v \\in V}(f \\uparrow f&#x27;)(v,u)\\\\\n\\end{align*}\nv∈V∑​(f↑f′)(u,v)​=v∈V∑​(f(u,v)+f′(u,v)−f′(v,u))=v∈V∑​f(u,v)+v∈V∑​f′(u,v)−v∈V∑​f′(v,u)=v∈V∑​f(v,u)+v∈V∑​f′(v,u)−v∈V∑​f′(u,v)=v∈V∑​(f(v,u)+f′(v,u)−f′(u,v))=v∈V∑​(f↑f′)(v,u)​\n​\t\t\t其中第二行推导到第三行使用了 fff 和 f′f&#x27;f′ 的流量守恒性质。\n​\t\t\t所以，∑v∈V(f↑f′)(u,v)=∑v∈V(f↑f′)(v,u)\\sum_{v \\in V}(f \\uparrow f&#x27;)(u,v)=\\sum_{v \\in V}(f \\uparrow f&#x27;)(v,u)∑v∈V​(f↑f′)(u,v)=∑v∈V​(f↑f′)(v,u)​ ，即该操作满足流量守恒性质。\n​\t\t因此，对于 f↑f′f\\uparrow f&#x27;f↑f′​ 这个操作，其满足容量限制性质以及流量守恒性质。\n​\t所以回流操作并不会影响到流的基本性质，所以这个操作是正确的。\n对于函数 ∣f↑f′∣|f \\uparrow f&#x27;|∣f↑f′∣ 值大小的证明\n​\t那么如果根据流的值的定义，不妨猜想 ∣f↑f′∣=∣f∣+∣f′∣|f \\uparrow f&#x27;|=|f|+|f&#x27;|∣f↑f′∣=∣f∣+∣f′∣​ ，证明如下：\n​\t\t根据定义，有：\n∣f↑f′∣=∑v∈V(f↑f′)(s,v)−∑v∈V(f↑f′)(v,s)|f \\uparrow f&#x27;|=\\sum_{v\\in V}(f \\uparrow f&#x27;)(s,v)-\\sum_{v \\in V}(f \\uparrow f&#x27;)(v,s)\n∣f↑f′∣=v∈V∑​(f↑f′)(s,v)−v∈V∑​(f↑f′)(v,s)\n​\t\t因为对于每个节点 v∈Vv \\in Vv∈V ，可以有边 (s,v)(s,v)(s,v) 或 (v,s)(v,s)(v,s) ，但是二者不允许同时存在。\n​\t\t因此我们定义 V1={v:(s,v)∈E}V_1=\\{v:(s,v) \\in E\\}V1​={v:(s,v)∈E} 为有边从源节点到达的节点集合，V2={v:(v,s)∈E}V_2=\\{v:(v,s) \\in E\\}V2​={v:(v,s)∈E} 为有边通往源节点的节点集合。我们有 V1∪V2⊆VV_1 \\cup V_2 \\subseteq VV1​∪V2​⊆V 且 V1∩V2=∅V_1 \\cap V_2 = \\emptysetV1​∩V2​=∅ 。\n​\t\t所以上式可化为：\n∣f↑f′∣=∑v∈V(f↑f′)(s,v)−∑v∈V(f↑f′)(v,s)=∑v∈V1(f↑f′)(s,v)−∑v∈V2(f↑f′)(v,s)=∑v∈V1(f(s,v)+f′(s,v)−f′(v,s))−∑v∈V2(f(v,s)+f′(v,s)−f′(s,v))=∑v∈V1f(s,v)+∑v∈V1f′(s,v)−∑v∈V1f′(v,s)−∑v∈V2f(v,s)−∑v∈V2f′(v,s)+∑v∈V2f′(s,v)=∑v∈V1f(s,v)−∑v∈V2f(v,s)+∑v∈V1∪V2f′(s,v)−∑v∈V1∪V2f′(v,s)\\begin{align*}\n|f \\uparrow f&#x27;|\n&amp;=\\sum_{v\\in V}(f \\uparrow f&#x27;)(s,v)-\\sum_{v \\in V}(f \\uparrow f&#x27;)(v,s)\\\\\n&amp;=\\sum_{v\\in V_1}(f \\uparrow f&#x27;)(s,v)-\\sum_{v \\in V_2}(f \\uparrow f&#x27;)(v,s)\\\\\n&amp;=\\sum_{v\\in V_1}(f(s,v)+f&#x27;(s,v)-f&#x27;(v,s))-\\sum_{v\\in V_2}(f(v,s)+f&#x27;(v,s)-f&#x27;(s,v))\\\\\n&amp;=\\sum_{v\\in V_1}f(s,v)+\\sum_{v\\in V_1}f&#x27;(s,v)-\\sum_{v\\in V_1}f&#x27;(v,s)-\\sum_{v\\in V_2}f(v,s)-\\sum_{v\\in V_2}f&#x27;(v,s)+\\sum_{v\\in V_2}f&#x27;(s,v)\\\\\n&amp;=\\sum_{v\\in V_1}f(s,v)-\\sum_{v\\in V_2}f(v,s)+\\sum_{v \\in V_1 \\cup V_2}f&#x27;(s,v)-\\sum_{v \\in V_1 \\cup V_2}f&#x27;(v,s)\\\\\n\\end{align*}\n∣f↑f′∣​=v∈V∑​(f↑f′)(s,v)−v∈V∑​(f↑f′)(v,s)=v∈V1​∑​(f↑f′)(s,v)−v∈V2​∑​(f↑f′)(v,s)=v∈V1​∑​(f(s,v)+f′(s,v)−f′(v,s))−v∈V2​∑​(f(v,s)+f′(v,s)−f′(s,v))=v∈V1​∑​f(s,v)+v∈V1​∑​f′(s,v)−v∈V1​∑​f′(v,s)−v∈V2​∑​f(v,s)−v∈V2​∑​f′(v,s)+v∈V2​∑​f′(s,v)=v∈V1​∑​f(s,v)−v∈V2​∑​f(v,s)+v∈V1​∪V2​∑​f′(s,v)−v∈V1​∪V2​∑​f′(v,s)​\n​\t\t又因为当 v∈V1v \\in V_1v∈V1​ 时，对于在集合 V2V_2V2​ 的边，vvv 对应的流 fff 为 000 ，即每一个额外的项都为 000 ，反之亦然。\n​\t\t所以可以将 V1,V2,V1∪V2V_1,V_2,V_1\\cup V_2V1​,V2​,V1​∪V2​ 扩展到整个节点范围 VVV 中。\n​\t\t因此：\n∣f↑f′∣=∑v∈V1f(s,v)−∑v∈V2f(v,s)+∑v∈V1∪V2f′(s,v)−∑v∈V1∪V2f′(v,s)=∑v∈Vf(s,v)−∑v∈Vf(v,s)+∑v∈Vf′(s,v)−∑v∈Vf′(v,s)=∣f∣+∣f′∣\\begin{align*}\n|f \\uparrow f&#x27;|\n&amp;=\\sum_{v\\in V_1}f(s,v)-\\sum_{v\\in V_2}f(v,s)+\\sum_{v \\in V_1 \\cup V_2}f&#x27;(s,v)-\\sum_{v \\in V_1 \\cup V_2}f&#x27;(v,s)\\\\\n&amp;=\\sum_{v\\in V}f(s,v)-\\sum_{v\\in V}f(v,s)+\\sum_{v \\in V}f&#x27;(s,v)-\\sum_{v \\in V}f&#x27;(v,s)\\\\\n&amp;=|f|+|f&#x27;|\\\\\n\\end{align*}\n∣f↑f′∣​=v∈V1​∑​f(s,v)−v∈V2​∑​f(v,s)+v∈V1​∪V2​∑​f′(s,v)−v∈V1​∪V2​∑​f′(v,s)=v∈V∑​f(s,v)−v∈V∑​f(v,s)+v∈V∑​f′(s,v)−v∈V∑​f′(v,s)=∣f∣+∣f′∣​\n​\t\t即：∣f↑f′∣=∣f∣+∣f′∣|f \\uparrow f&#x27;|=|f|+|f&#x27;|∣f↑f′∣=∣f∣+∣f′∣\n对于合并操作正确性的证明\n​\t我们设 G=(V,E)G=(V,E)G=(V,E) 为一个流网络，设 fff 为 GGG 中的一个流，设 ppp 为残存网络 GfG_fGf​ 中的一条增广路径。假定将 fff 增加 fpf_pfp​ 的量，则函数 f↑fpf \\uparrow f_pf↑fp​ 是图 GGG 中的一个流，由上述证明可知其值为 ∣f↑fp∣=∣f∣+∣fp∣|f \\uparrow f_p|=|f|+|f_p|∣f↑fp​∣=∣f∣+∣fp​∣ 。\n​\t又因为合并的操作视为 ∣fp1↑fp2∣|f_{p_1} \\uparrow f_{p_2}|∣fp1​​↑fp2​​∣ ，所以合并后的值为 ∣fp1↑fp2∣=∣fp1∣+∣fp2∣|f_{p_1} \\uparrow f_{p_2}|=|f_{p_1}|+|f_{p_2}|∣fp1​​↑fp2​​∣=∣fp1​​∣+∣fp2​​∣ ，即合并操作是合理的。\n最大流最小切割定理的证明\n​\t上述证明解释了为什么我们可以进行回流、合并（MergeMergeMerge） 这些操作。那么接下来，我们来证明为什么最后得到的流一定是最大流。该证明也是对最大流最小切割定理的证明。\n前置证明准备\n​\t我们首先证明对于网络中的一个流以及任意切割 (S,T)(\\mathcal{S},\\mathcal{T})(S,T) ，其净流量 f(S,T)=∣f∣f(\\mathcal{S},\\mathcal{T})=|f|f(S,T)=∣f∣ ：\n​\t\t对于任意节点 u∈V−{s,t}u \\in V-\\{s,t\\}u∈V−{s,t} ，其流量守恒定律可改写成：\n∑v∈Vf(v,u)−∑v∈Vf(u,v)=0①\\sum_{v \\in V}f(v,u)-\\sum_{v \\in V} f(u,v)=0 \\quad\\quad①\nv∈V∑​f(v,u)−v∈V∑​f(u,v)=0①\n​\t\t根据 ∣f∣|f|∣f∣ 的定义：∣f∣=∑v∈Vf(s,v)−∑v∈Vf(v,s)|f|=\\sum_{v\\in V}f(s,v)-\\sum_{v \\in V}f(v,s)∣f∣=∑v∈V​f(s,v)−∑v∈V​f(v,s) ，我们得到：\n∣f∣=∑v∈Vf(s,v)−∑v∈Vf(v,s)②|f|=\\sum_{v\\in V}f(s,v)-\\sum_{v \\in V}f(v,s) \\quad\\quad②\n∣f∣=v∈V∑​f(s,v)−v∈V∑​f(v,s)②\n​\t\t将 ①①① 式针对所有节点 S−{s}\\mathcal{S}-\\{s\\}S−{s} 求和并加到 ②②② 中，得到：\n∣f∣=∑v∈Vf(s,v)−∑v∈Vf(v,s)+∑v∈S−{s}(∑v∈Vf(u,v)−∑v∈Vf(v,u))=∑v∈Vf(s,v)−∑v∈Vf(v,s)+∑v∈S−{s}∑v∈Vf(u,v)−∑v∈S−{s}∑v∈Vf(v,u)=∑v∈V(f(s,v)+∑v∈S−{s}f(u,v))−∑v∈V(f(v,s)+∑v∈S−{s}f(v,u))=∑v∈V∑u∈Sf(u,v)−∑v∈V∑u∈Sf(v,u)\\begin{align*}\n|f|\n&amp;=\\sum_{v\\in V}f(s,v)-\\sum_{v \\in V}f(v,s)+\\sum_{v \\in \\mathcal{S}-\\{s\\}}(\\sum_{v\\in V}f(u,v)-\\sum_{v \\in V}f(v,u))\\\\\n&amp;=\\sum_{v\\in V}f(s,v)-\\sum_{v \\in V}f(v,s)+\\sum_{v \\in \\mathcal{S}-\\{s\\}}\\sum_{v\\in V}f(u,v)-\\sum_{v \\in \\mathcal{S}-\\{s\\}}\\sum_{v \\in V}f(v,u)\\\\\n&amp;=\\sum_{v\\in V}(f(s,v)+\\sum_{v \\in \\mathcal{S}-\\{s\\}}f(u,v))-\\sum_{v \\in V}(f(v,s)+\\sum_{v \\in \\mathcal{S}-\\{s\\}}f(v,u))\\\\\n&amp;=\\sum_{v\\in V}\\sum_{u \\in \\mathcal{S}}f(u,v)-\\sum_{v\\in V}\\sum_{u \\in \\mathcal{S}}f(v,u)\n\\end{align*}\n∣f∣​=v∈V∑​f(s,v)−v∈V∑​f(v,s)+v∈S−{s}∑​(v∈V∑​f(u,v)−v∈V∑​f(v,u))=v∈V∑​f(s,v)−v∈V∑​f(v,s)+v∈S−{s}∑​v∈V∑​f(u,v)−v∈S−{s}∑​v∈V∑​f(v,u)=v∈V∑​(f(s,v)+v∈S−{s}∑​f(u,v))−v∈V∑​(f(v,s)+v∈S−{s}∑​f(v,u))=v∈V∑​u∈S∑​f(u,v)−v∈V∑​u∈S∑​f(v,u)​\n​\t\t又因为 V=S∪TV=\\mathcal{S}\\cup\\mathcal{T}V=S∪T 并且 S∩T=∅\\mathcal{S} \\cap \\mathcal{T}=\\emptysetS∩T=∅ ，所以可以将上式的 VVV 进行分解，即：\n∣f∣=∑v∈V∑u∈Sf(u,v)−∑v∈V∑u∈Sf(v,u)=∑v∈S∑u∈Sf(u,v)−∑v∈S∑u∈Sf(v,u)+∑v∈T∑u∈Sf(u,v)−∑v∈T∑u∈Sf(v,u)=∑v∈T∑u∈Sf(u,v)−∑v∈T∑u∈Sf(v,u)+(∑v∈S∑u∈Sf(u,v)−∑v∈S∑u∈Sf(v,u))\\begin{align*}\n|f|\n&amp;=\\sum_{v\\in V}\\sum_{u \\in \\mathcal{S}}f(u,v)-\\sum_{v\\in V}\\sum_{u \\in \\mathcal{S}}f(v,u)\\\\\n&amp;=\\sum_{v\\in \\mathcal{S}}\\sum_{u \\in \\mathcal{S}}f(u,v)-\\sum_{v\\in \\mathcal{S}}\\sum_{u \\in \\mathcal{S}}f(v,u)+\\sum_{v\\in \\mathcal{T}}\\sum_{u \\in \\mathcal{S}}f(u,v)-\\sum_{v\\in \\mathcal{T}}\\sum_{u \\in \\mathcal{S}}f(v,u)\\\\\n&amp;=\\sum_{v\\in \\mathcal{T}}\\sum_{u \\in \\mathcal{S}}f(u,v)-\\sum_{v\\in \\mathcal{T}}\\sum_{u \\in \\mathcal{S}}f(v,u)+(\\sum_{v\\in \\mathcal{S}}\\sum_{u \\in \\mathcal{S}}f(u,v)-\\sum_{v\\in \\mathcal{S}}\\sum_{u \\in \\mathcal{S}}f(v,u))\\\\\n\\end{align*}\n∣f∣​=v∈V∑​u∈S∑​f(u,v)−v∈V∑​u∈S∑​f(v,u)=v∈S∑​u∈S∑​f(u,v)−v∈S∑​u∈S∑​f(v,u)+v∈T∑​u∈S∑​f(u,v)−v∈T∑​u∈S∑​f(v,u)=v∈T∑​u∈S∑​f(u,v)−v∈T∑​u∈S∑​f(v,u)+(v∈S∑​u∈S∑​f(u,v)−v∈S∑​u∈S∑​f(v,u))​\n​\t\t不难看出，上式括号内的两个求和项是一样的，所以有：\n∣f∣=∑v∈T∑u∈Sf(u,v)−∑v∈T∑u∈Sf(v,u)=f(S,T)|f|=\\sum_{v\\in \\mathcal{T}}\\sum_{u \\in \\mathcal{S}}f(u,v)-\\sum_{v\\in \\mathcal{T}}\\sum_{u \\in \\mathcal{S}}f(v,u)=f(\\mathcal{S},\\mathcal{T})\n∣f∣=v∈T∑​u∈S∑​f(u,v)−v∈T∑​u∈S∑​f(v,u)=f(S,T)\n​\t综上，净流量 f(S,T)=∣f∣f(\\mathcal{S},\\mathcal{T})=|f|f(S,T)=∣f∣​ 。\n​\t由这个性质，我们可以得到一个引理：∣f∣≤c(S,T)|f| \\le c(\\mathcal{S},\\mathcal{T})∣f∣≤c(S,T) ，证明如下：\n∣f∣=f(S,T)=∑v∈T∑u∈Sf(u,v)−∑v∈T∑u∈Sf(v,u)≤∑v∈T∑u∈Sf(u,v)≤∑v∈T∑u∈Sc(u,v)=c(S,T)\\begin{align*}\n|f|\n&amp;=f(\\mathcal{S},\\mathcal{T})=\\sum_{v\\in \\mathcal{T}}\\sum_{u \\in \\mathcal{S}}f(u,v)-\\sum_{v\\in \\mathcal{T}}\\sum_{u \\in \\mathcal{S}}f(v,u)\\\\\n&amp;\\le \\sum_{v\\in \\mathcal{T}}\\sum_{u \\in \\mathcal{S}}f(u,v) \\le \\sum_{v\\in \\mathcal{T}}\\sum_{u \\in \\mathcal{S}}c(u,v) = c(\\mathcal{S},\\mathcal{T})\n\\end{align*}\n∣f∣​=f(S,T)=v∈T∑​u∈S∑​f(u,v)−v∈T∑​u∈S∑​f(v,u)≤v∈T∑​u∈S∑​f(u,v)≤v∈T∑​u∈S∑​c(u,v)=c(S,T)​\n​\t这个引理给出的一个直接结论就是：一个流网络中最大流的值不能超过该网络最小切割的容量。\n最大流最小切割定理\n​\t那么接下来，我们就来证明最大流最小切割原理。该原理表明一个最大流的值等于一个最小切割的容量。\n​\t最大流最小切割定理：\n​\t\t(1)(1)(1) fff 是 GGG 的一个最大流\n​\t\t(2)(2)(2) 残存网络 GfG_fGf​ 不包括任何增广路径\n​\t\t(3)(3)(3) ∣f∣=c(S,T)|f|=c(\\mathcal{S},\\mathcal{T})∣f∣=c(S,T) ，其中 (S,T)(\\mathcal{S},\\mathcal{T})(S,T) 是流网络 GGG 的某个切割。即 ∣f∣max=cmin(S,T)|f|_{max}=c_{min}(\\mathcal{S},\\mathcal{T})∣f∣max​=cmin​(S,T)\n​\t\t上述的三个条件是等效的，即三个条件互为充要条件。\n​\t以下是最大流最小切割定理的证明：\n​\t\t证 (1)→(2)(1) \\to (2)(1)→(2) ：\n​\t\t\t假设 fff 是 GGG 的一个最大流，但残存网络 GfG_fGf​ 同时存在一条增广路径 ppp 。如果我们对 fff 增加流量 fpf_pfp​ ，那么 ∣f↑fp∣=∣f∣+∣fp∣&gt;∣f∣|f \\uparrow f_p|=|f|+|f_p| &gt; |f|∣f↑fp​∣=∣f∣+∣fp​∣&gt;∣f∣ ，与 fff 是最大流矛盾。\n​\t\t证 (2)→(3)(2) \\to (3)(2)→(3) ：\n​\t\t\t对于节点 u∈Su \\in \\mathcal{S}u∈S 和 v∈Tv \\in \\mathcal{T}v∈T ，如果边 (u,v)∈E(u,v) \\in E(u,v)∈E ，则必有 f(u,v)=c(u,v)f(u,v)=c(u,v)f(u,v)=c(u,v) ，否则 (u,v)∈Ef(u,v) \\in E_f(u,v)∈Ef​ ，上述操作会将 vvv 置于集合 S\\mathcal{S}S 中。如果边 (v,u)∈E(v,u) \\in E(v,u)∈E ，则必有 f(v,u)=0f(v,u)=0f(v,u)=0 ，否则 cf(u,v)=f(v,u)c_f(u,v)=f(v,u)cf​(u,v)=f(v,u) 将为正值，边 (u,v)(u,v)(u,v) 将属于 EfE_fEf​ ，上述操作会将节点 vvv 置于集合 S\\mathcal{S}S 中。当边 (u,v)(u,v)(u,v) 和 (v,u)(v,u)(v,u) 都不在 EEE 中，则 f(u,v)=f(v,u)=0f(u,v)=f(v,u)=0f(u,v)=f(v,u)=0 。因此：\nf(S,T)=∑v∈T∑u∈Sf(u,v)−∑v∈T∑u∈Sf(v,u)=∑v∈T∑u∈Sc(u,v)−∑v∈T∑u∈S0=c(S,T)f(\\mathcal{S},\\mathcal{T})=\\sum_{v\\in \\mathcal{T}}\\sum_{u \\in \\mathcal{S}}f(u,v)-\\sum_{v\\in \\mathcal{T}}\\sum_{u \\in \\mathcal{S}}f(v,u)=\\sum_{v\\in \\mathcal{T}}\\sum_{u \\in \\mathcal{S}}c(u,v)-\\sum_{v\\in \\mathcal{T}}\\sum_{u \\in \\mathcal{S}}0=c(\\mathcal{S},\\mathcal{T})\nf(S,T)=v∈T∑​u∈S∑​f(u,v)−v∈T∑​u∈S∑​f(v,u)=v∈T∑​u∈S∑​c(u,v)−v∈T∑​u∈S∑​0=c(S,T)\n​\t\t证 (3)→(1)(3) \\to (1)(3)→(1) ：\n​\t\t\t由 ∣f∣≤c(S,T)|f| \\le c(\\mathcal{S},\\mathcal{T})∣f∣≤c(S,T) 即可说明 fff​ 是一个最大流,。\n​\t综上所述，Ford−FulkersonFord-FulkersonFord−Fulkerson 算法是正确的。\nEdmonds−KarpEdmonds-KarpEdmonds−Karp​ 算法\n​\t简单来说，Edmonds−KarpEdmonds-KarpEdmonds−Karp 算法是 Ford−FulkersonFord-FulkersonFord−Fulkerson 算法的一种特殊情况，全部流程跟 Ford−FulkersonFord-FulkersonFord−Fulkerson 算法基本一致。\n​\t唯一的区别是 Edmonds−KarpEdmonds-KarpEdmonds−Karp 算法在寻找增广路径中，使用寻找无权图最短路径的算法，在残存网络中将所有边的权值视为 111​ ，由此来寻找从源节点到汇点的最短路径。\n算法实现步骤\n参照 Ford−FulkersonFord-FulkersonFord−Fulkerson 算法的步骤，对于 Edmonds−KarpEdmonds-KarpEdmonds−Karp 算法的实现步骤只需修改一句话：\n\n创建一个残存图 (residual graph)(residual\\ graph)(residual graph) ，初始化残存容量\nwhilewhilewhile  可以找到增广路径 :\n\n==找到最短的增广路径==\n找出增广路径中的瓶颈容量 xxx\n更新这条增广路径上每一条边，cf=cf−xc_f=c_f-xcf​=cf​−x​​ ，并删除饱和边\n添加该增广路径的回流边\n\n\n\n时间复杂度分析\n对于寻找最短的增广路径，我们可以使用类 BFSBFSBFS 的算法。\n因此寻找最短路径的时间复杂度为：O(V+E)=O(E)O(V+E)=O(E)O(V+E)=O(E)​\n然后循环最多实现 O(VE)O(VE)O(VE) 次~~（这里就不证明了吧 详细证明看算法导论）~~\n因此时间复杂度为：O(VE2)O(VE^2)O(VE2)\n*DinicDinicDinic 算法\n​\t下面我们来介绍一个时间复杂度更低，达到了 O(V2E)O(V^2E)O(V2E) 的算法—— DinicDinicDinic 算法。\nLevel GraphLevel\\ GraphLevel Graph ​\n​\tDinicDinicDinic 算法需要用到一个新东西：Level GraphLevel\\ GraphLevel Graph 。Level GraphLevel\\ GraphLevel Graph 基于 BFSBFSBFS 实现，是一个有权无向图。其随着层数记录每一层的节点，最终到达汇点 ttt 。\n\ne.g.\n\n\n    \n    \n\n算法实现步骤\n该算法的实现与 Ford−FulkersonFord-FulkersonFord−Fulkerson 算法类似，但在操作中，我们主要以 Level GraphLevel\\ GraphLevel Graph 为主要操作对象：\n\n创建一个残存图 (residual graph)(residual\\ graph)(residual graph)​ ，初始化残存容量\nwhilewhilewhile 可以找到阻塞流：\n\n用残存图创建 level graphlevel\\ graphlevel graph\n在 level graphlevel\\ graphlevel graph 中找到阻塞流\n更新残存图，并添加反向边\n\n\n\n图示\n\n初始化\n\n\n\n\n第一次循环\n\n根据残存图创建 level graphlevel\\ graphlevel graph\n\n \n     \n     \n \n\n找到阻塞流，更新残存图\n\n \n     \n     \n     \n \n\n\n第二次循环\n\n根据上一次的结果，创建新的 level graphlevel\\ graphlevel graph\n\n \n     \n     \n \n\n在 level graphlevel\\ graphlevel graph​ 中找到阻塞流，更新残存图\n\n \n     \n     \n     \n \n\n\n第三轮循环\n\n根据上一次的结果，创建新的 level graphlevel\\ graphlevel graph\n\n \n     \n     \n \n\n因为没有边能到汇点 ttt ，循环终止，最终结果如下：\n\n \n     \n     \n \n\n\n时间复杂度\n时间复杂度为：O(V2E)O(V^2E)O(V2E)\n\n若图为一个链表时，有 nnn 层，循环最多为 O(V−1)O(V-1)O(V−1) 轮\n每一轮的时间为 O(VE)O(VE)O(VE)\n\n二部图问题\n\n嘛~有时间在做吧awa 反正是最大流之后的一章(4th edition)\n\n参考文献\n\nWang, Shusen. (n.d.). AdvancedAlgorithms. GitHub. Retrieved April 5, 2022, from https://github.com/wangshusen/AdvancedAlgorithms\nCormen, T. H., Leiserson, C. E., Rivest, R. L., &amp; Stein, C. (2009). 算法导论 (第三版). 机械工业出版社.\nCormen, T. H., Leiserson, C. E., Rivest, R. L., &amp; Stein, C. (2022). Introduction to Algorithms (4th ed.). MIT Press.\n\n","categories":["DataStru&Algo"],"tags":["Data Structure","Algorithm","SCNU Turing Discussion","Maximum Flow","Graph"]},{"title":"Unsupervised Dense Retrieval with Relevance-Aware Contrastive Pre-Training","url":"/paper/Unsupervised-Dense-Retrieval-with-Relevance-Aware-Contrastive-Pre-Training/","content":"这是一篇关于如何改进无监督密集检索模型的论文。密集检索（Dense Retrieval）通过将查询（Query）和文档（Passage）都编码成向量，然后在向量空间中寻找最相似的文档来工作。\n问题 (Problem)\n传统的密集检索模型严重依赖大量的人工标注数据，这使得它们在新领域的应用成本高昂且泛化能力不足。\n为了解决这个问题，研究界提出了无监督的对比预训练方法，例如 Contriever 模型。这类方法通过在无标签的文档中自动构建正样本对（例如，从同一篇文档中随机裁剪出两段文字作为“查询”和“相关文档”）来进行学习。\n然而，这种自动构建的方式存在一个核心缺陷：“假正例”（False Positives）问题。如下图1所示，一篇文档中相邻的两个句子也可能在语义上毫不相关。如果模型被强制认为它们是相关的，就会学习到错误的表示，从而损害检索性能。\n\n图1：论文中的一个例子，源自维基百科。高亮的两个句子虽然来自同一篇文章，但内容上几乎没有关联。随机裁剪很容易将它们构造成一个“假正例”对。\n本文提出的 ReContriever 模型，旨在解决无监督预训练中的“假正例”问题。\n方法 (Method)\nReContriever 的核心思想是让模型在训练过程中学会区分“真”正例和“假”正例，并对前者赋予更高的学习权重。它建立在 Contriever 模型之上，并引入了两个关键组件：“一文多对”策略和**“相关性感知对比损失”**。\n一文多对 (One-Document-Multi-Pair)\n传统的对比学习方法通常从一个文档中只生成一个正样本对 (q, d+)。为了更充分地利用数据并为后续的相关性评估提供基础，ReContriever 采用“一文多对”策略。\n具体来说，对于一个文档 T，它会首先裁剪出一个固定的“查询”段落 q，然后再从该文档中裁剪出 n 个不同的“候选正例”段落 d_1+, d_2+, ..., d_n+。这样，就从单个文档中生成了 n 个正样本对。保持 q 不变是关键，因为它为后续比较这 n 个候选段落与 q 的相关性提供了一个统一的基准。\n相关性感知对比损失 (Relevance-Aware Contrastive Loss)\n这是该方法的核心。标准的对比学习损失函数是 InfoNCE，其目标是拉近正样本对的距离，推远负样本对的距离。其公式如下：\nLInfoNCE(q,d+)=−log⁡exp⁡(s(q,d+)/τ)exp⁡(s(q,d+)/τ)+∑i=1Dexp⁡(s(q,di−)/τ)\\mathcal{L}_{\\text{InfoNCE}}(q, d^+) = -\\log \\frac{\\exp(s(q, d^+) / \\tau)}{\\exp(s(q, d^+) / \\tau) + \\sum_{i=1}^{D} \\exp(s(q, d_i^-) / \\tau)}\nLInfoNCE​(q,d+)=−logexp(s(q,d+)/τ)+∑i=1D​exp(s(q,di−​)/τ)exp(s(q,d+)/τ)​\n其中：\n\ns(q,d+)s(q, d^+)s(q,d+) 是查询 q 和正例 d+ 的相似度得分（如点积）。\ndi−{d_i^-}di−​ 是一系列负例文档。\nτ\\tauτ 是一个温度超参数。\n\n传统方法中，一个批次的总损失是所有样本对 InfoNCE 损失的平均值。\nReContriever 对此进行了改进，引入了相关性权重。它利用模型自身在当前训练阶段的能力来充当一个“不完美的预言家”，评估每个正样本对的“可信度”。改进后的损失函数 LrelevanceL_{\\text{relevance}}Lrelevance​ 如下：\nLrelevance=1m∑i=1m∑j=1nsθ(qi,dij+)∑k=1nsθ(qi,dik+)⏟相关性权重⋅LInfoNCE(qi,dij+)\\mathcal{L}_{\\text{relevance}} = \\frac{1}{m} \\sum_{i=1}^{m} \\sum_{j=1}^{n} \\underbrace{\\frac{s_{\\theta}(q_i, d_{ij}^+)}{\\sum_{k=1}^{n} s_{\\theta}(q_i, d_{ik}^+)}}_{\\text{相关性权重}} \\cdot \\mathcal{L}_{\\text{InfoNCE}}(q_i, d_{ij}^+)\nLrelevance​=m1​i=1∑m​j=1∑n​相关性权重∑k=1n​sθ​(qi​,dik+​)sθ​(qi​,dij+​)​​​⋅LInfoNCE​(qi​,dij+​)\n其中：\n\nm 是一个批次中的文档数量，n 是每个文档生成的正样本对数量。\nsθ(qi,dij+)s_{\\theta}(q_i, d_{ij}^+)sθ​(qi​,dij+​) 是模型（参数为 θ\\thetaθ）计算出的第 i 个文档的查询 qiq_iqi​ 与其第 j 个正例 dij+d_{ij}^+dij+​ 之间的相似度。\n相关性权重部分是关键：对于源自同一个文档 i 的所有 n 个正样本对，模型会计算它们各自的相似度，然后进行归一化。模型认为更相关的样本对（即 sθs_{\\theta}sθ​ 得分更高）会获得更大的权重，从而在损失计算中占据主导地位。反之，那些可能是“假正例”的样本对权重会很小，对模型更新的影响也随之减弱。\n\n通过这种方式，ReContriever 能够自适应地关注那些质量更高的正样本对，从而减轻“假正例”带来的负面影响。\nBaseline (对比模型)\n论文将 ReContriever 与多个强大的基线模型进行了比较：\n\n传统稀疏检索模型: BM25。\n无监督密集检索模型:\n\n基于对比学习: Contriever (主要对比对象), SimCSE, coCondenser, Spider。\n基于自编码: RetroMAE。\n\n\n有监督密集检索模型 (作为参考): DPR。\n\n数据集 (Datasets)\n模型在两个主流的评测基准上进行了评估：\n\nBEIR: 一个异构的信息检索评测基准，包含了15个不同的公开数据集，用于评估模型的零样本（zero-shot）泛化能力。\n开放域问答 (Open-Domain QA): 三个代表性的问答检索数据集：Natural Questions (NQ), TriviaQA, 和 WebQuestions (WQ)。\n\n此外，还在4个特定领域的语料库上进行了进一步预训练的实验，以测试模型的实际应用潜力。\n可复现性 (Reproducibility)\n\n代码: 论文明确指出代码是公开的（在GitHub上：Yibin-Lei/ReContriever）。代码基于 Pytorch、Huggingface-Transformers 和 Contriever 的官方代码实现，具有较好的可复现基础。\n算力:\n\n预训练: 在 16 块 NVIDIA A100 GPU 上进行，批处理大小为 2048，训练了 500,000 步。\n领域内预训练: 在 8 块 NVIDIA A100 GPU 上进行。\n少样本实验: 在单块 NVIDIA A100 GPU 上进行。\n模型从 BERT-base-uncased 初始化。这些详细的配置和算力需求为复现工作提供了清晰的指引。\n\n\n\n可改进的几个点 (Potential Improvements)\n论文在局限性部分也坦诚地指出了几个未来可以改进的方向：\n\n通用性仍有不足: 尽管 ReContriever 表现出色，但在零样本、通用场景下，其性能仍然落后于经典的 BM25 模型。这意味着在面对一个全新的领域时，如果想获得最佳性能，可能仍需要基于该领域的语料进行额外的预训练，这在一定程度上限制了它的即开即用性。\n潜在的社会偏见: 模型基于 BERT-base 初始化，因此不可避免地会继承其预训练语料中可能存在的社会偏见（如性别、种族歧视等），这在使用时需要特别注意，并可能需要采取去偏见措施。\n\n可以被引用的一些结论 (Citable Conclusions)\n\n核心贡献: 通过“一文多对”和“相关性感知对比损失”，ReContriever 有效缓解了无监督对比学习中的“假正例”问题，为密集检索提供了一个更鲁棒的预训练范式。\n性能超越 SOTA: 在 BEIR 基准的15个任务中，ReContriever 在10个任务上优于之前的最强无监督模型 Contriever，平均排名第一。在开放域问答检索任务中，性能也几乎全面领先于所有无监督方法。\n实践应用价值高:\n\n领域适应性强: 在特定领域的语料上进行短暂的二次预训练后，ReContriever 能够稳定地超越强大的 BM25 基线，展现了很好的领域适应潜力。\n少样本学习能力强: 在只有少量（例如128个）标注样本的情况下进行微调，ReContriever 的性能可以媲美使用数千个样本训练的有监督 DPR 模型，这对于标注数据稀缺的场景非常有价值。\n\n\n方法组件的必要性: 消融实验证明，“一文多对”策略和“相关性感知损失”缺一不可。单独使用相关性感知损失甚至会导致性能下降，说明两者结合才能产生协同效应，共同提升模型性能。\n\n","categories":["paper"],"tags":["paper","Dense Retrieval","unsupervised"]},{"title":"Tree","url":"/DataStru-Algo/Tree/","content":"SCNU-Turing-Class CLRS Discussion Week4-5\n\n\nCiallo～(∠・ω&lt;)⌒★！Ciallo～(∠・ω&lt; )⌒★！Ciallo～(∠・ω&lt;)⌒★！​\n​右子树就是柚子树\n​柚子厨蒸鹅心\nCiallo～(∠・ω&lt;)⌒★！Ciallo～(∠・ω&lt; )⌒★！Ciallo～(∠・ω&lt;)⌒★！\n\n二叉树 Binary Tree\n二叉树的定义\n​一种非线性数据结构，代表“祖先”与“后代”之间的派生关系，体现了“一分为二”的分治逻辑。\n​每个节点包含的属性有：\n1. left指针，指向左子节点\n2. right指针，指向右子节点\n3. prev指针，指向父节点（书上这么写的，但实际构建可能不写这个。当然可以构建一个prev指针指向父节点，形成一个双向二叉树）\n4. key关键字，节点的值\n构建代码如下：\n//c++struct TreeNode &#123;    int val;          // 节点值    TreeNode *left;   // 左子节点指针    TreeNode *right;  // 右子节点指针    TreeNode *prev;   // 父节点指针    TreeNode(int x) : val(x), left(nullptr), right(nullptr), prev(nullptr) &#123;&#125;&#125;;\n二叉树常用术语\n\n「根节点 root noderoot\\ noderoot node 」：位于二叉树顶层的节点，没有父节点。\n「叶节点 leaf nodeleaf\\ nodeleaf node 」：没有子节点的节点，其两个指针均指向 None 。\n节点的「度 degreedegreedegree 」：节点的子节点的数量。在二叉树中，度的取值范围是 0、1、2 。\n二叉树的「高度 heightheightheight 」：从根节点到最远叶节点所经过的边的数量。\n节点的「深度 depthdepthdepth 」：从根节点到该节点所经过的边的数量。\n节点的「高度 heightheightheight 」：从距离该节点最远的叶节点到该节点所经过的边的数量。\n\n常见二叉树类型\n\n\n完美二叉树 perfect binary treeperfect\\ binary\\ treeperfect binary tree\n\n所有层的节点都被完全填满。\n在完美二叉树中，叶节点的度为 0，其余所有节点的度都为2\n若树的高度为 ℎ ，则节点总数为 2^ℎ+1^−1 ，呈现标准的指数级关系。\n\n\n\n完全二叉树 complete binary treecomplete\\ binary\\ treecomplete binary tree\n\n只有最底层的节点未被填满\n最底层节点尽量靠左填充。\n\n\n\n完满二叉树 full binary treefull\\ binary\\ treefull binary tree\n\n所有节点都有两个子节点（除了叶节点）。\n\n\n\n平衡二叉树 balanced binary treebalanced\\ binary\\ treebalanced binary tree\n\n任意节点的左子树和右子树的高度之差的绝对值不超过 1 。\n\n\n\n二叉树的退化\n​\t当二叉树的每层节点都被填满时，达到“完美二叉树”；而当所有节点都偏向一侧时，二叉树退化为“链表”。\n\n完美二叉树是理想情况，可以充分发挥二叉树“分治”的优势。\n链表则是另一个极端，各项操作都变为线性操作，时间复杂度退化至O(n)O(n)O(n)​。\n\n\n\n\n\n完美二叉树\n链表\n\n\n\n\n第iii层的节点数量\n2i−12^{i-1}2i−1\n111\n\n\n高度为hhh的树的叶节点数量\n2h−12^{h-1}2h−1\n111\n\n\n高度为hhh的树的节点总数\n2h−1−12^{h-1}-12h−1−1\nh+1h+1h+1\n\n\n节点总数为nnn的树的高度\nlog⁡2(n+1)−1\\log_{2}{(n+1)}-1log2​(n+1)−1\nn−1n-1n−1\n\n\n\n二叉搜索树 Binary Search Tree\n\n一棵 x.left.key≤\\le≤ x.right.key 的二叉树\n\n二叉树的遍历\n深度优先搜索 Depth-first traversal\n\n\n二叉树的遍历在「深度优先搜索 DFT」中分为三种：\n\n\n前序遍历\n\n\n中序遍历\n\n\n后续遍历\n\n\n\n\n\n代码实现\nINORDER-TREE-WALK(x) \tif x≠NIL \tINORDER-TREE-WALK(x.left) \tprint x.key\tINORDER-TREE-WALK(x.right)\n/* 前序遍历 */void preOrder(TreeNode *root) &#123;    if (root == nullptr)        return;    // 访问优先级：根节点 -&gt; 左子树 -&gt; 右子树    vec.push_back(root-&gt;val);    preOrder(root-&gt;left);    preOrder(root-&gt;right);&#125;/* 中序遍历 */void inOrder(TreeNode *root) &#123;    if (root == nullptr)        return;    // 访问优先级：左子树 -&gt; 根节点 -&gt; 右子树    inOrder(root-&gt;left);    vec.push_back(root-&gt;val);    inOrder(root-&gt;right);&#125;/* 后序遍历 */void postOrder(TreeNode *root) &#123;    if (root == nullptr)        return;    // 访问优先级：左子树 -&gt; 右子树 -&gt; 根节点    postOrder(root-&gt;left);    postOrder(root-&gt;right);    vec.push_back(root-&gt;val);&#125;\n复杂度分析\n\n时间复杂度：Θ(n)\\Theta(n)Θ(n)\n\n对于渐进下界：\n​\t遍历需要访问 BSTBSTBST 的全部节点。所以有：\nT(n)=Ω(n)T(n)=\\Omega(n)\nT(n)=Ω(n)\n接下来证明渐进上界：\n​\t对于一棵空树，调用遍历函数需要一个极小的常数时间。所以设T(0)=cT(0)=cT(0)=c，ccc为常数​\n​\t对于一棵节点数n&gt;0n&gt;0n&gt;0 的树，设左子树节点数 k&gt;0k&gt;0k&gt;0 ，右子树节点数为 n−k−1n-k-1n−k−1 ，额外开销为ddd，则递归不等式为：\nT(n)≤T(k)+T(n−k−1)+d，d为常数T(n)\\le T(k)+T(n-k-1)+d，d为常数\nT(n)≤T(k)+T(n−k−1)+d，d为常数\n​\t我们不妨假设 ∃ c ∀ R\\exists{\\ c\\ \\forall\\ R}∃ c ∀ R ，当 ccc 很大时，有：T(n)≤cnT(n)\\le cnT(n)≤cn\n​\t则上式可化为：\nT(n)≤ck+c(n−k−1)+d=cn−(c−d)T(n) \\le ck+c(n-k-1)+d=cn-(c-d)\nT(n)≤ck+c(n−k−1)+d=cn−(c−d)\n​\t当 c−d≤0c-d\\le 0c−d≤0 时，有 T(n)=O(n)T(n)=O(n)T(n)=O(n)​ 。\n综上所述，==T(n)=Θ(n)T(n)=\\Theta (n)T(n)=Θ(n)==​​​\n\n空间复杂度：O(n)O(n)O(n)\n\n​\t在最差情况下，即树退化为链表时，递归深度达到 nnn ，系统占用  O(n)O(n)O(n)​ 栈帧空间。\n广度优先遍历 Breadth-first traversal\n\n「层序遍历 level-order traversal」\n\n​\t从顶部到底部逐层遍历二叉树，并在每一层按照从左到右的顺序访问节点。\n代码实现\n​\t广度优先遍历通常借助“队列”来实现。队列遵循“先进先出”的规则，而广度优先遍历则遵循“逐层推进”的规则，两者背后的思想是一致的。\n/* 层序遍历 */vector&lt;int&gt; levelOrder(TreeNode *root) &#123;    // 初始化队列，加入根节点    queue&lt;TreeNode *&gt; queue;    queue.push(root);    // 初始化一个列表，用于保存遍历序列    vector&lt;int&gt; vec;    while (!queue.empty()) &#123;        TreeNode *node = queue.front();        queue.pop();              // 队列出队        vec.push_back(node-&gt;val); // 保存节点值        if (node-&gt;left != nullptr)            queue.push(node-&gt;left); // 左子节点入队        if (node-&gt;right != nullptr)            queue.push(node-&gt;right); // 右子节点入队    &#125;    return vec;&#125;\n复杂度分析\n\n时间复杂度：Θ(n)\\Theta (n)Θ(n)​\n\n​\t所有节点被访问一次，使用 Θ(n)\\Theta (n)Θ(n) 时间，其中 nnn 为节点数量。\n\n空间复杂度：O(n)O(n)O(n)​\n\n​\t在最差情况下，即满二叉树时，遍历到最底层之前，队列中最多同时存在 (n+1)/2(n+1)/2(n+1)/2 个节点，占用 O(n)O(n)O(n) 空间。\n二叉搜索树的查找\n​\t给定目标节点值 num ，可以根据二叉搜索树的性质来查找。如图 7-17 所示，我们声明一个节点 cur ，从二叉树的根节点 root 出发，循环比较节点值 cur.val 和 num 之间的大小关系。\n\n若 cur.val &lt; num ，说明目标节点在 cur 的右子树中，因此执行 cur = cur.right 。\n若 cur.val &gt; num ，说明目标节点在 cur 的左子树中，因此执行 cur = cur.left 。\n若 cur.val = num ，说明找到目标节点，跳出循环并返回该节点。\n\n代码实现\n/* 查找节点 */TreeNode *search(int num) &#123;    TreeNode *cur = root;    // 循环查找，越过叶节点后跳出    while (cur != nullptr &amp;&amp; cur-&gt;val != num) &#123;        // 目标节点在 cur 的右子树中        if (cur-&gt;val &lt; num)            cur = cur-&gt;right;        // 目标节点在 cur 的左子树中        else            cur = cur-&gt;left;    &#125;    // 返回目标节点    return cur;&#125;\n最大关键字元素和最小关键字元素\n/* 最小关键字-迭代 */TreeNode *Minimun(TreeNode *x)&#123;\twhile(x-&gt;left != nullptr)        x = x-&gt;left;    return x;&#125;\n/* 最小关键字-递归 */TreeNode *Minimun(TreeNode *x)&#123;    if(x-&gt;left != nullptr)        return Minimun(x-&gt;left);    else        return x;&#125;\n/* 最大关键字 */TreeNode *Maximun(TreeNode *x)&#123;\twhile(x != nullptr)        x = x-&gt;right;    return x;&#125;\n/* 最大关键字-递归 */TreeNode *Maximun(TreeNode *x)&#123;    if(x-&gt;right != nullptr)        return Minimun(x-&gt;right);    else        return x;&#125;\n后继和前驱\n\nTREE-SUCCESSOR(x)\tif x.right ≠ NIL \t\treturn TREE-MINIMUM(x.right) \ty = x.p \twhile y ≠ NIL and x == y.right \t\tx = y \t\ty = y.p \treturn y\n/* c++版后继 */TreeNode *Successor(TreeNode *x)&#123;    //如果结点x的右子树非空，那么x的后继恰是x右子树中的最左结点    if(x-&gt;right != nullptr)        return Minimun(x-&gt;right);    TreeNode *y=x-&gt;prev;    //如果x的右子树为空并有一个后继，那么向上回溯直到**x节点**是其父节点的左孩子    while(y != nullptr &amp;&amp; x-&gt;val == y-&gt;right-&gt;val)&#123;        x = y;        y = y-&gt;prev;    &#125;    return y;&#125;\n/* c++版前驱 */TreeNode *Predecessor(TreeNode *x)&#123;    //如果结点x的左子树非空，那么x的前驱恰是x左子树中的最右结点    if(x-&gt;left != nullptr)        return Maximun(x-&gt;left);    TreeNode *y=x-&gt;prev;    //如果x的左子树为空并有一个前驱，那么向上回溯直到**x节点**是其父节点的右孩子    while(y != nullptr &amp;&amp; x-&gt;val == y-&gt;left-&gt;val)&#123;        x = y;        y = y-&gt;prev;    &#125;    return y;&#125;\n二叉搜索树的插入\n​\t给定一个待插入元素 num ，为了保持二叉搜索树“左子树 &lt; 根节点 &lt; 右子树”的性质，插入操作流程如下所示：\n\n查找插入位置：与查找操作相似，从根节点出发，根据当前节点值和 num 的大小关系循环向下搜索，直到越过叶节点（遍历至 None ）时跳出循环。\n在该位置插入节点：初始化节点 num ，将该节点置于 None 的位置。\n\n代码实现\n/* 伪代码 */TREE-INSERT(T,z) \ty = NIL\tx = T.root \twhile x ≠ NIL \t\ty = x \t\tif z.key &lt; x.key \t\t\tx = x.left\t\telse \t\t\tx = x. right \tz.p = y \tif y == NIL \t\tT. root= z \t\t// tree T was empty \telseif z.key &lt; y.key \t\ty.left= z \telse y.right = z\n/* 插入节点-迭代版 */void insert(int num) &#123;    // 若树为空，则初始化根节点    if (root == nullptr) &#123;        root = new TreeNode(num);        return;    &#125;    TreeNode *cur = root, *pre = nullptr;    // 循环查找，越过叶节点后跳出    while (cur != nullptr) &#123;        // 找到重复节点，直接返回        if (cur-&gt;val == num)            return;        pre = cur;        // 插入位置在 cur 的右子树中        if (cur-&gt;val &lt; num)            cur = cur-&gt;right;        // 插入位置在 cur 的左子树中        else            cur = cur-&gt;left;    &#125;    // 插入节点    TreeNode *node = new TreeNode(num);    if (pre-&gt;val &lt; num)        pre-&gt;right = node;    else        pre-&gt;left = node;&#125;\n/* 插入节点-递归版 */void insert(TreeNode* &amp;cur, int num) &#123;    // 如果当前节点为空，则创建新节点并返回    if (cur == nullptr) &#123;        cur = new TreeNode(num);        return;    &#125;    // 如果值已存在，则直接返回    if (cur-&gt;val == num)        return;    // 如果要插入的值比当前节点的值大，则插入到右子树中    if (cur-&gt;val &lt; num)        insert(cur-&gt;right, num);    // 如果要插入的值比当前节点的值小，则插入到左子树中    else        insert(cur-&gt;left, num);&#125;\n复杂度分析\n\n时间复杂度：O(log n)O(log\\ n)O(log n) ，n为节点数\n空间复杂度：O(1)O(1)O(1)\n\n二叉搜索树的删除\n​\t先在二叉树中查找到目标节点，再将其删除。与插入节点类似，我们需要保证在删除操作完成后，二叉搜索树的“左子树 &lt; 根节点 &lt; 右子树”的性质仍然满足。\n​\t因此，我们根据目标节点的子节点数量，分 0、1 和 2 三种情况，执行对应的删除节点操作。\n\n\n若 zzz 有0个子节点，则简单地将 zzz 删除，并修改 zzz 的父节点，用 NILNILNIL​ 作为孩子来替换 zzz 。\n\n\n若 zzz 有1个子节点，则将这个子节点提升到 zzz 的位置，并修改 zzz 的父节点，用 zzz 的孩子来替换 zzz 。（图a、b）\n\n\n若 zzz 有2个子节点，找 zzz 的后继 yyy ，然后 yyy 的右孩子（若有）取代 yyy 的位置，最后用 yyy 取代 zzz 。 （图c、d）\n\n\n\n\n\n\n代码实现\n/* 将以u为根的子树用以v为根的子树代替 */TRANSPLANT(T,u,v)\t/*若u为根节点*/\tif u.p == NIL \t\tT.root=v \t/*若u是其父节点的左孩子*/\telse if u == u.p.left\t\tu.p.left = v\t/*若u是其父节点的右孩子*/\telse\t\tu.p.right = v\tif v ≠ NIL\t\tv.p=u.p\n/* 删除结点z */TREE-DELETE(T,z)\t/* 若z没有左孩子-图a */\tif z.left == NIL\t\tTRANSPLANT(T,z,z.right) /* 以z的右子树代替z */\t/* 若z没有右孩子-图b */\telse if z.right == NIL\t\tTRANSPLANT(T,z,z.left) /* 以z的左子树代替z */\t/* z有两个孩子 */\telse\t\ty = TREE-MINIMUM(z.right) /* y为z的后继，y必然没有左节点 */\t\t/* 若y不是z的孩子-图d */\t\tif y.p ≠ z\t\t\tTRANSPLANT(T,y,y.right) /* 将y的位置替换为y的右子树 */\t\t\ty.right = z.right /* 将y的右子树设置为z的右子树 */\t\t\ty.right.p = y /* 将y的右子树的父节点设置为y */\t\t/* 若y是z的右孩子-图c 以及图d的后续 */\t\tTRANSPLANT(T,z,y) /* 用y替换z */\t\ty.left = z.left \t\ty.left.p = y\n/* 删除节点 */void remove(int num) &#123;    // 若树为空，直接提前返回    if (root == nullptr)        return;    TreeNode *cur = root, *pre = nullptr;    // 循环查找，越过叶节点后跳出    while (cur != nullptr) &#123;        // 找到待删除节点，跳出循环        if (cur-&gt;val == num)            break;        pre = cur;        // 待删除节点在 cur 的右子树中        if (cur-&gt;val &lt; num)            cur = cur-&gt;right;        // 待删除节点在 cur 的左子树中        else            cur = cur-&gt;left;    &#125;    // 若无待删除节点，则直接返回    if (cur == nullptr)        return;        // 子节点数量 = 0 or 1    if (cur-&gt;left == nullptr || cur-&gt;right == nullptr) &#123;        // 当子节点数量 = 0 or 1 时， child = nullptr or 该子节点        TreeNode *child = cur-&gt;left != nullptr ? cur-&gt;left : cur-&gt;right;        // 删除节点 cur        if (cur != root) &#123;            if (pre-&gt;left == cur)                pre-&gt;left = child;            else                pre-&gt;right = child;        &#125;         else &#123;            // 若删除节点为根节点，则重新指定根节点            root = child;        &#125;        // 释放内存        delete cur;    &#125;    // 子节点数量 = 2    else &#123;        // 获取中序遍历中 cur 的下一个节点        TreeNode *tmp = cur-&gt;right;        while (tmp-&gt;left != nullptr) &#123;            tmp = tmp-&gt;left;        &#125;        int tmpVal = tmp-&gt;val;        // 递归删除节点 tmp        remove(tmp-&gt;val);        // 用 tmp 覆盖 cur        cur-&gt;val = tmpVal;    &#125;&#125;\n复杂度分析\n\n时间复杂度：O(log n)O(log\\ n)O(log n) ，n为节点数\n空间复杂度：O(1)O(1)O(1)​\n\n*随机构建二叉搜索树 Randomly built binary search trees\n​\t定义 nnn​ 个关键字的一棵随机构建二叉搜索树为按随机次序插入这些关键字到一棵初始的空树中而生成的树，这里输入关键字的 n!n!n! 个排列中的每个都是等可能地出现。那么对于这棵树的期望高度，则有如下定理：\n\n定理\t一棵有 nnn 个不同关键字的随机构建二叉搜索树的期望高度为 ==O(log n)O(log\\ n)O(log n)== 。\n证明如下：\n\n​\t我们先定义如下三个随机变量：\n​\t\t1. XnX_{n}Xn​ 为一棵有 nnn 个不同关键字的随机构建二叉搜索树的高度。\n​\t\t2. Yn=2XnY_{n}=2^{X_{n}}Yn​=2Xn​ 为指数高度，定义 Y0=0Y_{0}=0Y0​=0 和 Y1=20=1Y_{1}=2^{0}=1Y1​=20=1 。\n​\t\t3. RnR_{n}Rn​ 为一个随机变量，选择一个关键字作为树根，其为这个关键字在 nnn 个关键字集合中的秩（rankrankrank），表示这个关键字在排好序后应占据的位置。RnR_{n}Rn​ 的值对于集合 {1,2,...,n}\\{1, 2,...,n\\}{1,2,...,n} 中的任何元素都是等可能的。\n​\t\t若 Rn=iR_{n}=iRn​=i ，那么根的左子树是一棵有 i−1i-1i−1 个关键字的随机构建二叉搜索树，并且右子树是一棵有 n−in-in−i 个关键字的随机构建搜二叉索树。\n​\t\t因为二叉树的高度为：\nXn=max(leftSubtree.height,rightSubtree.height)+1X_{n} =max(leftSubtree.height,rightSubtree.height)+1\nXn​=max(leftSubtree.height,rightSubtree.height)+1\n​\t\t所以有：\nYn=2⋅max(Yi−1,Yn−i)Y_{n}=2\\cdot max(Y_{i-1},Y_{n-i})\nYn​=2⋅max(Yi−1​,Yn−i​)\n​\t\t接下来，我们定义指示器随机变量（chapter 5.2chapter\\ 5.2chapter 5.2）Zn,1Z_{n,1}Zn,1​ ，Zn,2Z_{n,2}Zn,2​ ，......... ，Zn,nZ_{n,n}Zn,n​ ，其中：\nZn,i=I{Rn=i}={ 1n=i 0n≠i①Z_{n,i}=I\\{R_{n}=i\\}=\\begin{cases}\n\\ 1     &amp;n=i\\\\\n\\ 0     &amp;n\\ne i\n\\end{cases}\n\\qquad ①\nZn,i​=I{Rn​=i}={ 1 0​n=in=i​①\n​\t\t又因为 Rn=iR_{n}=iRn​=i 对于集合中每一个元素取值的概率是相同的，所以：\nP{Rn=i}=1n ,(i=1,2,...,n)②P\\{R_{n}=i\\}=\\frac{1}{n}\\ ,(i=1,2,...,n) \\qquad ②\nP{Rn​=i}=n1​ ,(i=1,2,...,n)②\n​\t\t结合 ① ②①\\ ②① ② ，得到：\nE[Zn,i]=1⋅P{Rn=i}=1n ,(i=1,2,...,n)③E[Z_{n,i}]=1 \\cdot P\\{R_{n}=i\\}=\\frac{1}{n} \\ ,(i=1,2,...,n) \\quad ③\nE[Zn,i​]=1⋅P{Rn​=i}=n1​ ,(i=1,2,...,n)③\n​\t\t由于 Zn,iZ_{n,i}Zn,i​ 恰有一个值为 111 ，其余所有值为 000 ，因此：\nYn=∑i=1nZn,i(2⋅max(Yi−1,Yn−i))Y_{n}=\\sum_{i=1}^{n}{Z_{n,i}(2\\cdot max(Y_{i-1},Y_{n-i}))}\nYn​=i=1∑n​Zn,i​(2⋅max(Yi−1​,Yn−i​))\n​\t\t由于 Zn,iZ_{n,i}Zn,i​ 独立于 Yi−1,Yn−iY_{i-1},Y_{n-i}Yi−1​,Yn−i​ 的值，上式两边取期望，得：\nE[Yn]=E[∑i=1nZn,i(2⋅max⁡(Yi−1,Yn−i))]=∑i=1nE[Zn,i(2⋅max⁡(Yi−1,Yn−i))](期望的线性性质)=∑i=1nE[Zn,i]⋅E[2⋅max⁡(Yi−1,Yn−i)](独立性)=∑i=1n1n⋅E[2⋅max⁡(Yi−1,Yn−i)](③式)=2n∑i=1nE[max⁡(Yi−1,Yn−i)](期望的线性性质)≤2n∑i=1n(E[Yi−1]+E[Yn−i])(补充证明1)\\begin{align*}\nE[Y_{n}] &amp;= E\\left[\\sum_{i=1}^{n}{Z_{n,i}(2 \\cdot \\max(Y_{i-1},Y_{n-i}))}\\right] \\\\\n&amp;=\\sum_{i=1}^{n}E[{Z_{n,i}}(2 \\cdot \\max(Y_{i-1},Y_{n-i}))] &amp;(期望的线性性质)\\\\\n&amp;=\\sum_{i=1}^{n}E[{Z_{n,i}}]\\cdot E[2 \\cdot \\max(Y_{i-1},Y_{n-i})] &amp;(独立性)\\\\\n&amp;=\\sum_{i=1}^{n}\\frac{1}{n}\\cdot E[2 \\cdot \\max(Y_{i-1},Y_{n-i})] &amp;(③式)\\\\\n&amp;=\\frac{2}{n}\\sum_{i=1}^{n}E[\\max(Y_{i-1},Y_{n-i})] &amp;(期望的线性性质)\\\\\n&amp;\\le\\frac{2}{n}\\sum_{i=1}^{n}(E[Y_{i-1}]+E[Y_{n-i}]) &amp;(补充证明1)\\\\\n\\end{align*}\nE[Yn​]​=E[i=1∑n​Zn,i​(2⋅max(Yi−1​,Yn−i​))]=i=1∑n​E[Zn,i​(2⋅max(Yi−1​,Yn−i​))]=i=1∑n​E[Zn,i​]⋅E[2⋅max(Yi−1​,Yn−i​)]=i=1∑n​n1​⋅E[2⋅max(Yi−1​,Yn−i​)]=n2​i=1∑n​E[max(Yi−1​,Yn−i​)]≤n2​i=1∑n​(E[Yi−1​]+E[Yn−i​])​(期望的线性性质)(独立性)(③式)(期望的线性性质)(补充证明1)​\n​\t\t因为 E[Y0]E[Y_{0}]E[Y0​] ，E[Y1]E[Y_{1}]E[Y1​]，......... ，E[Yn−1]E[Y_{n-1}]E[Yn−1​] 每一项各出现两次，一次作为 E[Yi−1]E[Y_{i-1}]E[Yi−1​] ，另一次为 E[Yn−1]E[Y_{n-1}]E[Yn−1​] 。\n​\t\t所以上述公式可化为：\nE[Yn]≤4n∑i=0n−1E[Yi]E[Y_{n}] \\le\\frac{4}{n}\\sum_{i=0}^{n-1}E[Y_{i}]\nE[Yn​]≤n4​i=0∑n−1​E[Yi​]\n​\t\t使用替换法，证明 E[Y(n)]=O(n3)E[Y(n)]=O(n^3)E[Y(n)]=O(n3) ：\n4n∑i=0n−1E[Yi]≤4n∑i=0n−1cn3≤4n⋅cn44=cn3\\begin{align*}\n\\frac{4}{n}\\sum_{i=0}^{n-1}E[Y_{i}] &amp;\\le \\frac{4}{n}\\sum_{i=0}^{n-1}cn^3\\\\\n&amp;\\le \\frac{4}{n} \\cdot \\frac{cn^4}{4}=cn^3\n\\end{align*}\nn4​i=0∑n−1​E[Yi​]​≤n4​i=0∑n−1​cn3≤n4​⋅4cn4​=cn3​\n​\t\t所以，E[Y(n)]=O(n3)E[Y(n)]=O(n^3)E[Y(n)]=O(n3) ，即E[2X(n)]=O(n3)E[2^{X(n)}]=O(n^3)E[2X(n)]=O(n3)。\n​\t\t由 JensenJensenJensen 不等式（补充证明2）：\n2E[X(n)]≤E[2X(n)]=O(n3)2^{E[X(n)]}\\le E[2^{X(n)}]=O(n^3)\n2E[X(n)]≤E[2X(n)]=O(n3)\n​\t\t两边取对数：\nE[X(n)]=O(lgn)E[X(n)]=O(lgn)\nE[X(n)]=O(lgn)\n​\t综上所述，一棵有 nnn 个不同关键字的随机构建二叉搜索树的期望高度为 ==O(log n)O(log\\ n)O(log n)​== 。\n\n补充证明1：\n要证明 E[max⁡(X,Y)]≤E[X]+E[Y]E[\\max(X,Y)] \\leq E[X] + E[Y]E[max(X,Y)]≤E[X]+E[Y]，我们可以考虑分两种情况：\n\n当 X≤YX \\leq YX≤Y 时，有 max⁡(X,Y)=Y\\max(X,Y) = Ymax(X,Y)=Y。\n当 X&gt;YX &gt; YX&gt;Y 时，有 max⁡(X,Y)=X\\max(X,Y) = Xmax(X,Y)=X。\n\n因此，我们可以写出以下不等式：\nE[max⁡(X,Y)]=E[max⁡(X,Y)∣X≤Y]⋅P(X≤Y)+E[max⁡(X,Y)∣X&gt;Y]⋅P(X&gt;Y)≤E[Y]⋅P(X≤Y)+E[X]⋅P(X&gt;Y)=E[Y⋅1X≤Y]+E[X⋅1X&gt;Y]\\begin{align*}\nE[\\max(X,Y)] &amp;= E[\\max(X,Y) \\mid X \\leq Y] \\cdot P(X \\leq Y) + E[\\max(X,Y) \\mid X &gt; Y] \\cdot P(X &gt; Y) \\\\\n&amp;\\leq E[Y] \\cdot P(X \\leq Y) + E[X] \\cdot P(X &gt; Y) \\\\\n&amp;= E[Y \\cdot \\mathbb{1}_{X \\leq Y}] + E[X \\cdot \\mathbb{1}_{X &gt; Y}]\n\\end{align*}\nE[max(X,Y)]​=E[max(X,Y)∣X≤Y]⋅P(X≤Y)+E[max(X,Y)∣X&gt;Y]⋅P(X&gt;Y)≤E[Y]⋅P(X≤Y)+E[X]⋅P(X&gt;Y)=E[Y⋅1X≤Y​]+E[X⋅1X&gt;Y​]​\n其中，1X≤Y\\mathbb{1}_{X \\leq Y}1X≤Y​ 和 1X&gt;Y\\mathbb{1}_{X &gt; Y}1X&gt;Y​ 是指示函数。现在，因为指示函数的期望是概率本身，所以我们有：\nE[Y⋅1X≤Y]=P(X≤Y)⋅E[Y]≤P(X≤Y)⋅E[X]+P(X≤Y)⋅E[Y]E[X⋅1X&gt;Y]=P(X&gt;Y)⋅E[X]≤P(X&gt;Y)⋅E[X]+P(X&gt;Y)⋅E[Y]E[Y \\cdot \\mathbb{1}_{X \\leq Y}] = P(X \\leq Y) \\cdot E[Y] \\leq P(X \\leq Y) \\cdot E[X] + P(X \\leq Y) \\cdot E[Y] \\\\\nE[X \\cdot \\mathbb{1}_{X &gt; Y}] = P(X &gt; Y) \\cdot E[X] \\leq P(X &gt; Y) \\cdot E[X] + P(X &gt; Y) \\cdot E[Y]\nE[Y⋅1X≤Y​]=P(X≤Y)⋅E[Y]≤P(X≤Y)⋅E[X]+P(X≤Y)⋅E[Y]E[X⋅1X&gt;Y​]=P(X&gt;Y)⋅E[X]≤P(X&gt;Y)⋅E[X]+P(X&gt;Y)⋅E[Y]\n将上述两个不等式代入前面的不等式中，我们得到：\nE[max⁡(X,Y)]≤E[Y]⋅P(X≤Y)+E[X]⋅P(X&gt;Y)≤E[X]+E[Y]E[\\max(X,Y)] \\leq E[Y] \\cdot P(X \\leq Y) + E[X] \\cdot P(X &gt; Y) \\leq E[X] + E[Y]\nE[max(X,Y)]≤E[Y]⋅P(X≤Y)+E[X]⋅P(X&gt;Y)≤E[X]+E[Y]\n因此，E[max⁡(X,Y)]≤E[X]+E[Y]E[\\max(X,Y)] \\leq E[X] + E[Y]E[max(X,Y)]≤E[X]+E[Y]​​，证毕。\n\n\n补充证明2：\n假设 fff 是一个 convexconvexconvex 函数（国外是凸函数，国内是凹函数）。我们想要证明对于任意随机变量 XXX 和权重 wiw_iwi​，满足 ∑iwi=1\\sum_{i} w_i = 1∑i​wi​=1，有：\nE[f(X)]≥f(E[X])E[f(X)] \\geq f(E[X])\nE[f(X)]≥f(E[X])\n为了证明这一点，我们首先需要定义 convexconvexconvex 函数的性质。一个函数 fff 被称为 convexconvexconvex 函数，如果对于任意 x1,x2x_1, x_2x1​,x2​ 和 0≤λ≤10 \\leq \\lambda \\leq 10≤λ≤1，都有：\nf(λx1+(1−λ)x2)≤λf(x1)+(1−λ)f(x2)f(\\lambda x_1 + (1 - \\lambda) x_2) \\leq \\lambda f(x_1) + (1 - \\lambda) f(x_2)\nf(λx1​+(1−λ)x2​)≤λf(x1​)+(1−λ)f(x2​)\n现在，我们来证明 Jensen 不等式：\n根据 convexconvexconvex 函数的定义，我们有：\nf(λE[X]+(1−λ)E[X])≤λf(E[X])+(1−λ)f(E[X])f(\\lambda E[X] + (1 - \\lambda) E[X]) \\leq \\lambda f(E[X]) + (1 - \\lambda) f(E[X])\nf(λE[X]+(1−λ)E[X])≤λf(E[X])+(1−λ)f(E[X])\n⇒f(E[X])≤λf(E[X])+(1−λ)f(E[X])\\Rightarrow f(E[X]) \\leq \\lambda f(E[X]) + (1 - \\lambda) f(E[X])\n⇒f(E[X])≤λf(E[X])+(1−λ)f(E[X])\n⇒f(E[X])≤f(E[X])\\Rightarrow f(E[X]) \\leq f(E[X])\n⇒f(E[X])≤f(E[X])\n这是显然成立的。因此，我们得出：\nE[f(X)]≥f(E[X])E[f(X)] \\geq f(E[X])\nE[f(X)]≥f(E[X])\n这就证明了 Jensen 不等式。\n\n:sweat_smile: 红黑树 Red-Black Tree :fearful:\n\n🥵Q宝速速给我抄抄😋\n\n红黑树的性质\n\n每个节点要么红，要么黑\n根节点永远为黑\n叶节点 (NIL)(NIL)(NIL) 为黑\n不会有两个相连的红节点（一个节点是红的，那么俩子节点是黑的）\n对每个节点，从该节点到其所有后代叶节点的简单路径上，均包含相同数目的黑色节点（即黑高 bh(x)bh(x)bh(x) 相同）\n(*)每一棵红黑树都对应着一棵 2−3−42-3-42−3−4 树（或 2−32-32−3 树）\n\n\n其最明显的优势：一棵有 nnn 个节点的红黑树的高度至多为 ==2log(n+1)2log( n+1)2log(n+1)==​ 。\n\n证明如下：\n​\t设 xxx 为一棵红黑树的根节点，bh(x)bh(x)bh(x)​ 为一棵红黑树某个节点的黑高（即一条简单路径上黑色节点的个数），hhh 为一棵红黑树的高度。\n​\t我们先证明以 xxx 为根的子树中至少包含 2bh(x)−12^{bh(x)}-12bh(x)−1 个内部节点\n​\t要证明此点，我们以数学归纳法进行证明：\n​\t\t1. 当 x.depth=0x.depth=0x.depth=0 ，即 xxx 为根节点，此时子树中的节点满足 2bh(x)−1=20−1=02^{bh(x)}-1=2^{0}-1=02bh(x)−1=20−1=0​ 。\n​\t\t2. 当 x.depth=kx.depth=kx.depth=k ，假设该命题成立，即以 xxx 为根的子树中至少包含 2bh(xk)−12^{bh(x_{k})}-12bh(xk​)−1 个内部节点，此时的黑高为 bh(xk)bh(x_{k})bh(xk​) 或 bh(xk)−1bh(x_{k})-1bh(xk​)−1 ，这取决于 x.colorx.colorx.color 是黑还是红。\n​\t\t**3. **则当 x.depth=k+1x.depth=k+1x.depth=k+1 ，即 xxx​ 下一节点时，因为同一节点左右子树的黑高一致，所以有：\nbh(xk+1)=bh(xk)−1bh(x_{k+1})=bh(x_{k})-1\nbh(xk+1​)=bh(xk​)−1\n​\t\t\t此时 x.depth=kx.depth=kx.depth=k 的左右子树节点数至少为：2bh(xk+1)−1=2bh(xk)−1−12^{bh(x_{k+1})}-1=2^{bh(x_{k})-1}-12bh(xk+1​)−1=2bh(xk​)−1−1 。\n​\t\t\t所以内部总节点数至少为：(2bh(xk)−1−1)+(2bh(xk)−1−1)+1=2bh(xk)−1(2^{bh(x_{k})-1}-1)+(2^{bh(x_{k})-1}-1)+1=2^{bh(x_{k})}-1(2bh(xk​)−1−1)+(2bh(xk​)−1−1)+1=2bh(xk​)−1 。\n​\t\t\t（这里只加一次 111 是因为考虑到了左右子树可能存在 NILNILNIL ）\n​\t\t综上所述，以 xxx 为根的子树中至少包含 ==2bh(x)−12^{bh(x)}-12bh(x)−1== 个内部节点。\n​\t又因为根节点到叶节点至少有一半是黑节点，所以黑高至少为 ⌈h/2⌉\\left\\lceil h/2\\right\\rceil⌈h/2⌉ ，于是有：\nn≥2⌈h/2⌉−1n \\ge 2^{\\left\\lceil h/2\\right\\rceil}-1\nn≥2⌈h/2⌉−1\n​\t两边取对：\nlog(n+1)≥⌈h/2⌉log(n+1) \\ge \\left\\lceil h/2\\right\\rceil\nlog(n+1)≥⌈h/2⌉\n​\t即：\nh≤2log(n+1)h\\le 2log(n+1) \nh≤2log(n+1)\n红黑树的旋转\n在 CS61BCS61BCS61B​ 上，旋转的正式定义为：\n\nrotateLeft(G) : Let x be the right child of G. Make G the new left child of x.\n\n左旋：设 xxx 为 GGG 的右子结点，让 GGG 成为 xxx 的新左子结点。\n\n\nrotateRight(G): Let x be the left child of G. Make G the new right child of x.\n\n右旋：设 xxx 为 GGG 的左子结点，让 GGG 成为 xxx 的新右子结点。\n\n\n\n\n上面发生的事情的书面描述是这样的：\n\nGGG 的右子项 PPP 与 GGG 合并，并带来了它的孩子。\n然后 PPP 将其左子项传递给 GGG，GGG 向左下降成为 PPP 的左子项。\n\n可以看到树的结构及其高度发生了变化。我们也可以在非根节点上轮换。我们只是暂时断开节点与父节点的连接，旋转节点上的子树，然后重新连接新的根。\n代码实现\n/* 伪代码-左旋 */LEIT-ROTATE(T,x) \ty = x.right \tx.right = y.left \tif y.left ≠ T.nil\t\ty.left.p = x\ty.p = x.p \tif x.p == T.nil /* 根节点 */\t\tT.root=y \telse if x == x.p.left \t\tx.p.left = y \telse x.p.right = y \ty.left= x \tX.p = y\n/* cs61b-java版 *//* 建议结合图来分析代码 */// make a right-leaning link lean to the leftprivate Node rotateLeft(Node h) &#123;    // assert (h != null) &amp;&amp; isRed(h.right);    Node x = h.right;     //将x设为h的右节点    h.right = x.left;     //将h的右节点设为x的左节点(即h.left.right)    x.left = h;           //将x的左节点设为h    return x;             //返回旋转后的根节点&#125;private Node rotateRight(Node h) &#123;    // assert (h != null) &amp;&amp; isRed(h.left);    Node x = h.left;    h.left = x.right;    x.right = h;    return x;&#125;\n/* c++版 */TreeNode *rotateLeft(TreeNode *h)&#123;    TreeNode *child = h-&gt;right;    h-&gt;right = child-&gt;left;    child-&gt;left = h;    return x;&#125;TreeNode *rotateRight(TreeNode *h)&#123;    TreeNode *child = h-&gt;left;    h-&gt;left = child-&gt;right;    child-&gt;right = h;    return x;&#125;\n复杂度分析\n\n\n时间复杂度：O(1)O(1)O(1)\n\n\n空间复杂度：O(1)O(1)O(1)\n\n\n红黑树的插入\n插入操作与 BSTBSTBST​ 基本一致。只是将插入节点染成红色，接着调用 RBT−Insert−FixupRBT-Insert-FixupRBT−Insert−Fixup 函数，修复红黑树的性质。\n代码实现\nRB-INSERT(T,z)\ty = T.nil\tx = T.root\twhile x != T.nil \t\ty = x \t\tif z. key &lt; x. key \t\t\tx = x.left \t\telse\t\t\tx = x.right \tz.p = y\tif y == T.nil \t\tT.root= z \telseif z.key &lt; y.key \t\ty.left = z \telse\t\ty.right = z\t/* 以上是BST-Insert操作 */\tz.left = T.nil\tz.right = T.nil\tz.color = RED\tRB-INSERT-FIXUP(T,z)\nfixupfixupfixup 原理分析\n接下来我们来分析一下 FixupFixupFixup​ 的原理：\n​\t执行 InsertInsertInsert 操作后，针对 RBTRBTRBT 的五个性质，我们不难发现：\n​\t\t性质 111 和性质 333 以及性质 555 依然成立\n​\t\t可能被破坏的是\n​\t\t\t性质 222 （根节点为黑色）\n​\t\t\t性质 444 （不会有两个相连的红节点）\n​\t\t\t并且这两个性质至多只有一条被破坏。\n​\t因此我们从修复性质 444 入手，确定循环终止条件，即 zzz 的父节点是黑色时终止。\n​\t接下来我们以 z.pz.pz.p 是左孩子为例，右孩子的代码直接所有方向取反。\n​\t此时，有如下 333​ 种情况：\n​\t\tcase1case1case1：zzz 的叔节点 yyy 是红色的\n​\t\t\t该情况在 zzz 和 z.pz.pz.p 都是红色时发生。因为 z.p.pz.p.pz.p.p 是黑色的，所以我们把 z.pz.pz.p 和 yyy 染成黑色，把 z.p.pz.p.pz.p.p 染成红色以保持性质 555 。然后我们以 z.p.pz.p.pz.p.p 作为新的 zzz 节点来重复循环（即 zzz 上移两个节点）。\n\n​\t\tcase2case2case2：zzz 的叔结点 yyy 是黑色的且 zzz 是一个右孩子（ zzz 和其父节点方向相反）\n​\t\tcase3case3case3：zzz 的叔结点 yyy 是黑色的且 zzz 是一个左孩子（ zzz 和其父节点方向一致）\n​\t\t\t我们将情况 222 和 333​ 合并在一起来看。\n​\t\t\t对于情况 222 ，我们立即对 z.pz.pz.p 执行一次左旋，将其转变为情况 333 。\n​\t\t\t对于情况 333 ，我们能肯定的是，z.p.pz.p.pz.p.p 必定为黑色。为了修复性质，我们交换 z.pz.pz.p 和z.p.pz.p.pz.p.p 的颜色，并对 z.p.pz.p.pz.p.p 执行一次右旋。此时，z.pz.pz.p 的颜色是黑色，循环终止，修复成功。\n\n\nfixupfixupfixup 代码\nRB-INSERT-FIXUP(T,z) \twhile z.p.color == RED \t\tif z.p == z.p.p.left \t\t\ty = z.p.p.right /* y是z的叔节点 */\t\t\t/* case1 */\t\t\tif y.color == RED \t\t\t\tz.p.color = BLACK \t\t\t\ty.color = BLACK \t\t\t\tz.p.p.color = RED \t\t\t\tz = z.p.p \t\t\telse\t\t\t\t/* case2 */\t\t\t\tif z == z. p. right \t\t\t\t\tz = z.p \t\t\t\t\tLEFT-ROTATE(T,z)\t\t\t\t/* case3 */\t\t\t\tz.p.color = BLACK \t\t\t\tz.p.p.color = RED \t\t\t\tRIGHT-ROTATE(T,z.p.p)\t\telse(same as then clause with &quot;right&quot; and &quot;left&quot; exchanged)\t\t\t......\tT.root.color= BLACK\n复杂度分析\n\n时间复杂度：O(log n)O(log\\ n)O(log n)​\n\n树高为 O(log n)O(log\\ n)O(log n) ，则插入操作需要 O(log n)O(log\\ n)O(log n) 的时间\n调用 FixupFixupFixup 时，仅当情况 111 发生时，zzz 才会沿树上升两层，此时需要O(log n)O(log\\ n)O(log n) 的时间，其他情况都是 O(1)O(1)O(1) 。\n因此是 O(log n)O(log\\ n)O(log n)\n\n\n空间复杂度：O(1)O(1)O(1)\n\n红黑树的删除\n删除操作跟 BSTBSTBST 的删除操作类似，只是多了一个记录节点 xxx 以及在最后添加了一个 RBT−Delete−FixupRBT-Delete-FixupRBT−Delete−Fixup 的操作\n代码实现\n/* 参考BST的删除 *//* 用以v为根的子树替换以u为根的子树 */RB-TRANSPLANT(T, u, v) \tif u.p == T.nil  /* 若u为根节点 */\t\tT.root=v \telse if u == u.p.left  /* 若u是其父节点的左孩子 */\t\tu.p.left = v  \telse  /* 若u是其父节点的右孩子 */\t\tu.p.right = v \tv.p = u.p\n/* 参考BST的删除 *//* attention:第9、13、24行传递的第三个参数与x相同，x只是引用作用，不参与实际删除操作，只是给后续的fixup操作提供参数 */RB-DELETE(T, z) \ty = z \ty-original-color = y.color \t/* 若z的左树为空 */\tif z.left == T.nil \t\tx = z.right \t\tRB-TRANSPLANT(T, z, z.right)  /* 用z的右孩子代替z */\t/* 若z的右树为空 */\telse if z.right == T.nil \t\tx = z.left \t\tRB-TRANSPLANT(T, z, z.left)   /* 用z的左孩子代替z */\t/* 若z有两个孩子 */\telse \t\ty = TREE-MINIMUM(z.right)     /* y是z的后继 */\t\ty-original-color = y.color \t\tx = y.right\t\t/* 若y是z的孩子 */\t\tif y.p == z \t\t\tx.p = y\t\t/* 若y不是z的孩子 */\t\telse\t\t\tRB-TRANSPLANT(T, y, y.right)  /* 用y的右孩子代替y */\t\t\ty.right = z.right             /* 令z的右孩子 */\t\t\ty.right.p = y                 /* 成为y的右孩子 */\t\tRB-TRANSPLANT(T, z, y)        /* 用后继y代替z */\t\ty.left = z.left               /* 将z的左孩子给 */\t\ty.left.p = y                  /* (肯定)没有左孩子的y */\t\ty.color = z.color\tif y-original-color == BLACK      /* y是红的不用管 不会影响红黑树的性质 */\t\tRB-DELETE-FIXUP(T, x)\nfixupfixupfixup 原理分析\n下面重点讲 FixupFixupFixup 的原理：\n​\t记录节点 xxx ，其为我们替换 yyy 所需的节点。\n​\t因为其引用的节点替换 yyy 后可能会违反红黑树的性质，所以我们要对 xxx 及其子树向上进行 FixupFixupFixup 操作。\n​\t如果 yyy 是黑色的话，会产生三个问题：\n​\t\t1. 若 yyy 是原先的根节点，而其一个红色的孩子代替了 yyy ，则会违反==性质2==，即根节点不能为红色。\n​\t\t2. 若 xxx 和 x.px.px.p 是红色的话，则违反了==性质4==，即两个红色节点不能相连。\n​\t\t**3. **若移动 yyy 后导致之前包含 yyy 的简单路径的 bh(x)bh(x)bh(x) 少了 111 ，则 yyy 的任何祖先都不满足性质5，即 bh(x)bh(x)bh(x) 相等。\n​\t\t\t解决这一问题的方法就是将现在占有 yyy 原先位置的 xxx 视作还有额外一层黑色，即黑黑色或红黑色。\n​\t\t\t（但 x.colorx.colorx.color​ 不变，只是视作有两种颜色，这里则变为违反==性质1==，但是后面会进行修复）\n​\t所以问题转变为 FixupFixupFixup 操作需要修复性质 1、2、41、2、41、2、4 。\n​\twhilewhilewhile 循环的目标就是要把额外的黑色沿树上移，直到：\n​\t\t1. xxx​​ 指向红黑节点，此时将其染成单个黑色\n​\t\t2. xxx 指向根节点，此时可以简单地&quot;移除&quot;额外的黑色\n​\t\t3. 执行适当的旋转及重新染色，退出循环\n​\t在循环里， xxx 是黑黑色的~~（如果是红黑色，那就直接染成黑色了，瞎进什么循环）~~，下面设 xxx 是其父节点左孩子， www 是 xxx 的兄弟节点。\n则会有如下几种情况：\n​\tcase1case1case1：xxx 的兄弟节点 www 是红色的  兄弟兄弟，你怎么是红色的(疾旋鼬.jpg)\n​\t\t因为 www 是红色，其必有俩黑色子节点，所以可以改变 www 以及 w.pw.pw.p 的颜色，并做一次旋转。\n​\t\t这样就将 case1case1case1 转换为接下来的三种情况处理。\n\n​\tcase2case2case2：xxx 的兄弟节点 www 是黑色的且 www​ 的两个孩子都是黑色\n​\t\t因为 www 是黑色的，所以从 xxx 和 www 上各去掉一层黑色，即 www 变为红色，xxx 上移。为补偿删掉的一层黑色，将新的 xxx 加一层黑色，继续循环，直到满足条件。\n​\t\t若从 case1case1case1 过来的，此时新的 xxx 是红黑色的，满足条件，直接将新的 xxx​ 染成黑色，结束循环，修复完成。\n\n​\tcase3case3case3：xxx 的兄弟节点 www 是黑色的且 www​ 的右节点是黑色，左节点是红色\n​\t\t此情况需要转换为 case4case4case4 来解决，所以我们先交换 www 和 w.leftw.leftw.left 的颜色，然后对 www 进行右旋。\n​\tcase4case4case4：xxx 的兄弟节点 www 是黑色的且 www 的右节点是红色\n​\t\t此情况下，将 w.rightw.rightw.right 变为黑色，x.px.px.p 与 www 交换颜色，并对 x.px.px.p 进行一次左旋，消除 xxx 的额外黑色，最后将 xxx 设为根节点，跳出循环。（ xxx 的那一层黑色给到了原来 www​ 的右节点）\n\nfixupfixupfixup 代码\nRB-DELETE-FIXUP(T, x) \twhile x != T. root and x.color == BLACK \t\tif x == x.p.left \t\t\tw = x.p.right \t\t\t/* case1 */\t\t\tif w.color == RED \t\t\t\tw.color = BLACK \t\t\t\tx.p.color = RED \t\t\t\tLEFT-ROTATE(T, x.p) \t\t\t\tw = x.p.right \t\t\t/* case2 */\t\t\tif w.left.color == BLACK and w.right.color == BLACK \t\t\t\tw.color = RED \t\t\t\tx = x.p\t\t\telse \t\t\t/* case3 */\t\t\tif w.right.color == BLACK \t\t\t\tw.left.color = BLACK \t\t\t\tw.color = RED \t\t\t\tRIGHT-ROTATE(T,w) \t\t\t\tw = x.p.right\t\t\t/* case4- */\t\t\tw.color = x.p.color \t\t\tx.p.color = BLACK \t\t\tw.right.color = BLACK \t\t\tLEFT-ROTATE(T, x.p) \t\t\tx = T.root \t\telse (same as then clause with &quot;right&quot; and &quot;left&quot; exchanged) \t\t\t......\tx.color = BLACK\n复杂度分析\n\n\n时间复杂度：O(log n)O(log\\ n)O(log n) ​\n\n不调用 FixupFixupFixup 时需要 O(log n)O(log\\ n)O(log n) ，因为树高是 O(log n)O(log\\ n)O(log n)\n而调用FixupFixupFixup ，case1、3、4case1、3、4case1、3、4 进行常数次数操作，case2case2case2 至多循环到根节点，也即树高 O(log n)O(log\\ n)O(log n) ​\n因此是 O(log n)O(log\\ n)O(log n)\n\n\n\n空间复杂度：O(1)O(1)O(1)\n\n\nB树 B-tree\n\n未完工喵~:yum:\nshmmshmmshmm 速速🤺\n\n","categories":["DataStru&Algo"],"tags":["Data Structure","Algorithm","SCNU Turing Discussion","BST","RBT"]},{"title":"RAG for Knowledge-Intensive NLP Tasks","url":"/paper/RAG-for-Knowledge-Intensive-NLP-Tasks/","content":"1. 摘要与引言 (Abstract &amp; Introduction)\n\n\n核心问题: 大型预训练语言模型（如BART, T5）虽然在参数中存储了大量事实知识，但在知识的精确访问和操作上能力有限 。它们无法轻易扩展或修正知识，其决策过程缺乏透明度，并且可能产生“幻觉”（即捏造事实）。\n\n\n解决方案: 提出一种名为 RAG (Retrieval-Augmented Generation) 的通用模型框架，它将预训练的参数化记忆（一个seq2seq模型）与非参数化记忆（一个可通过神经检索器访问的密集向量索引，如维基百科）相结合 。\n\n\n模型构成: RAG模型包含一个预训练的seq2seq生成器（BART）和一个预训练的密集段落检索器（DPR）。\n\n\n主要贡献:\n\n\n在三个开放域问答任务上取得了最先进的（SOTA）成果 。\n\n\n在语言生成任务上，RAG比纯参数化模型生成的内容更具体、更多样、更符合事实 。\n\n\n展示了通过替换非参数化记忆（文档索引）来有效更新模型世界知识的能力 。\n\n\n\n\n\n2. 方法 (Methods)\n\n\n整体架构 (Figure 1):\n\n\n\n输入查询 (x) 进入 查询编码器 (Query Encoder) 生成查询向量 q(x) 。\n\n\n使用 最大内积搜索 (MIPS) 在 文档索引 (Document Index) 中检索与 q(x) 最相关的Top-K个文档 z 。\n\n\n生成器 (Generator) 将原始输入 x 和检索到的文档 z 结合起来，生成最终输出 y 。\n\n\n整个过程通过端到端训练，同时优化生成器和查询编码器 。\n\n\n\n\nRAG的两种模型:\n\n\nRAG-Sequence: 模型为整个输出序列选择同一个检索文档 z 。其概率模型是对不同文档 z 生成完整序列 y 的概率进行加权求和 。\n\n公式:\np(y∣x)≈∑z∈top−k(p(⋅∣x))pη(z∣x)pθ(y∣x,z)=∑z∈top−k(p(⋅∣x))pη(z∣x)Πi=1Npθ(yi∣x,z,y1:i−1)p(y|x) \\approx \\sum_{z \\in top-k(p(·|x))} p_\\eta(z|x)p_\\theta(y|x,z) = \\sum_{z \\in top-k(p(·|x))} p_\\eta(z|x)\\Pi_{i=1}^{N}p_\\theta(y_i|x,z,y_{1:i−1})p(y∣x)≈∑z∈top−k(p(⋅∣x))​pη​(z∣x)pθ​(y∣x,z)=∑z∈top−k(p(⋅∣x))​pη​(z∣x)Πi=1N​pθ​(yi​∣x,z,y1:i−1​)\n\n\n\nRAG-Token: 模型在生成每一个目标词元 (token) 时，都可以从多个文档中汲取信息 。其概率模型是在生成每个词元时，都对所有Top-K文档的贡献进行一次加权求和，然后将每一步的概率连乘 。\n\n公式:\np(y∣x)≈Πi=1N∑z∈top−(p(⋅∣x))kpη(z∣x)pθ(yi∣x,z,y1:i−1)p(y|x) \\approx \\Pi_{i=1}^{N} \\sum_{z∈top-(p(·|x))k}p_\\eta(z|x)p_\\theta(y_i|x,z,y_{1:i−1})p(y∣x)≈Πi=1N​∑z∈top−(p(⋅∣x))k​pη​(z∣x)pθ​(yi​∣x,z,y1:i−1​)\n\n\n\n\n\n核心组件:\n\n\n检索器 (Retriever): 基于DPR（Dense Passage Retriever），它包含一个BERT-base的查询编码器和文档编码器。\n\n\n生成器 (Generator): 使用BART-large，一个拥有4亿参数的预训练seq2seq模型。检索到的文档内容与原始输入被简单地拼接在一起送入BART。\n\n\n非参数化记忆: 使用2018年12月的维基百科快照，分割成2100万个100词的文档块。\n\n\n\n\n训练与解码:\n\n\n训练: 联合训练查询编码器和生成器，但保持文档编码器（和索引）固定以降低计算成本。\n\n\n解码: RAG-Token可以直接使用标准beam search解码 。RAG-Sequence需要为每个检索到的文档分别运行beam search，然后对生成的候选集进行边缘化概率计算，有&quot;Thorough Decoding&quot;和&quot;Fast Decoding&quot;两种方式。\n\n\n\n\n\n3. 实验与结果 (Experiments &amp; Results)\n\n\n开放域问答 (Open-domain QA):\n\n\n在Natural Questions (NQ), WebQuestions (WQ), 和 CuratedTrec (CT) 数据集上，RAG均取得了SOTA成绩。\n\n\nRAG-Sequence在TQA数据集上也超过了T5-11B+SSM。\n\n\n即使正确答案未出现在任何检索到的文档中，RAG仍能在NQ上实现11.8%的准确率，这是纯抽取式模型无法做到的。\n\n\n\n\n生成任务 (Generation Tasks):\n\n\nMS-MARCO: RAG-Sequence在Bleu和Rouge-L指标上均优于BART基线。\n\n\nJeopardy问题生成: RAG-Token在Q-BLEU-1指标上表现最佳。\n人工评估显示，RAG的生成结果在事实性 (42.7% vs 7.1%) 和具体性 (37.4% vs 16.8%) 上远超BART。\n\n\n多样性: RAG的生成结果比BART更多样化，其中RAG-Sequence的多样性最高。\n\n\n\n\n事实核查 (Fact Verification):\n\n\n在FEVER任务中，RAG在未接收任何检索监督信号的情况下，其准确率与经过复杂设计的SOTA模型差距在4.3%以内。\n\n\nRAG检索到的Top-10文档中有90%的概率包含FEVER任务中的黄金标准证据文章。\n\n\n\n\n核心能力分析:\n\n\n知识更新 (Hot-swapping): 实验证明，通过切换2016年和2018年的维基百科索引，RAG能正确回答相应年份的问题，证明了其知识可以被轻松更新。\n\n\n记忆协同: 通过分析生成过程，发现检索的非参数化记忆可以引导并“触发”生成器的参数化记忆，使其生成存储在内部的特定知识（如书名。\n\n\n可学习的检索: 消融实验表明，对查询编码器进行微调（Learned Retrieval）对于大多数任务的性能至关重要，尤其是在开放域问答上。\n\n\n\n\n\n4. 讨论与影响 (Discussion &amp; Broader Impact)\n\n\n总结: RAG成功地将参数化和非参数化记忆结合，在知识密集型任务上表现出色，生成的文本更真实、更具体，并且知识易于更新。\n\n\n社会效益: RAG更强地根植于事实知识，能减少“幻觉”，提供更好的可解释性和可控性。\n\n\n潜在风险: 与其他大型语言模型类似，RAG也可能被用于生成误导性内容或垃圾邮件。但其事实性 grounding 可以在一定程度上缓解此问题。\n\n\n","categories":["paper"],"tags":["paper","RAG","NLP"]},{"title":"REPLUG: Retrieval-Augmented Black-Box Language Models","url":"/paper/REPLUG-Retrieval-Augmented-Black-Box-Language-Models/","content":"问题 (Problem)\n大型语言模型（LLMs）如GPT-3虽然强大，但存在两个核心问题：\n\n知识局限性：模型参数中存储的知识是静态的，无法实时更新，且对于长尾知识（rare knowledge）的覆盖不全，容易产生事实性错误或“幻觉”。\n黑盒特性：当前最先进的LLMs（通常 &gt;100B 参数）往往通过API提供服务，用户无法访问模型的内部参数、梯度或进行微调。这使得传统的、需要“白盒”访问权限的检索增强方法（如RETRO、Atlas）无法适用。\n\n因此，本文的核心问题是：如何在只能“黑盒”访问（即只能输入文本、获取输出）的前提下，通过外部知识库对大型语言模型进行有效的检索增强，以提升其性能并减少幻觉？\n方法 (Method)\n作者提出了 REPLUG (Retrieve and Plug) 框架，它将语言模型视为一个不可更改的黑盒，并将一个可调优的检索器作为插件来增强它。\nREPLUG 推理过程\nREPLUG的推理过程分为两步：文档检索 和 输入重构与集成。\n\nREPLUG 推理流程图（论文Figure 2）\n\n\n文档检索 (Document Retrieval)\n\n给定一个输入上下文 xxx，使用一个**密集检索器（Dense Retriever）**从一个大规模的外部语料库 D\\mathcal{D}D 中检索出与 xxx 最相关的 kkk 个文档。\n该检索器采用双编码器架构，将输入 xxx 和文档 ddd 分别编码为向量 E(x)E(x)E(x) 和 E(d)E(d)E(d)。\n相关性得分通过余弦相似度计算：s(d,x)=cos⁡(E(d),E(x))s(d,x) = \\cos(E(d), E(x))\ns(d,x)=cos(E(d),E(x))\n\n为了效率，所有文档的向量会预先计算并用FAISS等工具建立索引，以实现快速检索。\n\n\n\n输入重构与集成 (Input Reformulation &amp; Ensemble)\n\n直接将所有 kkk 个文档拼接到输入 xxx 前面会受到模型上下文窗口长度的限制。\nREPLUG 采用了一种巧妙的集成策略：\n\n对于检索到的top-k个文档 d1,d2,...,dk{d_1, d_2, ..., d_k}d1​,d2​,...,dk​，分别与原始输入 xxx 进行拼接，构造出 kkk 个独立的输入：d1∘x,d2∘x,...,dk∘x{d_1 \\circ x, d_2 \\circ x, ..., d_k \\circ x}d1​∘x,d2​∘x,...,dk​∘x。\n将这 kkk 个输入并行地送入黑盒语言模型中，得到 kkk 组关于下一个词的输出概率分布。\n最终的输出概率是对这 kkk 组概率分布进行加权平均的结果。权重由检索器的相关性得分决定。\n\n\n下一个词 yyy 的最终概率计算公式为：\n\np(y∣x,D′)=∑d∈D′p(y∣d∘x)⋅λ(d,x)p(y|x, \\mathcal{D}&#x27;) = \\sum_{d \\in \\mathcal{D}&#x27;} p(y|d \\circ x) \\cdot \\lambda(d, x)\np(y∣x,D′)=d∈D′∑​p(y∣d∘x)⋅λ(d,x)\n其中，D′\\mathcal{D}&#x27;D′ 是 top-k 文档集合，d∘xd \\circ xd∘x 表示文档和输入的拼接。权重 λ(d,x)\\lambda(d,x)λ(d,x) 是检索得分经过Softmax归一化后的结果：\nλ(d,x)=es(d,x)∑d′∈D′es(d′,x)\\lambda(d,x) = \\frac{e^{s(d,x)}}{\\sum_{d&#x27; \\in \\mathcal{D}&#x27;} e^{s(d&#x27;,x)}}\nλ(d,x)=∑d′∈D′​es(d′,x)es(d,x)​\n\n这种方法的优势是，虽然需要调用模型 kkk 次，但它绕过了上下文长度限制，可以集成更多的文档信息，并且计算可以完全并行，只是增加了API调用成本。\n\n\n\nREPLUG LSR: 训练可适变的检索器\n为了让检索器更能“投其所好”，即检索到能最大化帮助特定黑盒LLM的文档，作者提出了 REPLUG LSR (LM-Supervised Retrieval) 训练方案。其核心思想是用黑盒LLM本身作为监督信号来指导检索器的训练。\n\nREPLUG LSR 训练流程图（论文Figure 3）\n训练过程包含四个步骤：\n\n\n计算检索似然度 (Computing Retriever Likelihood, PRP_RPR​)\n\n对于一个输入 xxx，用当前检索器检索 top-k 个文档 D′\\mathcal{D}&#x27;D′。\n基于检索器的相似度得分 s(d,x)s(d,x)s(d,x)，计算出一个在 D′\\mathcal{D}&#x27;D′ 上的概率分布 PR(d∣x)P_R(d|x)PR​(d∣x)，代表了检索器认为每个文档的重要程度。\n公式为：\n\nPR(d∣x)=es(d,x)/τ∑d′∈D′es(d′,x)/τP_R(d|x) = \\frac{e^{s(d,x)/\\tau}}{\\sum_{d&#x27; \\in \\mathcal{D}&#x27;} e^{s(d&#x27;,x)/\\tau}}\nPR​(d∣x)=∑d′∈D′​es(d′,x)/τes(d,x)/τ​\n其中 τ\\tauτ 是温度超参数。\n\n\n计算语言模型似然度 (Computing LM Likelihood, QLMQ_{LM}QLM​)\n\n这是获取监督信号的关键一步。\n对于 D′\\mathcal{D}&#x27;D′ 中的每一个文档 ddd，将其与输入 xxx 拼接后，喂给黑盒LLM，计算模型对于真实答案（ground truth） yyy 的预测概率 PLM(y∣d,x)P_{LM}(y|d,x)PLM​(y∣d,x)。\n这个概率值越高，说明文档 ddd 对LLM做出正确预测的帮助越大。\n将这些概率值也转换成一个目标分布 QLM(d∣x,y)Q_{LM}(d|x,y)QLM​(d∣x,y)，代表了LLM认为每个文档的重要程度。\n公式为：\n\nQLM(d∣x,y)=ePLM(y∣d,x)/β∑d′∈D′ePLM(y∣d′,x)/βQ_{LM}(d|x, y) = \\frac{e^{P_{LM}(y|d,x)/\\beta}}{\\sum_{d&#x27; \\in \\mathcal{D}&#x27;} e^{P_{LM}(y|d&#x27;,x)/\\beta}}\nQLM​(d∣x,y)=∑d′∈D′​ePLM​(y∣d′,x)/βePLM​(y∣d,x)/β​\n其中 β\\betaβ 是另一个温度超参数。\n\n\n优化损失函数 (Loss Function)\n\n通过最小化检索器分布 PRP_RPR​ 和语言模型目标分布 QLMQ_{LM}QLM​ 之间的 KL散度 (KL Divergence) 来更新检索器的参数。\n这会迫使检索器学习去预测那些能够最大化LLM性能的文档。\n损失函数为：L=1∣B∣∑x∈BKL(PR(d∣x) ∣∣ QLM(d∣x,y))\\mathcal{L} = \\frac{1}{|\\mathcal{B}|}\\sum_{x \\in \\mathcal{B}}KL(P_R(d|x) \\ || \\ Q_{LM}(d|x,y))\nL=∣B∣1​x∈B∑​KL(PR​(d∣x) ∣∣ QLM​(d∣x,y))\n其中 B\\mathcal{B}B 是一组输入上下文\n在训练过程中，只有检索器的参数被更新，LLM始终保持冻结和黑盒。\n\n\n\n异步更新数据索引 (Asynchronous Update of Datastore Index)\n\n由于检索器的编码器在训练中不断变化，预先计算的文档向量会变得“过时”。\n因此，训练过程中需要周期性地（例如每T个训练步）使用更新后的编码器重新计算整个文档库的向量，并重建FAISS索引。这是一个计算量巨大的步骤。\n\n\n\nBaseline\n论文在多个任务上与不同的基线模型进行了对比：\n\n\n语言建模 (Language Modeling)：\n\n基线: 原始的、未增强的GPT-2和GPT-3系列模型。\n对比模型: REPLUG增强版和REPLUG LSR增强版。\n\n\n\nMMLU (多任务语言理解)：\n\n基线LLMs: Codex (175B), PaLM (540B), Flan-PaLM (540B)。\n白盒检索模型: Atlas (11B)。\n\n\n\n开放域问答 (Open-Domain QA)：\n\n基线LLMs (few-shot): Chinchilla, PaLM, Codex。\n白盒检索模型 (fine-tuned): RETRO, R2-D2, Atlas。\n\n\n\n数据集 (Datasets)\n\n\nREPLUG LSR 训练:\n\n训练查询: 从 The Pile 训练集中采样的80万个序列。\n外部文档库: 从 The Pile 训练集中采样的3600万个文档（与查询无重叠）。\n\n\n\n评估:\n\n语言建模: The Pile 基准测试集。\n多任务语言理解: MMLU 数据集，采用5-shot设置。\n开放域问答: Natural Questions (NQ) 和 TriviaQA (TQA) 数据集，采用few-shot设置。\n泛化性分析: Wikitext-103 数据集，用于测试REPLUG在GPT-2, BLOOM, OPT等不同模型家族上的效果。\n\n\n\n可复现性 (Reproducibility)\n\n代码: 论文中未提供公开的代码库链接。\n算力: 复现该方法需要巨大的计算资源。\n\nAPI成本: REPLUG LSR的训练需要大量调用一个中等大小的LLM（如GPT-3 Curie）作为监督，推理过程则需要将API调用成本乘以 kkk（集成文档的数量）。\nGPU资源: 训练一个强大的密集检索器本身就需要大量GPU算力。\n索引更新瓶颈: REPLUG LSR训练中最耗费计算的步骤是异步索引更新。周期性地为数千万文档重新编码并建立索引，需要非常强大的计算集群，是个人或小团队难以承受的。\n\n\n\n可改进的几个点 (Potential Improvements)\n\n推理效率: 推理成本是原始模型的 kkk 倍，这是一个主要缺点。未来的研究可以探索更高效的集成方法，例如，如何在单次前向传播中融合多个文档的信息，或者用更小的模型来预处理和压缩检索到的信息。\n训练效率: 异步索引更新是训练的巨大瓶颈。可以研究更高效的索引更新策略，如增量更新，或者探索不需要静态索引的检索方法。\n集成权重: 当前的集成权重 λ(d,x)\\lambda(d,x)λ(d,x) 仅依赖于检索器的初始分数。可以设计更动态的权重方案，比如结合LLM对每个文档的“置信度”进行二次加权。\n可解释性: 论文作者也提到，模型何时依赖参数知识、何时依赖检索知识是不透明的。增强模型的可解释性，让用户知道答案的来源，是未来一个重要的研究方向。\n检索粒度: REPLUG检索的是整个文档。可以探索更细粒度的检索，如段落、句子甚至短语，这可能为模型提供更精确、噪声更少的信息。\n\n可以被引用的一些结论 (Citable Conclusions)\n\n黑盒检索增强是可行且有效的：本文首次证明，在不访问模型内部参数的情况下，通过外部检索能显著提升超大规模（&gt;100B）黑盒语言模型的性能。\n语言模型自身是最好的监督者：REPLUG LSR的核心洞见是，可以用LLM本身来指导检索器的训练，让检索器学会去寻找对该LLM最有帮助的知识，实现了检索器和语言模型的“适配”。\n在多样的模型和任务上表现稳定：REPLUG框架具有普适性，能够稳定地提升包括GPT系列、BLOOM、OPT在内的多种模型家族在语言建模、知识问答和复杂推理等多个任务上的表现。\n显著提升少样本学习能力：在开放域问答的少样本（few-shot）设定下，REPLUG LSR增强的Codex模型甚至超越了使用更多样本进行微调的先进白盒模型（如Atlas），创造了新的SOTA记录。\n检索对于长尾知识至关重要：定性分析表明，REPLUG对于包含**稀有实体（rare entities）**的文本尤其有效，这证明了检索在弥补模型参数化知识短板方面的重要作用。\n用更小的模型达到更优的效果：通过REPLUG增强，一个175B参数的模型（Codex）在MMLU任务上的表现可以媲美一个三倍大于它、且经过指令微调的模型（540B的Flan-PaLM），展示了检索在提升模型效率上的巨大潜力。\n\n","categories":["paper"],"tags":["paper","Retrieval","LLM","RAG"]}]