[{"title":"Hello World","url":"/uncategorized/hello-world/","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.\nQuick Start\nCreate a new post\n$ hexo new &quot;My New Post&quot;\nMore info: Writing\nRun server\n$ hexo server\nMore info: Server\nGenerate static files\n$ hexo generate\nMore info: Generating\nDeploy to remote sites\n$ hexo deploy\nMore info: Deployment\n"},{"title":"RAG for Knowledge-Intensive NLP Tasks","url":"/paper/RAG-for-Knowledge-Intensive-NLP-Tasks/","content":"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n[TOC]\n1. 摘要与引言 (Abstract &amp; Introduction)\n\n\n核心问题: 大型预训练语言模型（如BART, T5）虽然在参数中存储了大量事实知识，但在知识的精确访问和操作上能力有限 。它们无法轻易扩展或修正知识，其决策过程缺乏透明度，并且可能产生“幻觉”（即捏造事实）。\n\n\n解决方案: 提出一种名为 RAG (Retrieval-Augmented Generation) 的通用模型框架，它将预训练的参数化记忆（一个seq2seq模型）与非参数化记忆（一个可通过神经检索器访问的密集向量索引，如维基百科）相结合 。\n\n\n模型构成: RAG模型包含一个预训练的seq2seq生成器（BART）和一个预训练的密集段落检索器（DPR）。\n\n\n主要贡献:\n\n\n在三个开放域问答任务上取得了最先进的（SOTA）成果 。\n\n\n在语言生成任务上，RAG比纯参数化模型生成的内容更具体、更多样、更符合事实 。\n\n\n展示了通过替换非参数化记忆（文档索引）来有效更新模型世界知识的能力 。\n\n\n\n\n\n2. 方法 (Methods)\n\n\n整体架构 (Figure 1):\n\n\n\n输入查询 (x) 进入 查询编码器 (Query Encoder) 生成查询向量 q(x) 。\n\n\n使用 最大内积搜索 (MIPS) 在 文档索引 (Document Index) 中检索与 q(x) 最相关的Top-K个文档 z 。\n\n\n生成器 (Generator) 将原始输入 x 和检索到的文档 z 结合起来，生成最终输出 y 。\n\n\n整个过程通过端到端训练，同时优化生成器和查询编码器 。\n\n\n\n\nRAG的两种模型:\n\n\nRAG-Sequence: 模型为整个输出序列选择同一个检索文档 z 。其概率模型是对不同文档 z 生成完整序列 y 的概率进行加权求和 。\n\n公式:\np(y∣x)≈∑z∈top−k(p(⋅∣x))pη(z∣x)pθ(y∣x,z)=∑z∈top−k(p(⋅∣x))pη(z∣x)Πi=1Npθ(yi∣x,z,y1:i−1)p(y|x) \\approx \\sum_{z \\in top-k(p(·|x))} p_\\eta(z|x)p_\\theta(y|x,z) = \\sum_{z \\in top-k(p(·|x))} p_\\eta(z|x)\\Pi_{i=1}^{N}p_\\theta(y_i|x,z,y_{1:i−1})p(y∣x)≈∑z∈top−k(p(⋅∣x))​pη​(z∣x)pθ​(y∣x,z)=∑z∈top−k(p(⋅∣x))​pη​(z∣x)Πi=1N​pθ​(yi​∣x,z,y1:i−1​)\n\n\n\nRAG-Token: 模型在生成每一个目标词元 (token) 时，都可以从多个文档中汲取信息 。其概率模型是在生成每个词元时，都对所有Top-K文档的贡献进行一次加权求和，然后将每一步的概率连乘 。\n\n公式:\np(y∣x)≈Πi=1N∑z∈top−(p(⋅∣x))kpη(z∣x)pθ(yi∣x,z,y1:i−1)p(y|x) \\approx \\Pi_{i=1}^{N} \\sum_{z∈top-(p(·|x))k}p_\\eta(z|x)p_\\theta(y_i|x,z,y_{1:i−1})p(y∣x)≈Πi=1N​∑z∈top−(p(⋅∣x))k​pη​(z∣x)pθ​(yi​∣x,z,y1:i−1​)\n\n\n\n\n\n核心组件:\n\n\n检索器 (Retriever): 基于DPR（Dense Passage Retriever），它包含一个BERT-base的查询编码器和文档编码器。\n\n\n生成器 (Generator): 使用BART-large，一个拥有4亿参数的预训练seq2seq模型。检索到的文档内容与原始输入被简单地拼接在一起送入BART。\n\n\n非参数化记忆: 使用2018年12月的维基百科快照，分割成2100万个100词的文档块。\n\n\n\n\n训练与解码:\n\n\n训练: 联合训练查询编码器和生成器，但保持文档编码器（和索引）固定以降低计算成本。\n\n\n解码: RAG-Token可以直接使用标准beam search解码 。RAG-Sequence需要为每个检索到的文档分别运行beam search，然后对生成的候选集进行边缘化概率计算，有&quot;Thorough Decoding&quot;和&quot;Fast Decoding&quot;两种方式。\n\n\n\n\n\n3. 实验与结果 (Experiments &amp; Results)\n\n\n开放域问答 (Open-domain QA):\n\n\n在Natural Questions (NQ), WebQuestions (WQ), 和 CuratedTrec (CT) 数据集上，RAG均取得了SOTA成绩。\n\n\nRAG-Sequence在TQA数据集上也超过了T5-11B+SSM。\n\n\n即使正确答案未出现在任何检索到的文档中，RAG仍能在NQ上实现11.8%的准确率，这是纯抽取式模型无法做到的。\n\n\n\n\n生成任务 (Generation Tasks):\n\n\nMS-MARCO: RAG-Sequence在Bleu和Rouge-L指标上均优于BART基线。\n\n\nJeopardy问题生成: RAG-Token在Q-BLEU-1指标上表现最佳。\n人工评估显示，RAG的生成结果在事实性 (42.7% vs 7.1%) 和具体性 (37.4% vs 16.8%) 上远超BART。\n\n\n多样性: RAG的生成结果比BART更多样化，其中RAG-Sequence的多样性最高。\n\n\n\n\n事实核查 (Fact Verification):\n\n\n在FEVER任务中，RAG在未接收任何检索监督信号的情况下，其准确率与经过复杂设计的SOTA模型差距在4.3%以内。\n\n\nRAG检索到的Top-10文档中有90%的概率包含FEVER任务中的黄金标准证据文章。\n\n\n\n\n核心能力分析:\n\n\n知识更新 (Hot-swapping): 实验证明，通过切换2016年和2018年的维基百科索引，RAG能正确回答相应年份的问题，证明了其知识可以被轻松更新。\n\n\n记忆协同: 通过分析生成过程，发现检索的非参数化记忆可以引导并“触发”生成器的参数化记忆，使其生成存储在内部的特定知识（如书名。\n\n\n可学习的检索: 消融实验表明，对查询编码器进行微调（Learned Retrieval）对于大多数任务的性能至关重要，尤其是在开放域问答上。\n\n\n\n\n\n4. 讨论与影响 (Discussion &amp; Broader Impact)\n\n\n总结: RAG成功地将参数化和非参数化记忆结合，在知识密集型任务上表现出色，生成的文本更真实、更具体，并且知识易于更新。\n\n\n社会效益: RAG更强地根植于事实知识，能减少“幻觉”，提供更好的可解释性和可控性。\n\n\n潜在风险: 与其他大型语言模型类似，RAG也可能被用于生成误导性内容或垃圾邮件。但其事实性 grounding 可以在一定程度上缓解此问题。\n\n\n","categories":["paper"],"tags":["paper RAG NLP"]}]